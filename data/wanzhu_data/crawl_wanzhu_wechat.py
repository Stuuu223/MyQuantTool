#!/usr/bin/env python3
"""
é¡½ä¸»æ¯å°ç¨‹åºæ•°æ®é‡‡é›† - Playwrightæ–¹æ¡ˆ
ç›´æ¥æ§åˆ¶å¾®ä¿¡å†…ç½®æµè§ˆå™¨ï¼Œæ— éœ€æ‰‹æœºæŠ“åŒ…
"""

from playwright.sync_api import sync_playwright
import pandas as pd
import json
import time
from datetime import datetime
from pathlib import Path

BASE_DIR = Path(__file__).resolve().parent

def crawl_wanzhu():
    """
    ä½¿ç”¨Playwrightè®¿é—®é¡½ä¸»æ¯å°ç¨‹åº
    """
    with sync_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨ï¼ˆè¿æ¥åˆ°å·²æ‰“å¼€çš„å¾®ä¿¡å†…ç½®æµè§ˆå™¨ï¼‰
        print("æ­£åœ¨è¿æ¥æµè§ˆå™¨...")
        print("è¯·ç¡®ä¿å¾®ä¿¡å·²æ‰“å¼€é¡½ä¸»æ¯å°ç¨‹åº")
        
        # å°è¯•è¿æ¥å·²å­˜åœ¨çš„æµè§ˆå™¨å®ä¾‹
        try:
            browser = p.chromium.connect_over_cdp("http://localhost:9222")
            print("âœ“ å·²è¿æ¥åˆ°å¾®ä¿¡æµè§ˆå™¨")
        except:
            print("âœ— æ— æ³•è¿æ¥ï¼Œå°è¯•å¯åŠ¨æ–°æµè§ˆå™¨...")
            browser = p.chromium.launch(headless=False)
        
        context = browser.contexts[0] if browser.contexts else browser.new_context()
        page = context.pages[0] if context.pages else context.new_page()
        
        # ç›‘å¬ç½‘ç»œè¯·æ±‚
        api_data = []
        
        def handle_route(route, request):
            url = request.url
            if "wanzhu" in url or "rank" in url or "api" in url:
                print(f"\nğŸ¯ æ•è·APIè¯·æ±‚: {url[:80]}...")
                try:
                    response = route.fetch()
                    body = response.body()
                    data = json.loads(body)
                    api_data.append({
                        'url': url,
                        'data': data,
                        'time': datetime.now().strftime('%Y%m%d_%H%M%S')
                    })
                    print(f"âœ“ æ•°æ®å·²ä¿å­˜")
                except:
                    pass
            route.continue_()
        
        page.route("**/*", handle_route)
        
        print("\nè¯·åœ¨å¾®ä¿¡ä¸­æ“ä½œé¡½ä¸»æ¯å°ç¨‹åº...")
        print("æµè§ˆæ¦œå•é¡µé¢ï¼Œæ•°æ®ä¼šè‡ªåŠ¨æ•è·")
        print("æŒ‰ Ctrl+C åœæ­¢é‡‡é›†\n")
        
        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            print("\n\né‡‡é›†åœæ­¢ï¼Œæ­£åœ¨ä¿å­˜æ•°æ®...")
        
        # ä¿å­˜æ•°æ®
        if api_data:
            save_path = BASE_DIR / f"wanzhu_api_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(save_path, 'w', encoding='utf-8') as f:
                json.dump(api_data, f, ensure_ascii=False, indent=2)
            print(f"âœ“ æ•°æ®å·²ä¿å­˜: {save_path}")
            
            # è§£æå¹¶ç”ŸæˆCSV
            parse_and_save_csv(api_data)
        
        browser.close()

def parse_and_save_csv(api_data):
    """è§£æAPIæ•°æ®å¹¶ä¿å­˜ä¸ºCSV"""
    all_records = []
    
    for item in api_data:
        data = item['data']
        # æ ¹æ®é¡½ä¸»æ¯å®é™…æ•°æ®ç»“æ„è§£æ
        if isinstance(data, dict):
            # å°è¯•ä¸åŒçš„å­—æ®µå
            records = data.get('list') or data.get('data') or data.get('records') or data.get('rankings') or []
            if records:
                for record in records:
                    all_records.append(record)
    
    if all_records:
        df = pd.DataFrame(all_records)
        csv_path = BASE_DIR / f"wanzhu_data_{datetime.now().strftime('%Y%m%d')}.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"âœ“ CSVå·²ç”Ÿæˆ: {csv_path}")
        print(f"  å…± {len(df)} æ¡è®°å½•")
        print(f"  å­—æ®µ: {df.columns.tolist()}")

if __name__ == "__main__":
    crawl_wanzhu()