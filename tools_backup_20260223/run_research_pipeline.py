#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç ”ç©¶æµæ°´çº¿æ€»é©±åŠ¨
CTOæŒ‡ä»¤ï¼šç»™å®šlabels + æ—¥æœŸèŒƒå›´ï¼Œä¸€é”®é‡è·‘ï¼šTickå›æ”¾â†’ratioè®¡ç®—â†’summaryè¾“å‡º
ç›®æ ‡ï¼šå¹‚ç­‰+å¯é‡æ”¾ï¼Œé‡å¤è¿è¡ŒNæ¬¡ç»“æœå®Œå…¨ä¸€è‡´
"""

import sys
import os
from pathlib import Path
from datetime import datetime
import json
import pandas as pd

PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from logic.services.data_service import data_service
from logic.qmt_historical_provider import QMTHistoricalProvider
from logic.rolling_metrics import RollingFlowCalculator


def load_labels(config_path: Path) -> list:
    """
    åŠ è½½æ ‡ç­¾é…ç½®ï¼ˆå”¯ä¸€æ­£ç¡®æ¥æºï¼‰
    æ ¼å¼: [{"code": "300017", "name": "ç½‘å®¿ç§‘æŠ€", "dates": [...]}, ...]
    """
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    return config.get('samples', [])


def analyze_single_case(code: str, name: str, date: str, label: str, 
                        output_dir: Path, log_file) -> dict:
    """
    åˆ†æå•ä¸ªæ¡ˆä¾‹ï¼Œè¿”å›ç»“æœå­—å…¸
    """
    log_msg = f"\n[{code} {name}] {date} ({label})"
    print(log_msg)
    log_file.write(log_msg + "\n")
    
    try:
        # 1. ç¯å¢ƒæ£€æŸ¥
        passed, env_info = data_service.env_check()
        if not passed:
            raise Exception(f"ç¯å¢ƒæ£€æŸ¥å¤±è´¥: {env_info}")
        
        # 2. éªŒè¯æ•°æ®å­˜åœ¨
        exists, estimated = data_service.verify_tick_exists(code)
        if not exists:
            raise Exception(f"Tickæ•°æ®ä¸å­˜åœ¨")
        
        # 3. è·å–æ˜¨æ”¶ä»·ï¼ˆå”¯ä¸€æ­£ç¡®æ¥æºï¼‰
        pre_close = data_service.get_pre_close(code, date)
        if pre_close <= 0:
            raise Exception(f"æ— æ³•è·å–æ˜¨æ”¶ä»·")
        
        log_msg = f"  æ˜¨æ”¶ä»·: {pre_close}"
        print(log_msg)
        log_file.write(log_msg + "\n")
        
        # 4. Tickå›æ”¾
        formatted_code = data_service._format_code(code)
        start_time = date.replace('-', '') + '093000'
        end_time = date.replace('-', '') + '150000'
        
        provider = QMTHistoricalProvider(
            stock_code=formatted_code,
            start_time=start_time,
            end_time=end_time,
            period='tick'
        )
        
        # 5. è®¡ç®—èµ„é‡‘æµ
        calc = RollingFlowCalculator(windows=[1, 5, 15])
        results = []
        last_tick = None
        
        for tick in provider.iter_ticks():
            metrics = calc.add_tick(tick, last_tick)
            
            true_change = (tick['lastPrice'] - pre_close) / pre_close * 100
            
            results.append({
                'time': datetime.fromtimestamp(int(tick['time']) / 1000).strftime('%H:%M:%S'),
                'price': tick['lastPrice'],
                'true_change_pct': true_change,
                'flow_1min': metrics.flow_1min.total_flow,
                'flow_5min': metrics.flow_5min.total_flow,
                'flow_15min': metrics.flow_15min.total_flow,
            })
            last_tick = tick
        
        if not results:
            raise Exception("æ— Tickæ•°æ®")
        
        df = pd.DataFrame(results)
        
        # 6. ä¿å­˜åŸå§‹æ•°æ®
        label_str = 'true' if label == 'çœŸèµ·çˆ†' else 'trap'
        output_file = output_dir / f"{code}_{date}_{label_str}.csv"
        df.to_csv(output_file, index=False)
        
        # 7. è®¡ç®—å…³é”®æŒ‡æ ‡
        max_flow = df['flow_5min'].max()
        final_change = df['true_change_pct'].iloc[-1]
        max_change = df['true_change_pct'].max()
        
        log_msg = f"  âœ… å®Œæˆ: æ¶¨å¹…{final_change:.2f}%, æœ€å¤§æ¶¨å¹…{max_change:.2f}%, 5minæµ{max_flow/1e6:.1f}M"
        print(log_msg)
        log_file.write(log_msg + "\n")
        
        return {
            'code': code,
            'name': name,
            'date': date,
            'label': label,
            'pre_close': pre_close,
            'final_change': final_change,
            'max_change': max_change,
            'max_flow_5min': max_flow,
            'tick_count': len(df),
            'status': 'success',
            'env_info': {
                'data_dir': env_info.get('data_dir'),
                'timestamp': datetime.now().isoformat()
            }
        }
        
    except Exception as e:
        error_msg = f"  âŒ å¤±è´¥: {str(e)}"
        print(error_msg)
        log_file.write(error_msg + "\n")
        return {
            'code': code,
            'name': name,
            'date': date,
            'label': label,
            'status': 'failed',
            'error': str(e)
        }


def run_pipeline(config_path: Path, output_dir: Path, log_dir: Path):
    """
    æ‰§è¡Œå®Œæ•´ç ”ç©¶æµæ°´çº¿
    """
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir.mkdir(parents=True, exist_ok=True)
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # æ‰“å¼€æ—¥å¿—æ–‡ä»¶
    log_file_path = log_dir / f"pipeline_{timestamp}.log"
    
    with open(log_file_path, 'w', encoding='utf-8') as log_file:
        # å†™å…¥å¤´ä¿¡æ¯
        header = f"""
{'='*80}
ç ”ç©¶æµæ°´çº¿æ‰§è¡Œæ—¥å¿—
å¼€å§‹æ—¶é—´: {datetime.now().isoformat()}
é…ç½®æ–‡ä»¶: {config_path}
è¾“å‡ºç›®å½•: {output_dir}
{'='*80}
"""
        print(header)
        log_file.write(header + "\n")
        
        # 1. ç¯å¢ƒæ£€æŸ¥
        print("\nã€1. ç¯å¢ƒè‡ªæ£€ã€‘")
        log_file.write("\nã€1. ç¯å¢ƒè‡ªæ£€ã€‘\n")
        passed, env_info = data_service.env_check()
        
        if not passed:
            print("âŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œç»ˆæ­¢æ‰§è¡Œ")
            log_file.write("âŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œç»ˆæ­¢æ‰§è¡Œ\n")
            return
        
        print(f"âœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡")
        print(f"  æ•°æ®ç›®å½•: {env_info.get('data_dir')}")
        print(f"  æ·±åœ³è‚¡ç¥¨: {env_info.get('sz_stock_count')}åª")
        print(f"  ä¸Šæµ·è‚¡ç¥¨: {env_info.get('sh_stock_count')}åª")
        
        log_file.write(f"âœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡\n")
        log_file.write(f"  æ•°æ®ç›®å½•: {env_info.get('data_dir')}\n")
        log_file.write(f"  æ·±åœ³è‚¡ç¥¨: {env_info.get('sz_stock_count')}åª\n")
        log_file.write(f"  ä¸Šæµ·è‚¡ç¥¨: {env_info.get('sh_stock_count')}åª\n")
        
        # 2. åŠ è½½æ ‡ç­¾
        print("\nã€2. åŠ è½½æ ‡ç­¾é…ç½®ã€‘")
        log_file.write("\nã€2. åŠ è½½æ ‡ç­¾é…ç½®ã€‘\n")
        
        samples = load_labels(config_path)
        total_cases = sum(len(s.get('dates', [])) for s in samples)
        
        print(f"æ ·æœ¬æ•°: {len(samples)}åª, æ¡ˆä¾‹æ•°: {total_cases}ä¸ª")
        log_file.write(f"æ ·æœ¬æ•°: {len(samples)}åª, æ¡ˆä¾‹æ•°: {total_cases}ä¸ª\n")
        
        # 3. æ‰§è¡Œåˆ†æ
        print("\nã€3. æ‰§è¡ŒTickå›æ”¾åˆ†æã€‘")
        log_file.write("\nã€3. æ‰§è¡ŒTickå›æ”¾åˆ†æã€‘\n")
        
        results = []
        success_count = 0
        failed_count = 0
        
        for sample in samples:
            code = sample['code']
            name = sample['name']
            
            for date_info in sample.get('dates', []):
                if isinstance(date_info, dict):
                    date = date_info['date']
                    label = date_info['label']
                else:
                    # è·³è¿‡æœªæ ‡æ³¨çš„
                    continue
                
                result = analyze_single_case(code, name, date, label, output_dir, log_file)
                results.append(result)
                
                if result['status'] == 'success':
                    success_count += 1
                else:
                    failed_count += 1
        
        # 4. ä¿å­˜æ±‡æ€»
        print("\nã€4. ç”Ÿæˆæ±‡æ€»æŠ¥å‘Šã€‘")
        log_file.write("\nã€4. ç”Ÿæˆæ±‡æ€»æŠ¥å‘Šã€‘\n")
        
        summary_df = pd.DataFrame([r for r in results if r['status'] == 'success'])
        if not summary_df.empty:
            summary_file = output_dir / f"analysis_summary_{timestamp}.csv"
            summary_df.to_csv(summary_file, index=False)
            print(f"âœ… æ±‡æ€»å·²ä¿å­˜: {summary_file}")
            log_file.write(f"âœ… æ±‡æ€»å·²ä¿å­˜: {summary_file}\n")
        
        # 5. ç»Ÿè®¡ä¿¡æ¯
        print("\nã€5. æ‰§è¡Œç»Ÿè®¡ã€‘")
        log_file.write("\nã€5. æ‰§è¡Œç»Ÿè®¡ã€‘\n")
        
        if not summary_df.empty and 'label' in summary_df.columns:
            for label in summary_df['label'].unique():
                subset = summary_df[summary_df['label'] == label]
                print(f"\nã€{label}ã€‘æ ·æœ¬æ•°: {len(subset)}")
                print(f"  å¹³å‡æ¶¨å¹…: {subset['final_change'].mean():.2f}%")
                print(f"  å¹³å‡5åˆ†é’Ÿæµ: {subset['max_flow_5min'].mean()/1e6:.1f}M")
                
                log_file.write(f"\nã€{label}ã€‘æ ·æœ¬æ•°: {len(subset)}\n")
                log_file.write(f"  å¹³å‡æ¶¨å¹…: {subset['final_change'].mean():.2f}%\n")
                log_file.write(f"  å¹³å‡5åˆ†é’Ÿæµ: {subset['max_flow_5min'].mean()/1e6:.1f}M\n")
        
        # ç»“å°¾
        footer = f"""
{'='*80}
æ‰§è¡Œå®Œæˆ
æˆåŠŸ: {success_count}ä¸ª, å¤±è´¥: {failed_count}ä¸ª
ç»“æŸæ—¶é—´: {datetime.now().isoformat()}
{'='*80}
"""
        print(footer)
        log_file.write(footer + "\n")
    
    print(f"\nğŸ“„ å®Œæ•´æ—¥å¿—: {log_file_path}")


def main():
    """
    ä¸»å‡½æ•°
    """
    import argparse
    
    parser = argparse.ArgumentParser(description='ç ”ç©¶æµæ°´çº¿æ€»é©±åŠ¨')
    parser.add_argument('--config', type=str, 
                        default='data/wanzhu_data/research_sample_config.json',
                        help='æ ‡ç­¾é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str,
                        default='data/wanzhu_data/samples',
                        help='è¾“å‡ºç›®å½•')
    parser.add_argument('--log', type=str,
                        default='logs/research_pipeline',
                        help='æ—¥å¿—ç›®å½•')
    
    args = parser.parse_args()
    
    config_path = PROJECT_ROOT / args.config
    output_dir = PROJECT_ROOT / args.output
    log_dir = PROJECT_ROOT / args.log
    
    if not config_path.exists():
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return
    
    run_pipeline(config_path, output_dir, log_dir)


if __name__ == "__main__":
    main()
