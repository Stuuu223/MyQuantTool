#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é¡½ä¸»æ¯HALFWAYç­–ç•¥åˆ†æå™¨

åˆ†æHALFWAYç­–ç•¥åœ¨é¡½ä¸»æ¯å¼ºåŠ¿è‚¡ä¸Šçš„è¡¨ç°ï¼š
1. ä»é¡½ä¸»æ¯å†å²æ’åæ•°æ®ä¸­æå–æ¯åªè‚¡ç¥¨çš„"é¦–æ¬¡ä¸Šæ¦œæ—¥æœŸ"
2. åœ¨é¦–æ¬¡ä¸Šæ¦œå‰çš„Nä¸ªäº¤æ˜“æ—¥è¿›è¡ŒHALFWAYå›æµ‹
3. ç»Ÿè®¡HALFWAYæå‰æ•æ‰åˆ°ä¿¡å·çš„æ¯”ä¾‹ã€æå‰å¤©æ•°ã€ç›ˆäºåˆ†å¸ƒ

Author: AI Project Director
Version: V1.0
Date: 2026-02-18
"""

import sys
import json
import csv
import random
import argparse
import requests
import time
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field
from collections import defaultdict
import pandas as pd
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from backtest.run_single_holding_t1_backtest import (
    SingleHoldingT1Backtester, HalfwaySignalAdapter, CostModel, T1BacktestResult
)
from logic.strategies.halfway_tick_strategy import HalfwayTickStrategy
from logic.utils.logger import get_logger

logger = get_logger(__name__)


@dataclass
class WanzhuStockInfo:
    """é¡½ä¸»æ¯è‚¡ç¥¨ä¿¡æ¯"""
    code: str
    name: str
    first_rank_date: str
    first_rank_pos: int
    first_rank_weight: float
    sector: str = ""


@dataclass
class HalfwayPreRankResult:
    """å•åªè‚¡ç¥¨å›æµ‹ç»“æœ"""
    stock_code: str
    stock_name: str
    first_rank_date: str
    backtest_start_date: str
    backtest_end_date: str
    days_before_rank: int
    
    # ä¿¡å·ç»Ÿè®¡
    has_signal: bool = False
    signal_date: Optional[str] = None
    signal_time: Optional[str] = None
    signal_price: Optional[float] = None
    signal_strength: Optional[float] = None
    days_ahead: Optional[int] = None  # æå‰å¤©æ•°
    
    # ç›ˆäºç»Ÿè®¡ï¼ˆå¦‚æœæœ‰ä¿¡å·ï¼‰
    entry_price: Optional[float] = None
    exit_price: Optional[float] = None
    exit_date: Optional[str] = None
    pnl: Optional[float] = None
    pnl_pct: Optional[float] = None
    exit_reason: Optional[str] = None
    
    # Rawä¿¡å·ç»Ÿè®¡
    raw_signal_count: int = 0
    executable_signal_count: int = 0
    
    # é”™è¯¯ä¿¡æ¯
    error: Optional[str] = None


class WanzhuDataGenerator:
    """é¡½ä¸»æ¯Mockæ•°æ®ç”Ÿæˆå™¨
    
    ç”±äºçœŸå®å†å²æ’åæ•°æ®å¯èƒ½ä¸å¯ç”¨ï¼Œç”Ÿæˆåˆç†çš„mockæ•°æ®ç”¨äºæµ‹è¯•
    """
    
    def __init__(self, start_date: str = "2025-01-01", end_date: str = "2025-12-31"):
        self.start_date = datetime.strptime(start_date, "%Y-%m-%d")
        self.end_date = datetime.strptime(end_date, "%Y-%m-%d")
        
    def generate_mock_history(
        self, 
        stock_list: List[Dict],
        output_path: Path,
        min_appearances: int = 1,
        max_appearances: int = 5
    ) -> pd.DataFrame:
        """ç”ŸæˆMockå†å²æ’åæ•°æ®
        
        Args:
            stock_list: è‚¡ç¥¨åˆ—è¡¨ [{code, name, sector}]
            output_path: è¾“å‡ºCSVè·¯å¾„
            min_appearances: æ¯åªè‚¡ç¥¨æœ€å°‘ä¸Šæ¦œæ¬¡æ•°
            max_appearances: æ¯åªè‚¡ç¥¨æœ€å¤šä¸Šæ¦œæ¬¡æ•°
            
        Returns:
            pd.DataFrame: ç”Ÿæˆçš„å†å²æ•°æ®
        """
        records = []
        
        # ç”Ÿæˆäº¤æ˜“æ—¥åˆ—è¡¨ï¼ˆæ’é™¤å‘¨æœ«ï¼‰
        trading_days = []
        current = self.start_date
        while current <= self.end_date:
            if current.weekday() < 5:  # å‘¨ä¸€åˆ°å‘¨äº”
                trading_days.append(current.strftime("%Y-%m-%d"))
            current += timedelta(days=1)
        
        logger.info(f"ç”ŸæˆMockæ•°æ®: {len(trading_days)}ä¸ªäº¤æ˜“æ—¥, {len(stock_list)}åªè‚¡ç¥¨")
        
        for stock in stock_list:
            code = stock['code']
            name = stock.get('name', '')
            sector = stock.get('sector', '')
            
            # éšæœºå†³å®šè¿™åªè‚¡ç¥¨ä¸Šæ¦œå‡ æ¬¡
            num_appearances = random.randint(min_appearances, max_appearances)
            
            # éšæœºé€‰æ‹©ä¸Šæ¦œæ—¥æœŸï¼ˆç¡®ä¿æ—¥æœŸé€’å¢ï¼‰
            if len(trading_days) >= num_appearances:
                rank_dates = sorted(random.sample(trading_days, num_appearances))
            else:
                rank_dates = trading_days[:num_appearances]
            
            for i, date in enumerate(rank_dates):
                # é¦–æ¬¡ä¸Šæ¦œæ’åæ›´é å‰ï¼Œåç»­å¯èƒ½æ³¢åŠ¨
                if i == 0:
                    rank = random.randint(1, 30)
                    weight = round(random.uniform(0.8, 1.0), 2)
                else:
                    rank = random.randint(10, 80)
                    weight = round(random.uniform(0.5, 0.9), 2)
                
                records.append({
                    'date': date,
                    'code': code,
                    'name': name,
                    'rank': rank,
                    'weight': weight,
                    'sector': sector
                })
        
        # æŒ‰æ—¥æœŸæ’åº
        df = pd.DataFrame(records)
        df = df.sort_values(['date', 'rank']).reset_index(drop=True)
        
        # ä¿å­˜CSV
        output_path.parent.mkdir(parents=True, exist_ok=True)
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        logger.info(f"Mockæ•°æ®å·²ä¿å­˜: {output_path} ({len(df)}æ¡è®°å½•)")
        
        return df


class WanzhuAPILoader:
    """é¡½ä¸»æ¯å®˜æ–¹APIæ•°æ®åŠ è½½å™¨
    
    ä»å®˜æ–¹APIè·å–å†å²æ’åæ•°æ®:
    https://www.hunanwanzhu.com/api/rankings?date=YYYY-MM-DD
    """
    
    def __init__(self, base_url: str = "https://www.hunanwanzhu.com/api/rankings"):
        self.base_url = base_url
        self.session = requests.Session()
        # è®¾ç½®è¯·æ±‚å¤´æ¨¡æ‹Ÿæµè§ˆå™¨
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
    def fetch_rankings_by_date(self, date_str: str) -> List[Dict]:
        """è·å–æŒ‡å®šæ—¥æœŸçš„æ’è¡Œæ¦œæ•°æ®
        
        Args:
            date_str: æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)
            
        Returns:
            List[Dict]: æ’åè®°å½•åˆ—è¡¨
        """
        url = f"{self.base_url}?date={date_str}"
        
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            # è§£æAPIè¿”å›çš„æ•°æ®
            # æ ¹æ®å®é™…APIç»“æ„è°ƒæ•´å­—æ®µæ˜ å°„
            records = []
            if isinstance(data, list):
                # å¦‚æœè¿”å›çš„æ˜¯åˆ—è¡¨æ ¼å¼
                for item in data:
                    record = {
                        'date': date_str,
                        'code': item.get('code') or item.get('stock_code'),
                        'name': item.get('name') or item.get('stock_name'),
                        'rank': item.get('rank'),
                        'weight': item.get('weight') or item.get('position_weight', 0),
                        'player_id': item.get('player_id') or item.get('user_id'),
                        'sector': item.get('sector', '')
                    }
                    records.append(record)
            elif isinstance(data, dict):
                # å¦‚æœè¿”å›çš„æ˜¯å­—å…¸æ ¼å¼ï¼Œæå–dataå­—æ®µ
                items = data.get('data', []) or data.get('rankings', []) or data.get('list', [])
                for item in items:
                    record = {
                        'date': date_str,
                        'code': item.get('code') or item.get('stock_code'),
                        'name': item.get('name') or item.get('stock_name'),
                        'rank': item.get('rank'),
                        'weight': item.get('weight') or item.get('position_weight', 0),
                        'player_id': item.get('player_id') or item.get('user_id'),
                        'sector': item.get('sector', '')
                    }
                    records.append(record)
            
            logger.info(f"è·å– {date_str} æ•°æ®: {len(records)}æ¡è®°å½•")
            return records
            
        except requests.exceptions.RequestException as e:
            logger.error(f"è·å– {date_str} æ•°æ®å¤±è´¥: {e}")
            return []
        except json.JSONDecodeError as e:
            logger.error(f"è§£æ {date_str} æ•°æ®å¤±è´¥: {e}")
            return []
    
    def fetch_date_range(
        self, 
        start_date: str, 
        end_date: str,
        delay_seconds: float = 0.5
    ) -> pd.DataFrame:
        """è·å–æ—¥æœŸèŒƒå›´å†…çš„æ‰€æœ‰æ’åæ•°æ®
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            delay_seconds: è¯·æ±‚é—´éš”ç§’æ•°(é¿å…è¯·æ±‚è¿‡å¿«)
            
        Returns:
            pd.DataFrame: æ‰€æœ‰æ—¥æœŸçš„æ’åæ•°æ®
        """
        start = datetime.strptime(start_date, "%Y-%m-%d")
        end = datetime.strptime(end_date, "%Y-%m-%d")
        
        all_records = []
        current = start
        
        while current <= end:
            date_str = current.strftime("%Y-%m-%d")
            records = self.fetch_rankings_by_date(date_str)
            all_records.extend(records)
            
            # å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            if delay_seconds > 0:
                time.sleep(delay_seconds)
            
            current += timedelta(days=1)
        
        if not all_records:
            logger.warning("æœªè·å–åˆ°ä»»ä½•æ•°æ®")
            return pd.DataFrame()
        
        df = pd.DataFrame(all_records)
        df = df.sort_values(['date', 'rank']).reset_index(drop=True)
        
        logger.info(f"è·å–å®Œæˆ: {len(df)}æ¡è®°å½•ï¼Œæ—¥æœŸèŒƒå›´ {start_date} è‡³ {end_date}")
        return df
    
    def save_to_csv(self, df: pd.DataFrame, output_path: Path):
        """ä¿å­˜æ•°æ®åˆ°CSV"""
        output_path.parent.mkdir(parents=True, exist_ok=True)
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        logger.info(f"æ•°æ®å·²ä¿å­˜: {output_path}")


class WanzhuDataLoader:
    """é¡½ä¸»æ¯æ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, history_csv_path: Optional[Path] = None):
        self.history_csv_path = history_csv_path
        self.history_df: Optional[pd.DataFrame] = None
        self.first_rank_dict: Dict[str, WanzhuStockInfo] = {}
        
    def load_from_csv(self, csv_path: Path) -> pd.DataFrame:
        """ä»CSVåŠ è½½å†å²æ’åæ•°æ®"""
        if not csv_path.exists():
            raise FileNotFoundError(f"å†å²æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        
        df = pd.read_csv(csv_path)
        required_cols = ['date', 'code', 'rank', 'weight']
        for col in required_cols:
            if col not in df.columns:
                raise ValueError(f"CSVç¼ºå°‘å¿…è¦åˆ—: {col}")
        
        self.history_df = df
        logger.info(f"åŠ è½½å†å²æ•°æ®: {len(df)}æ¡è®°å½•")
        return df
    
    def extract_first_rank_info(self) -> Dict[str, WanzhuStockInfo]:
        """æå–æ¯åªè‚¡ç¥¨çš„é¦–æ¬¡ä¸Šæ¦œä¿¡æ¯"""
        if self.history_df is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨load_from_csvåŠ è½½æ•°æ®")
        
        # æŒ‰codeåˆ†ç»„ï¼Œæ‰¾å‡ºæ¯åªè‚¡ç¥¨çš„æœ€æ—©ä¸Šæ¦œæ—¥æœŸ
        grouped = self.history_df.groupby('code').agg({
            'date': 'min',
            'rank': 'first',
            'weight': 'first',
            'name': 'first',
            'sector': 'first'
        }).reset_index()
        
        for _, row in grouped.iterrows():
            info = WanzhuStockInfo(
                code=row['code'],
                name=row.get('name', ''),
                first_rank_date=row['date'],
                first_rank_pos=int(row['rank']),
                first_rank_weight=float(row['weight']),
                sector=row.get('sector', '')
            )
            self.first_rank_dict[row['code']] = info
        
        logger.info(f"æå–é¦–æ¬¡ä¸Šæ¦œä¿¡æ¯: {len(self.first_rank_dict)}åªè‚¡ç¥¨")
        return self.first_rank_dict
    
    def get_stock_list(self) -> List[str]:
        """è·å–è‚¡ç¥¨ä»£ç åˆ—è¡¨"""
        return list(self.first_rank_dict.keys())


class WanzhuHalfwayAnalyzer:
    """é¡½ä¸»æ¯HALFWAYåˆ†æå™¨"""
    
    def __init__(
        self,
        lookback_days: int = 5,
        strategy_params: Optional[Dict] = None,
        initial_capital: float = 100000.0,
        position_size: float = 0.5,
        stop_loss_pct: float = 0.02,
        take_profit_pct: float = 0.05,
        max_holding_minutes: int = 120,
    ):
        self.lookback_days = lookback_days
        self.strategy_params = strategy_params or {
            'volatility_threshold': 0.02,
            'volume_surge': 1.2,
            'breakout_strength': 0.005,
            'window_minutes': 30,
            'min_history_points': 5
        }
        self.initial_capital = initial_capital
        self.position_size = position_size
        self.stop_loss_pct = stop_loss_pct
        self.take_profit_pct = take_profit_pct
        self.max_holding_minutes = max_holding_minutes
        
        self.results: List[HalfwayPreRankResult] = []
        
    def _get_trading_days_before(
        self, 
        target_date: str, 
        n_days: int
    ) -> Tuple[str, str]:
        """è·å–ç›®æ ‡æ—¥æœŸå‰Nä¸ªäº¤æ˜“æ—¥
        
        Returns:
            (start_date, end_date) - å›æµ‹çª—å£èµ·æ­¢æ—¥æœŸ
        """
        target = datetime.strptime(target_date, "%Y-%m-%d")
        
        # ç®€å•å¤„ç†ï¼šå¾€å‰æ¨N+2ä¸ªè‡ªç„¶æ—¥ï¼ˆç¡®ä¿åŒ…å«Nä¸ªäº¤æ˜“æ—¥ï¼‰
        start = target - timedelta(days=n_days + 5)
        
        return start.strftime("%Y-%m-%d"), target_date
    
    def _run_single_stock_backtest(
        self, 
        stock_info: WanzhuStockInfo
    ) -> HalfwayPreRankResult:
        """å¯¹å•åªè‚¡ç¥¨è¿›è¡Œå›æµ‹"""
        
        # è®¡ç®—å›æµ‹çª—å£
        start_date, end_date = self._get_trading_days_before(
            stock_info.first_rank_date, 
            self.lookback_days
        )
        
        # åˆ›å»ºç»“æœå¯¹è±¡
        result = HalfwayPreRankResult(
            stock_code=stock_info.code,
            stock_name=stock_info.name,
            first_rank_date=stock_info.first_rank_date,
            backtest_start_date=start_date,
            backtest_end_date=end_date,
            days_before_rank=self.lookback_days
        )
        
        logger.info(f"\nğŸ” å›æµ‹ {stock_info.code} ({stock_info.name})")
        logger.info(f"   é¦–æ¬¡ä¸Šæ¦œ: {stock_info.first_rank_date}")
        logger.info(f"   å›æµ‹çª—å£: {start_date} ~ {end_date}")
        
        try:
            # åˆ›å»ºHALFWAYç­–ç•¥
            halfway_strategy = HalfwayTickStrategy(self.strategy_params)
            signal_generator = HalfwaySignalAdapter(halfway_strategy)
            
            # åˆ›å»ºå›æµ‹å™¨
            backtester = SingleHoldingT1Backtester(
                initial_capital=self.initial_capital,
                position_size=self.position_size,
                stop_loss_pct=self.stop_loss_pct,
                take_profit_pct=self.take_profit_pct,
                max_holding_minutes=self.max_holding_minutes,
                signal_generator=signal_generator,
                cost_model=CostModel()
            )
            
            # è¿è¡Œå›æµ‹
            backtest_result = backtester.run_backtest(
                stock_codes=[stock_info.code],
                start_date=start_date,
                end_date=end_date
            )
            
            # ç»Ÿè®¡Rawä¿¡å·
            result.raw_signal_count = backtest_result.raw_signal_total
            result.executable_signal_count = backtest_result.executable_signal_total
            
            logger.info(f"   Rawä¿¡å·: {result.raw_signal_count}")
            logger.info(f"   Executableä¿¡å·: {result.executable_signal_count}")
            logger.info(f"   å®é™…æˆäº¤: {backtest_result.trade_total}")
            
            # æ£€æŸ¥æ˜¯å¦åœ¨é¦–æ¬¡ä¸Šæ¦œå‰æœ‰ä¿¡å·
            if backtest_result.raw_signal_trades:
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªRawä¿¡å·
                first_signal = backtest_result.raw_signal_trades[0]
                result.has_signal = True
                result.signal_date = first_signal.entry_date
                result.signal_time = first_signal.entry_time
                result.signal_price = first_signal.entry_price
                
                # è®¡ç®—æå‰å¤©æ•°
                signal_dt = datetime.strptime(first_signal.entry_date, "%Y-%m-%d")
                rank_dt = datetime.strptime(stock_info.first_rank_date, "%Y-%m-%d")
                result.days_ahead = (rank_dt - signal_dt).days
                
                logger.info(f"   âœ… æå‰{result.days_ahead}å¤©å‘ç°ä¿¡å·!")
                logger.info(f"      ä¿¡å·æ—¥æœŸ: {result.signal_date} {result.signal_time}")
                logger.info(f"      ä¿¡å·ä»·æ ¼: {result.signal_price}")
            else:
                logger.info(f"   âŒ æœªæå‰å‘ç°ä¿¡å·")
            
            # å¦‚æœæœ‰å®é™…æˆäº¤ï¼Œè®°å½•ç›ˆäº
            if backtest_result.t1_trades:
                trade = backtest_result.t1_trades[0]
                result.entry_price = trade.entry_price
                result.exit_price = trade.exit_price
                result.exit_date = trade.exit_date
                result.pnl = trade.pnl
                result.pnl_pct = trade.pnl_pct
                result.exit_reason = trade.exit_reason
                
                if trade.pnl is not None:
                    logger.info(f"   ç›ˆäº: {trade.pnl_pct*100:.2f}% ({trade.exit_reason})")
            
        except Exception as e:
            logger.error(f"å›æµ‹ {stock_info.code} å¤±è´¥: {e}")
            result.error = str(e)
        
        return result
    
    def analyze_stocks(
        self, 
        stock_infos: List[WanzhuStockInfo],
        max_stocks: Optional[int] = None
    ) -> List[HalfwayPreRankResult]:
        """æ‰¹é‡åˆ†æè‚¡ç¥¨"""
        
        stocks_to_analyze = stock_infos[:max_stocks] if max_stocks else stock_infos
        logger.info(f"å¼€å§‹åˆ†æ {len(stocks_to_analyze)} åªè‚¡ç¥¨...")
        
        for i, info in enumerate(stocks_to_analyze):
            logger.info(f"\n[{i+1}/{len(stocks_to_analyze)}] {info.code}")
            result = self._run_single_stock_backtest(info)
            self.results.append(result)
        
        return self.results
    
    def generate_report(self) -> Dict:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        
        total = len(self.results)
        if total == 0:
            return {"error": "æ²¡æœ‰åˆ†æç»“æœ"}
        
        # æœ‰ä¿¡å·çš„è‚¡ç¥¨
        with_signals = [r for r in self.results if r.has_signal]
        # æ— ä¿¡å·çš„è‚¡ç¥¨
        without_signals = [r for r in self.results if not r.has_signal and not r.error]
        # å‡ºé”™çš„è‚¡ç¥¨
        with_errors = [r for r in self.results if r.error]
        
        # è®¡ç®—æå‰å¤©æ•°ç»Ÿè®¡
        days_ahead_list = [r.days_ahead for r in with_signals if r.days_ahead is not None]
        avg_days_ahead = np.mean(days_ahead_list) if days_ahead_list else 0
        
        # è®¡ç®—ç›ˆäºç»Ÿè®¡ï¼ˆä»…é™æœ‰æˆäº¤çš„è‚¡ç¥¨ï¼‰
        completed_trades = [r for r in self.results if r.pnl is not None]
        winning_trades = [r for r in completed_trades if r.pnl and r.pnl > 0]
        losing_trades = [r for r in completed_trades if r.pnl and r.pnl < 0]
        
        win_rate = len(winning_trades) / len(completed_trades) * 100 if completed_trades else 0
        avg_pnl_pct = np.mean([r.pnl_pct for r in completed_trades if r.pnl_pct]) * 100 if completed_trades else 0
        
        report = {
            "summary": {
                "total_stocks_analyzed": total,
                "stocks_with_signals": len(with_signals),
                "stocks_without_signals": len(without_signals),
                "stocks_with_errors": len(with_errors),
                "signal_detection_rate": round(len(with_signals) / total * 100, 2),
                "avg_days_ahead": round(avg_days_ahead, 2),
                "lookback_window_days": self.lookback_days,
            },
            "performance": {
                "completed_trades": len(completed_trades),
                "winning_trades": len(winning_trades),
                "losing_trades": len(losing_trades),
                "win_rate_pct": round(win_rate, 2),
                "avg_pnl_pct": round(avg_pnl_pct, 2),
            },
            "data_quality": {
                "source": "api" if hasattr(self, '_use_api') and self._use_api else "csv/mock",
                "filter_applied": f"Top{args.min_rank}" if hasattr(args, 'min_rank') and args.min_rank > 0 else "none",
                "total_records_in_csv": len(loader.history_df) if 'loader' in locals() and loader.history_df is not None else 0,
            },
            "strategy_params": self.strategy_params,
            "details": [
                {
                    "stock_code": r.stock_code,
                    "stock_name": r.stock_name,
                    "first_rank_date": r.first_rank_date,
                    "has_signal": r.has_signal,
                    "signal_date": r.signal_date,
                    "days_ahead": r.days_ahead,
                    "pnl_pct": round(r.pnl_pct * 100, 2) if r.pnl_pct else None,
                    "exit_reason": r.exit_reason,
                    "error": r.error
                }
                for r in self.results
            ],
            "signals_detail": [
                {
                    "stock_code": r.stock_code,
                    "stock_name": r.stock_name,
                    "first_rank_date": r.first_rank_date,
                    "first_rank_pos": r.first_rank_pos if hasattr(r, 'first_rank_pos') else None,
                    "signal_date": r.signal_date,
                    "signal_time": r.signal_time,
                    "signal_price": r.signal_price,
                    "days_ahead": r.days_ahead,
                    "entry_price": r.entry_price,
                    "exit_price": r.exit_price,
                    "pnl_pct": round(r.pnl_pct * 100, 2) if r.pnl_pct else None,
                    "exit_reason": r.exit_reason,
                    "raw_signals": r.raw_signal_count,
                    "executable_signals": r.executable_signal_count
                }
                for r in with_signals
            ]
        }
        
        return report
    
    def save_report(self, output_path: Path):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        report = self.generate_report()
        
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"\nğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
        
        # åŒæ—¶ä¿å­˜CSVæ ¼å¼
        csv_path = output_path.with_suffix('.csv')
        self._save_csv_report(csv_path)
        
        return report
    
    def _save_csv_report(self, csv_path: Path):
        """ä¿å­˜CSVæ ¼å¼æŠ¥å‘Š"""
        with open(csv_path, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            writer.writerow([
                'è‚¡ç¥¨ä»£ç ', 'è‚¡ç¥¨åç§°', 'é¦–æ¬¡ä¸Šæ¦œæ—¥æœŸ', 'é¦–æ¬¡ä¸Šæ¦œæ’å',
                'æ˜¯å¦æœ‰ä¿¡å·', 'ä¿¡å·æ—¥æœŸ', 'ä¿¡å·æ—¶é—´', 'æå‰å¤©æ•°',
                'å…¥åœºä»·æ ¼', 'å‡ºåœºä»·æ ¼', 'ç›ˆäº%', 'å‡ºåœºåŸå› ',
                'Rawä¿¡å·æ•°', 'Executableä¿¡å·æ•°', 'é”™è¯¯ä¿¡æ¯'
            ])
            
            for r in self.results:
                writer.writerow([
                    r.stock_code,
                    r.stock_name,
                    r.first_rank_date,
                    getattr(r, 'first_rank_pos', ''),
                    'æ˜¯' if r.has_signal else 'å¦',
                    r.signal_date or '',
                    r.signal_time or '',
                    r.days_ahead if r.days_ahead is not None else '',
                    round(r.entry_price, 2) if r.entry_price else '',
                    round(r.exit_price, 2) if r.exit_price else '',
                    round(r.pnl_pct * 100, 2) if r.pnl_pct else '',
                    r.exit_reason or '',
                    r.raw_signal_count,
                    r.executable_signal_count,
                    r.error or ''
                ])
        
        logger.info(f"ğŸ’¾ CSVæŠ¥å‘Šå·²ä¿å­˜: {csv_path}")


def main():
    parser = argparse.ArgumentParser(description='é¡½ä¸»æ¯HALFWAYç­–ç•¥åˆ†æå™¨')
    parser.add_argument('--stocks-json', type=str, 
                        default='config/wanzhu_top_120.json',
                        help='é¡½ä¸»æ¯è‚¡ç¥¨åˆ—è¡¨JSONæ–‡ä»¶')
    parser.add_argument('--history-csv', type=str,
                        default='data/wanzhu_history_mock.csv',
                        help='é¡½ä¸»æ¯å†å²æ’åCSVæ–‡ä»¶ï¼ˆå¦‚ä¸å­˜åœ¨åˆ™ç”Ÿæˆmockæ•°æ®ï¼‰')
    parser.add_argument('--output', type=str,
                        default='backtest/results/wanzhu_halfway_analysis.json',
                        help='è¾“å‡ºæŠ¥å‘Šè·¯å¾„')
    parser.add_argument('--lookback-days', type=int, default=5,
                        help='å›æµ‹çª—å£ï¼šé¦–æ¬¡ä¸Šæ¦œå‰Nä¸ªäº¤æ˜“æ—¥')
    parser.add_argument('--max-stocks', type=int, default=None,
                        help='æœ€å¤šåˆ†æå¤šå°‘åªè‚¡ç¥¨ï¼ˆç”¨äºæµ‹è¯•ï¼‰')
    parser.add_argument('--generate-mock-only', action='store_true',
                        help='ä»…ç”Ÿæˆmockæ•°æ®ï¼Œä¸è¿è¡Œå›æµ‹')
    
    # V17: æ·»åŠ å®˜æ–¹APIæ•°æ®è·å–é€‰é¡¹
    parser.add_argument('--use-api', action='store_true',
                        help='ä½¿ç”¨å®˜æ–¹APIè·å–æ•°æ®ï¼ˆé»˜è®¤ä½¿ç”¨æœ¬åœ°CSVï¼‰')
    parser.add_argument('--api-start-date', type=str,
                        default='2025-11-01',
                        help='APIæ•°æ®è·å–å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--api-end-date', type=str,
                        default='2025-12-31',
                        help='APIæ•°æ®è·å–ç»“æŸæ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--api-delay', type=float, default=0.5,
                        help='APIè¯·æ±‚é—´éš”ç§’æ•°ï¼ˆé¿å…è¯·æ±‚è¿‡å¿«ï¼‰')
    parser.add_argument('--min-rank', type=int, default=10,
                        help='åªå¤„ç†æ’ååœ¨TopNä»¥å†…çš„è‚¡ç¥¨')
    
    args = parser.parse_args()
    
    # è·¯å¾„å¤„ç†
    stocks_json_path = PROJECT_ROOT / args.stocks_json
    history_csv_path = PROJECT_ROOT / args.history_csv
    output_path = PROJECT_ROOT / args.output
    
    logger.info("=" * 60)
    logger.info("é¡½ä¸»æ¯HALFWAYç­–ç•¥åˆ†æå™¨")
    logger.info("=" * 60)
    
    # 1. åŠ è½½è‚¡ç¥¨åˆ—è¡¨
    logger.info(f"\nğŸ“‹ åŠ è½½è‚¡ç¥¨åˆ—è¡¨: {stocks_json_path}")
    with open(stocks_json_path, 'r', encoding='utf-8') as f:
        stock_list = json.load(f)
    
    # ç¡®ä¿æ ¼å¼æ­£ç¡®
    if isinstance(stock_list, list) and len(stock_list) > 0:
        if isinstance(stock_list[0], str):
            # å¦‚æœæ˜¯ç®€å•å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºdictæ ¼å¼
            stock_list = [{'code': code, 'name': '', 'sector': ''} for code in stock_list]
    
    logger.info(f"åŠ è½½äº† {len(stock_list)} åªè‚¡ç¥¨")
    
    # 2. å‡†å¤‡å†å²æ•°æ®ï¼ˆæœ¬åœ°CSVæˆ–APIè·å–ï¼‰
    if args.use_api:
        # V17: ä½¿ç”¨å®˜æ–¹APIè·å–æ•°æ®
        logger.info(f"\nğŸŒ ä»å®˜æ–¹APIè·å–æ•°æ®...")
        logger.info(f"   æ—¥æœŸèŒƒå›´: {args.api_start_date} è‡³ {args.api_end_date}")
        logger.info(f"   è¯·æ±‚é—´éš”: {args.api_delay}ç§’")
        
        api_loader = WanzhuAPILoader()
        history_df = api_loader.fetch_date_range(
            start_date=args.api_start_date,
            end_date=args.api_end_date,
            delay_seconds=args.api_delay
        )
        
        if history_df.empty:
            logger.error("âŒ APIæœªè¿”å›æ•°æ®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–APIåœ°å€")
            logger.info("ğŸ’¡ æç¤º: å¯ä»¥ä½¿ç”¨ --use-api å‚æ•°åˆ‡æ¢åˆ°æœ¬åœ°CSVæ¨¡å¼")
            return
        
        # ä¿å­˜APIæ•°æ®åˆ°CSVï¼ˆç¼“å­˜ï¼‰
        api_loader.save_to_csv(history_df, history_csv_path)
        logger.info(f"âœ… APIæ•°æ®å·²ç¼“å­˜: {history_csv_path}")
        
    elif not history_csv_path.exists():
        logger.info(f"\nğŸ“ ç”ŸæˆMockå†å²æ•°æ®...")
        # ä½¿ç”¨ä¸QMTæ•°æ®åŒ¹é…çš„æ—¥æœŸèŒƒå›´ï¼ˆ2025å¹´11æœˆï¼‰
        generator = WanzhuDataGenerator(start_date="2025-11-01", end_date="2025-12-31")
        generator.generate_mock_history(
            stock_list=stock_list,
            output_path=history_csv_path,
            min_appearances=1,
            max_appearances=3
        )
    else:
        logger.info(f"\nğŸ“‚ ä½¿ç”¨å·²æœ‰å†å²æ•°æ®: {history_csv_path}")
    
    if args.generate_mock_only:
        logger.info("ä»…ç”Ÿæˆmockæ•°æ®ï¼Œé€€å‡º")
        return
    
    # 3. åŠ è½½å†å²æ•°æ®å¹¶æå–é¦–æ¬¡ä¸Šæ¦œä¿¡æ¯
    logger.info(f"\nğŸ“Š åŠ è½½å†å²æ’åæ•°æ®...")
    loader = WanzhuDataLoader()
    loader.load_from_csv(history_csv_path)
    first_rank_dict = loader.extract_first_rank_info()
    
    # V17: åº”ç”¨æ’åè¿‡æ»¤
    if args.min_rank > 0:
        logger.info(f"\nğŸ” åº”ç”¨æ’åè¿‡æ»¤: åªä¿ç•™Top{args.min_rank}")
        filtered_dict = {
            code: info for code, info in first_rank_dict.items()
            if info.first_rank_pos <= args.min_rank
        }
        logger.info(f"è¿‡æ»¤å‰: {len(first_rank_dict)}åªï¼Œè¿‡æ»¤å: {len(filtered_dict)}åª")
        first_rank_dict = filtered_dict
    
    # 4. è¿è¡Œåˆ†æ
    logger.info(f"\nğŸ¯ å¼€å§‹HALFWAYå›æµ‹åˆ†æ...")
    logger.info(f"   å›æµ‹çª—å£: é¦–æ¬¡ä¸Šæ¦œå‰ {args.lookback_days} ä¸ªäº¤æ˜“æ—¥")
    
    analyzer = WanzhuHalfwayAnalyzer(lookback_days=args.lookback_days)
    
    stock_infos = list(first_rank_dict.values())
    analyzer.analyze_stocks(stock_infos, max_stocks=args.max_stocks)
    
    # 5. ç”ŸæˆæŠ¥å‘Š
    report = analyzer.save_report(output_path)
    
    # 6. æ‰“å°æ‘˜è¦
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“ˆ åˆ†æç»“æœæ‘˜è¦")
    logger.info("=" * 60)
    
    summary = report['summary']
    performance = report['performance']
    
    logger.info(f"\næ€»ä½“ç»Ÿè®¡:")
    logger.info(f"  - åˆ†æè‚¡ç¥¨æ•°: {summary['total_stocks_analyzed']}")
    logger.info(f"  - æå‰å‘ç°ä¿¡å·: {summary['stocks_with_signals']} åª ({summary['signal_detection_rate']}%)")
    logger.info(f"  - å¹³å‡æå‰å¤©æ•°: {summary['avg_days_ahead']} å¤©")
    logger.info(f"  - æ•°æ®å‡ºé”™: {summary['stocks_with_errors']} åª")
    
    logger.info(f"\næˆäº¤ç»Ÿè®¡:")
    logger.info(f"  - å®Œæˆäº¤æ˜“: {performance['completed_trades']} ç¬”")
    logger.info(f"  - èƒœç‡: {performance['win_rate_pct']}%")
    logger.info(f"  - å¹³å‡ç›ˆäº: {performance['avg_pnl_pct']}%")
    
    logger.info("\n" + "=" * 60)
    logger.info("åˆ†æå®Œæˆ!")
    logger.info(f"è¯¦ç»†æŠ¥å‘Š: {output_path}")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()
