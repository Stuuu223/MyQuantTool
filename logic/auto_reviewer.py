"""
V14 AutoReviewer - è‡ªåŠ¨åŒ–æ¡ˆä¾‹æ”¶é›†ä¸å¤ç›˜ç³»ç»Ÿ

åŠŸèƒ½ï¼š
1. "æ‰“è„¸"æ¡ˆä¾‹é›†ï¼šç³»ç»Ÿè¯„åˆ†>85ä½†æ¬¡æ—¥è·Œå¹…>3%
2. "è¸ç©º"æ¡ˆä¾‹é›†ï¼šç³»ç»Ÿè¯„åˆ†<60ä½†ä»Šæ—¥æ¶¨åœ
3. "æ•‘å‘½"æ¡ˆä¾‹é›†ï¼šè¢«äº‹å®ç†”æ–­æŒ‰ä½ä½†æ¬¡æ—¥å¤§è·Œ
4. V14.3 æ¨¡å¼æ•è·ï¼ˆPattern Hunterï¼‰ï¼šåˆ†æè¸ç©ºæ¡ˆä¾‹çš„æ¨¡å¼ç‰¹å¾

ä½¿ç”¨ï¼š
æ¯å¤©15:30æ”¶ç›˜åè¿è¡Œï¼Œç”Ÿæˆã€Šæ¯æ—¥å¼‚å¸¸äº¤æ˜“æŠ¥å‘Šã€‹
"""

import pandas as pd
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Tuple, Optional
from pathlib import Path
from collections import Counter
from logic.data_manager import DataManager
from logic.signal_generator import get_signal_generator_v13
from logic.signal_history import get_signal_history_manager
from logic.logger import get_logger

logger = get_logger(__name__)


class AutoReviewer:
    """
    è‡ªåŠ¨æ¡ˆä¾‹æ”¶é›†å™¨
    """
    
    def __init__(self, data_manager: DataManager = None):
        """
        åˆå§‹åŒ–
        
        Args:
            data_manager: æ•°æ®ç®¡ç†å™¨å®ä¾‹
        """
        self.dm = data_manager or DataManager()
        self.sg = get_signal_generator_v13()
        self.shm = get_signal_history_manager()
        
        # åˆ›å»ºæ¡ˆä¾‹å­˜å‚¨ç›®å½•
        self.base_dir = Path("data/review_cases")
        self.slap_dir = self.base_dir / "false_positives"  # æ‰“è„¸æ¡ˆä¾‹
        self.missed_dir = self.base_dir / "missed_opportunities"  # è¸ç©ºæ¡ˆä¾‹
        self.lifesaver_dir = self.base_dir / "lifesavers"  # æ•‘å‘½æ¡ˆä¾‹
        
        for dir_path in [self.base_dir, self.slap_dir, self.missed_dir, self.lifesaver_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
    
    def collect_slap_cases(self, date: str = None) -> List[Dict]:
        """
        æ”¶é›†"æ‰“è„¸"æ¡ˆä¾‹ï¼šç³»ç»Ÿè¯„åˆ†>85ä½†æ¬¡æ—¥è·Œå¹…>3%
        
        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼YYYY-MM-DDï¼Œé»˜è®¤ä¸ºæ˜¨å¤©
        
        Returns:
            æ¡ˆä¾‹åˆ—è¡¨
        """
        if date is None:
            yesterday = datetime.now() - timedelta(days=1)
            date = yesterday.strftime("%Y-%m-%d")
        
        logger.info(f"æ­£åœ¨æ”¶é›† {date} çš„æ‰“è„¸æ¡ˆä¾‹...")
        
        # TODO: ä»æ•°æ®åº“æˆ–Redisè·å–å½“å¤©çš„BUYä¿¡å·åˆ—è¡¨
        # è¿™é‡Œéœ€è¦å®ç°å†å²ä¿¡å·å­˜å‚¨åŠŸèƒ½
        buy_signals = self._get_historical_buy_signals(date)
        
        slap_cases = []
        
        for signal in buy_signals:
            stock_code = signal['stock_code']
            score = signal['final_score']
            
            # è·å–ä»Šæ—¥æ•°æ®
            today_data = self.dm.get_realtime_data(stock_code)
            
            if today_data and 'change_percent' in today_data:
                change_pct = today_data['change_percent']
                
                # æ‰“è„¸æ¡ä»¶ï¼šç³»ç»Ÿè¯„åˆ†>85ä½†è·Œå¹…>3%
                if score > 85 and change_pct < -3:
                    case = {
                        'stock_code': stock_code,
                        'date': date,
                        'system_score': score,
                        'today_change': change_pct,
                        'signal_type': signal['signal'],
                        'reason': signal['reason'],
                        'fact_veto': signal.get('fact_veto', False)
                    }
                    slap_cases.append(case)
                    logger.warning(f"æ‰“è„¸æ¡ˆä¾‹: {stock_code} è¯„åˆ†{score} ä»Šæ—¥è·Œå¹…{change_pct:.2f}%")
        
        # ä¿å­˜æ¡ˆä¾‹
        if slap_cases:
            self._save_cases(slap_cases, self.slap_dir, f"slap_{date}.csv")
            logger.info(f"ä¿å­˜ {len(slap_cases)} ä¸ªæ‰“è„¸æ¡ˆä¾‹")
        
        return slap_cases
    
    def collect_missed_opportunities(self, date: str = None) -> List[Dict]:
        """
        æ”¶é›†"è¸ç©º"æ¡ˆä¾‹ï¼šç³»ç»Ÿè¯„åˆ†<60ä½†ä»Šæ—¥æ¶¨åœ
        
        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼YYYY-MM-DDï¼Œé»˜è®¤ä¸ºä»Šå¤©
        
        Returns:
            æ¡ˆä¾‹åˆ—è¡¨
        """
        if date is None:
            date = datetime.now().strftime("%Y-%m-%d")
        
        logger.info(f"æ­£åœ¨æ”¶é›† {date} çš„è¸ç©ºæ¡ˆä¾‹...")
        
        # è·å–ä»Šæ—¥æ¶¨åœæ¿åå•
        limit_up_stocks = self._get_limit_up_stocks(date)
        
        missed_cases = []
        
        for stock_code in limit_up_stocks:
            # è·å–å†å²æ•°æ®
            df = self.dm.get_history_data(symbol=stock_code)
            
            if df is None or len(df) < 2:
                continue
            
            # è·å–æ˜¨å¤©çš„æ•°æ®
            yesterday_data = df.iloc[-2]
            
            # è·å–èµ„é‡‘æµå‘
            capital_flow, market_cap = self.sg.get_capital_flow(stock_code, self.dm)
            
            # è·å–è¶‹åŠ¿
            trend = self.sg.get_trend_status(df.iloc[:-1])
            
            # è®¡ç®—ç³»ç»Ÿè¯„åˆ†ï¼ˆä½¿ç”¨é»˜è®¤AIåˆ†æ•°75ï¼‰
            result = self.sg.calculate_final_signal(
                stock_code=stock_code,
                ai_narrative_score=75,
                capital_flow_data=capital_flow,
                trend_status=trend,
                circulating_market_cap=market_cap
            )
            
            # è¸ç©ºæ¡ä»¶ï¼šç³»ç»Ÿè¯„åˆ†<60ä½†ä»Šæ—¥æ¶¨åœ
            if result['final_score'] < 60:
                case = {
                    'stock_code': stock_code,
                    'date': date,
                    'system_score': result['final_score'],
                    'today_status': 'LIMIT_UP',
                    'signal': result['signal'],
                    'reason': result['reason'],
                    'fact_veto': result.get('fact_veto', False),
                    'capital_flow': capital_flow,
                    'trend': trend
                }
                missed_cases.append(case)
                logger.warning(f"è¸ç©ºæ¡ˆä¾‹: {stock_code} è¯„åˆ†{result['final_score']:.1f} ä»Šæ—¥æ¶¨åœ")
        
        # ä¿å­˜æ¡ˆä¾‹
        if missed_cases:
            self._save_cases(missed_cases, self.missed_dir, f"missed_{date}.csv")
            logger.info(f"ä¿å­˜ {len(missed_cases)} ä¸ªè¸ç©ºæ¡ˆä¾‹")
        
        return missed_cases
    
    def collect_lifesaver_cases(self, date: str = None) -> List[Dict]:
        """
        æ”¶é›†"æ•‘å‘½"æ¡ˆä¾‹ï¼šè¢«äº‹å®ç†”æ–­æŒ‰ä½ä½†æ¬¡æ—¥å¤§è·Œ
        
        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼YYYY-MM-DDï¼Œé»˜è®¤ä¸ºä»Šå¤©
        
        Returns:
            æ¡ˆä¾‹åˆ—è¡¨
        """
        if date is None:
            date = datetime.now().strftime("%Y-%m-%d")
        
        logger.info(f"æ­£åœ¨æ”¶é›† {date} çš„æ•‘å‘½æ¡ˆä¾‹...")
        
        # TODO: ä»æ•°æ®åº“è·å–æ˜¨å¤©è¢«äº‹å®ç†”æ–­çš„è‚¡ç¥¨åˆ—è¡¨
        vetoed_stocks = self._get_fact_vetoed_stocks(date)
        
        lifesaver_cases = []
        
        for stock_data in vetoed_stocks:
            stock_code = stock_data['stock_code']
            ai_score = stock_data['ai_score']
            veto_reason = stock_data['veto_reason']
            
            # è·å–ä»Šæ—¥æ•°æ®
            today_data = self.dm.get_realtime_data(stock_code)
            
            if today_data and 'change_percent' in today_data:
                change_pct = today_data['change_percent']
                
                # æ•‘å‘½æ¡ä»¶ï¼šè¢«äº‹å®ç†”æ–­ä¸”ä»Šæ—¥è·Œå¹…>3%
                if change_pct < -3:
                    case = {
                        'stock_code': stock_code,
                        'date': date,
                        'ai_score': ai_score,
                        'veto_reason': veto_reason,
                        'today_change': change_pct,
                        'saved_loss': abs(change_pct)  # é¿å…çš„æŸå¤±
                    }
                    lifesaver_cases.append(case)
                    logger.info(f"æ•‘å‘½æ¡ˆä¾‹: {stock_code} AIè¯„åˆ†{ai_score} è¢«ç†”æ–­({veto_reason}) ä»Šæ—¥è·Œå¹…{change_pct:.2f}%")
        
        # ä¿å­˜æ¡ˆä¾‹
        if lifesaver_cases:
            self._save_cases(lifesaver_cases, self.lifesaver_dir, f"lifesaver_{date}.csv")
            logger.info(f"ä¿å­˜ {len(lifesaver_cases)} ä¸ªæ•‘å‘½æ¡ˆä¾‹")
        
        return lifesaver_cases
    
    def generate_daily_report(self, date: str = None) -> str:
        """
        ç”Ÿæˆæ¯æ—¥å¼‚å¸¸äº¤æ˜“æŠ¥å‘Š
        
        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼YYYY-MM-DDï¼Œé»˜è®¤ä¸ºä»Šå¤©
        
        Returns:
            æŠ¥å‘Šæ–‡æœ¬
        """
        if date is None:
            date = datetime.now().strftime("%Y-%m-%d")
        
        logger.info(f"æ­£åœ¨ç”Ÿæˆ {date} çš„æ¯æ—¥å¼‚å¸¸äº¤æ˜“æŠ¥å‘Š...")
        
        # æ”¶é›†ä¸‰ç±»æ¡ˆä¾‹
        slap_cases = self.collect_slap_cases(date)
        missed_cases = self.collect_missed_opportunities(date)
        lifesaver_cases = self.collect_lifesaver_cases(date)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"""
# æ¯æ—¥å¼‚å¸¸äº¤æ˜“æŠ¥å‘Š
æ—¥æœŸ: {date}
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

---

## ğŸ“Š ç»Ÿè®¡æ‘˜è¦
- æ‰“è„¸æ¡ˆä¾‹: {len(slap_cases)} ä¸ª
- è¸ç©ºæ¡ˆä¾‹: {len(missed_cases)} ä¸ª
- æ•‘å‘½æ¡ˆä¾‹: {len(lifesaver_cases)} ä¸ª

---

## ğŸš¨ æ‰“è„¸æ¡ˆä¾‹ï¼ˆç³»ç»Ÿè¯„åˆ†>85ä½†æ¬¡æ—¥è·Œå¹…>3%ï¼‰
"""
        
        if slap_cases:
            for i, case in enumerate(slap_cases, 1):
                report += f"""
### æ¡ˆä¾‹ {i}: {case['stock_code']}
- ç³»ç»Ÿè¯„åˆ†: {case['system_score']:.1f}
- ä»Šæ—¥è·Œå¹…: {case['today_change']:.2f}%
- ä¿¡å·ç±»å‹: {case['signal']}
- åŸå› : {case['reason']}
- äº‹å®ç†”æ–­: {case['fact_veto']}

"""
        else:
            report += "\næ— æ‰“è„¸æ¡ˆä¾‹ âœ…\n"
        
        report += """
---

## ğŸ’¨ è¸ç©ºæ¡ˆä¾‹ï¼ˆç³»ç»Ÿè¯„åˆ†<60ä½†ä»Šæ—¥æ¶¨åœï¼‰
"""
        
        if missed_cases:
            for i, case in enumerate(missed_cases, 1):
                report += f"""
### æ¡ˆä¾‹ {i}: {case['stock_code']}
- ç³»ç»Ÿè¯„åˆ†: {case['system_score']:.1f}
- ä»Šæ—¥çŠ¶æ€: {case['today_status']}
- ä¿¡å·: {case['signal']}
- åŸå› : {case['reason']}
- äº‹å®ç†”æ–­: {case['fact_veto']}
- èµ„é‡‘æµå‘: {case['capital_flow']/10000:.0f}ä¸‡
- è¶‹åŠ¿: {case['trend']}

"""
        else:
            report += "\næ— è¸ç©ºæ¡ˆä¾‹ âœ…\n"
        
        report += """
---

## ğŸ›¡ï¸ æ•‘å‘½æ¡ˆä¾‹ï¼ˆè¢«äº‹å®ç†”æ–­æŒ‰ä½ä½†æ¬¡æ—¥å¤§è·Œï¼‰
"""
        
        if lifesaver_cases:
            total_saved = sum(case['saved_loss'] for case in lifesaver_cases)
            report += f"\n**ç´¯è®¡é¿å…æŸå¤±: {total_saved:.2f}%**\n\n"
            
            for i, case in enumerate(lifesaver_cases, 1):
                report += f"""
### æ¡ˆä¾‹ {i}: {case['stock_code']}
- AIè¯„åˆ†: {case['ai_score']}
- ç†”æ–­åŸå› : {case['veto_reason']}
- ä»Šæ—¥è·Œå¹…: {case['today_change']:.2f}%
- é¿å…æŸå¤±: {case['saved_loss']:.2f}%

"""
        else:
            report += "\næ— æ•‘å‘½æ¡ˆä¾‹\n"
        
        report += f"""

---

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}*
*V14 AutoReviewer v1.0*
"""
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.base_dir / f"daily_report_{date}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"æŠ¥å‘Šå·²ä¿å­˜åˆ° {report_file}")
        
        return report
    
    def _get_historical_buy_signals(self, date: str) -> List[Dict]:
        """
        è·å–å†å²BUYä¿¡å·åˆ—è¡¨
        """
        buy_signals = self.shm.get_buy_signals_by_date(date)
        logger.info(f"ä»å†å²è®°å½•è·å–åˆ° {len(buy_signals)} ä¸ªBUYä¿¡å·")
        return buy_signals
    
    def _get_limit_up_stocks(self, date: str) -> List[str]:
        """
        è·å–æ¶¨åœæ¿è‚¡ç¥¨åˆ—è¡¨
        
        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼YYYY-MM-DD
        
        Returns:
            æ¶¨åœæ¿è‚¡ç¥¨ä»£ç åˆ—è¡¨
        """
        try:
            import akshare as ak
            import pandas as pd
            
            # æ ¼å¼è½¬æ¢ï¼š2026-01-18 -> 20260118
            date_str = date.replace('-', '')
            
            logger.info(f"æ­£åœ¨è·å– {date} çš„æ¶¨åœæ¿æ•°æ®...")
            
            # è·å–æ¶¨åœæ¿æ•°æ®
            df = ak.stock_zt_pool_em(date=date_str)
            
            if df is not None and not df.empty:
                # æå–è‚¡ç¥¨ä»£ç åˆ—è¡¨
                stock_codes = df['ä»£ç '].tolist()
                logger.info(f"æˆåŠŸè·å– {len(stock_codes)} åªæ¶¨åœæ¿è‚¡ç¥¨")
                return stock_codes
            else:
                logger.warning(f"{date} æ— æ¶¨åœæ¿æ•°æ®")
                return []
                
        except ImportError:
            logger.error("akshareæ¨¡å—æœªå®‰è£…ï¼Œæ— æ³•è·å–æ¶¨åœæ¿æ•°æ®")
            return []
        except Exception as e:
            logger.error(f"è·å–æ¶¨åœæ¿æ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _get_fact_vetoed_stocks(self, date: str) -> List[Dict]:
        """
        è·å–è¢«äº‹å®ç†”æ–­çš„è‚¡ç¥¨åˆ—è¡¨
        """
        vetoed_signals = self.shm.get_fact_vetoed_signals(date)
        
        # è½¬æ¢æ ¼å¼
        result = []
        for signal in vetoed_signals:
            result.append({
                'stock_code': signal['stock_code'],
                'ai_score': signal['ai_score'],
                'veto_reason': signal['reason']
            })
        
        logger.info(f"ä»å†å²è®°å½•è·å–åˆ° {len(result)} ä¸ªè¢«ç†”æ–­çš„ä¿¡å·")
        return result
    
    def _save_cases(self, cases: List[Dict], directory: Path, filename: str):
        """
        ä¿å­˜æ¡ˆä¾‹åˆ°CSVæ–‡ä»¶
        
        Args:
            cases: æ¡ˆä¾‹åˆ—è¡¨
            directory: ç›®å½•è·¯å¾„
            filename: æ–‡ä»¶å
        """
        if not cases:
            return
        
        df = pd.DataFrame(cases)
        filepath = directory / filename
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        logger.info(f"æ¡ˆä¾‹å·²ä¿å­˜åˆ° {filepath}")
    
    def analyze_missed_patterns(self, days: int = 5) -> Dict:
        """
        V14.3 æ¨¡å¼æ•è·ï¼šåˆ†æè¸ç©ºæ¡ˆä¾‹çš„æ¨¡å¼ç‰¹å¾
        
        åˆ†æç»´åº¦ï¼š
        1. å¸‚å€¼åˆ†å¸ƒï¼šå¾®ç›˜(<20äº¿)ã€ä¸­å°ç›˜(20-100äº¿)ã€å¤§ç›˜(>100äº¿)
        2. è¡Œä¸šåˆ†å¸ƒï¼šTop 3 çƒ­é—¨è¡Œä¸š
        3. é‡ä»·ç‰¹å¾ï¼šå¹³å‡æ¢æ‰‹ç‡å’Œé‡æ¯”
        4. æ—¶é—´åˆ†å¸ƒï¼šé¦–æ¿æ—¶é—´åˆ†å¸ƒ
        
        Args:
            days: åˆ†æè¿‡å»Nå¤©çš„æ•°æ®
        
        Returns:
            åˆ†æç»“æœå­—å…¸ï¼ŒåŒ…å«æ¨¡å¼å‘ç°å’Œä¼˜åŒ–å»ºè®®
        """
        logger.info(f"å¼€å§‹åˆ†æè¿‡å» {days} å¤©çš„è¸ç©ºæ¡ˆä¾‹æ¨¡å¼...")
        
        # 1. è¯»å–è¿‡å»Nå¤©çš„è¸ç©ºæ¡ˆä¾‹
        missed_cases = self._load_missed_cases(days)
        
        # åˆå§‹åŒ–é»˜è®¤ç»“æœç»“æ„
        default_result = {
            'total_cases': 0,
            'date_range': {
                'start': datetime.now().strftime("%Y-%m-%d"),
                'end': datetime.now().strftime("%Y-%m-%d")
            },
            'market_cap_distribution': {
                'micro_cap': {'count': 0, 'percentage': 0, 'avg_cap': 0},
                'small_mid_cap': {'count': 0, 'percentage': 0, 'avg_cap': 0},
                'large_cap': {'count': 0, 'percentage': 0, 'avg_cap': 0}
            },
            'industry_distribution': {
                'top_3': [],
                'total_industries': 0
            },
            'volume_price_features': {
                'turnover_rate': {'avg': 0, 'max': 0, 'min': 0},
                'volume_ratio': {'avg': 0, 'max': 0, 'min': 0}
            },
            'time_distribution': {
                'note': 'é¦–æ¿æ—¶é—´åˆ†æéœ€è¦å†å²Kçº¿æ•°æ®ï¼Œå½“å‰ç‰ˆæœ¬æš‚ä¸æ”¯æŒ',
                'suggestion': 'å»ºè®®åç»­ç‰ˆæœ¬å¢åŠ é¦–æ¿æ—¶é—´è¿½è¸ªåŠŸèƒ½'
            },
            'score_distribution': {
                'note': 'æ— è¯„åˆ†æ•°æ®'
            },
            'patterns': [],
            'recommendations': []
        }
        
        if not missed_cases:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°è¸ç©ºæ¡ˆä¾‹ï¼Œæ— æ³•è¿›è¡Œæ¨¡å¼åˆ†æ")
            default_result['recommendations'].append("âœ… æš‚æ— è¸ç©ºæ¡ˆä¾‹ï¼Œç³»ç»Ÿè¡¨ç°è‰¯å¥½")
            return default_result
        
        logger.info(f"å…±æ‰¾åˆ° {len(missed_cases)} ä¸ªè¸ç©ºæ¡ˆä¾‹")
        
        # 2. è·å–æ¯åªè‚¡ç¥¨çš„è¯¦ç»†ä¿¡æ¯ï¼ˆå¸‚å€¼ã€è¡Œä¸šã€é‡ä»·ï¼‰
        enriched_cases = self._enrich_cases_with_details(missed_cases)
        
        if not enriched_cases:
            logger.warning("æ— æ³•è·å–è‚¡ç¥¨è¯¦ç»†ä¿¡æ¯ï¼Œæ¨¡å¼åˆ†æå¤±è´¥")
            default_result['total_cases'] = len(missed_cases)
            default_result['recommendations'].append("âš ï¸ æ— æ³•è·å–è‚¡ç¥¨è¯¦ç»†ä¿¡æ¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
            return default_result
        
        # 3. è¿›è¡Œèšç±»åˆ†æ
        analysis_result = {
            'total_cases': len(enriched_cases),
            'date_range': {
                'start': min(c['date'] for c in enriched_cases),
                'end': max(c['date'] for c in enriched_cases)
            },
            'market_cap_distribution': self._analyze_market_cap(enriched_cases),
            'industry_distribution': self._analyze_industry(enriched_cases),
            'volume_price_features': self._analyze_volume_price(enriched_cases),
            'time_distribution': self._analyze_time_distribution(enriched_cases),
            'score_distribution': self._analyze_score_distribution(enriched_cases)
        }
        
        # 4. ç”Ÿæˆæ¨¡å¼å‘ç°å’Œä¼˜åŒ–å»ºè®®
        patterns, recommendations = self._generate_insights(analysis_result)
        
        analysis_result['patterns'] = patterns
        analysis_result['recommendations'] = recommendations
        
        # 5. ä¿å­˜åˆ†æç»“æœ
        self._save_pattern_analysis(analysis_result)
        
        return analysis_result
    
    def _load_missed_cases(self, days: int) -> List[Dict]:
        """
        åŠ è½½è¿‡å»Nå¤©çš„è¸ç©ºæ¡ˆä¾‹
        
        Args:
            days: å¤©æ•°
        
        Returns:
            æ¡ˆä¾‹åˆ—è¡¨
        """
        cases = []
        end_date = datetime.now()
        
        for i in range(days):
            date = (end_date - timedelta(days=i)).strftime("%Y-%m-%d")
            filepath = self.missed_dir / f"missed_{date}.csv"
            
            if filepath.exists():
                try:
                    df = pd.read_csv(filepath, encoding='utf-8-sig')
                    
                    # ç¡®ä¿è‚¡ç¥¨ä»£ç ä¿æŒå­—ç¬¦ä¸²æ ¼å¼
                    if 'stock_code' in df.columns:
                        df['stock_code'] = df['stock_code'].astype(str)
                    
                    case_list = df.to_dict('records')
                    cases.extend(case_list)
                    logger.info(f"åŠ è½½ {date} çš„è¸ç©ºæ¡ˆä¾‹: {len(case_list)} ä¸ª")
                except Exception as e:
                    logger.error(f"åŠ è½½ {filepath} å¤±è´¥: {e}")
        
        return cases
    
    def _enrich_cases_with_details(self, cases: List[Dict]) -> List[Dict]:
        """
        ä¸ºæ¡ˆä¾‹æ·»åŠ è¯¦ç»†ä¿¡æ¯ï¼ˆå¸‚å€¼ã€è¡Œä¸šã€é‡ä»·ï¼‰
        
        Args:
            cases: åŸå§‹æ¡ˆä¾‹åˆ—è¡¨
        
        Returns:
            å¢å¼ºåçš„æ¡ˆä¾‹åˆ—è¡¨
        """
        enriched = []
        
        for case in cases:
            stock_code = case['stock_code']
            
            try:
                # è·å–è‚¡ç¥¨è¯¦ç»†ä¿¡æ¯
                stock_details = self._get_stock_details(stock_code)
                
                if stock_details:
                    enriched_case = {**case, **stock_details}
                    enriched.append(enriched_case)
                else:
                    logger.warning(f"æ— æ³•è·å– {stock_code} çš„è¯¦ç»†ä¿¡æ¯")
                    
            except Exception as e:
                logger.error(f"è·å– {stock_code} è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")
        
        return enriched
    
    def _get_stock_details(self, stock_code: str) -> Optional[Dict]:
        """
        è·å–è‚¡ç¥¨è¯¦ç»†ä¿¡æ¯ï¼ˆå¸‚å€¼ã€è¡Œä¸šã€é‡ä»·ï¼‰
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
        
        Returns:
            è‚¡ç¥¨è¯¦ç»†ä¿¡æ¯å­—å…¸
        """
        try:
            import akshare as ak
            
            # è·å–ä¸ªè‚¡ä¿¡æ¯
            stock_info = ak.stock_individual_info_em(symbol=stock_code)
            
            if stock_info is None or stock_info.empty:
                return None
            
            # è½¬æ¢ä¸ºå­—å…¸
            info_dict = stock_info.set_index('item')['value'].to_dict()
            
            # æå–å…³é”®ä¿¡æ¯
            details = {
                'market_cap': self._parse_market_cap(info_dict.get('æ€»å¸‚å€¼', '0')),
                'circulating_cap': self._parse_market_cap(info_dict.get('æµé€šå¸‚å€¼', '0')),
                'industry': info_dict.get('æ‰€å±è¡Œä¸š', 'æœªçŸ¥'),
                'concept': info_dict.get('æ¦‚å¿µ', ''),
                'pe_ratio': self._parse_float(info_dict.get('å¸‚ç›ˆç‡-åŠ¨æ€', '0')),
                'pb_ratio': self._parse_float(info_dict.get('å¸‚å‡€ç‡', '0'))
            }
            
            # è·å–å®æ—¶è¡Œæƒ…æ•°æ®ï¼ˆé‡ä»·ä¿¡æ¯ï¼‰
            realtime_data = self.dm.get_realtime_data(stock_code)
            
            if realtime_data:
                details.update({
                    'turnover_rate': realtime_data.get('turnover_rate', 0),
                    'volume_ratio': realtime_data.get('volume_ratio', 0),
                    'current_price': realtime_data.get('current', 0),
                    'change_percent': realtime_data.get('change_percent', 0)
                })
            
            return details
            
        except ImportError:
            logger.error("akshareæ¨¡å—æœªå®‰è£…")
            return None
        except Exception as e:
            logger.error(f"è·å– {stock_code} è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def _parse_market_cap(self, value_str: str) -> float:
        """
        è§£æå¸‚å€¼å­—ç¬¦ä¸²
        
        Args:
            value_str: å¸‚å€¼å­—ç¬¦ä¸²ï¼Œå¦‚ "123.45äº¿"
        
        Returns:
            å¸‚å€¼ï¼ˆäº¿å…ƒï¼‰
        """
        try:
            if isinstance(value_str, str):
                # ç§»é™¤æ‰€æœ‰éæ•°å­—å­—ç¬¦ï¼ˆä¿ç•™å°æ•°ç‚¹ï¼‰
                import re
                num_str = re.sub(r'[^\d.]', '', value_str)
                return float(num_str) if num_str else 0.0
            elif isinstance(value_str, (int, float)):
                return float(value_str)
            else:
                return 0.0
        except:
            return 0.0
    
    def _parse_float(self, value_str: str) -> float:
        """
        è§£ææµ®ç‚¹æ•°å­—ç¬¦ä¸²
        
        Args:
            value_str: æµ®ç‚¹æ•°å­—ç¬¦ä¸²
        
        Returns:
            æµ®ç‚¹æ•°
        """
        try:
            if isinstance(value_str, str):
                return float(value_str)
            elif isinstance(value_str, (int, float)):
                return float(value_str)
            else:
                return 0.0
        except:
            return 0.0
    
    def _analyze_market_cap(self, cases: List[Dict]) -> Dict:
        """
        åˆ†æå¸‚å€¼åˆ†å¸ƒ
        
        Args:
            cases: æ¡ˆä¾‹åˆ—è¡¨
        
        Returns:
            å¸‚å€¼åˆ†å¸ƒç»Ÿè®¡
        """
        micro_cap = []  # < 20äº¿
        small_mid_cap = []  # 20-100äº¿
        large_cap = []  # > 100äº¿
        
        for case in cases:
            cap = case.get('market_cap', 0)
            
            if cap < 20:
                micro_cap.append(cap)
            elif cap < 100:
                small_mid_cap.append(cap)
            else:
                large_cap.append(cap)
        
        total = len(cases)
        
        return {
            'micro_cap': {
                'count': len(micro_cap),
                'percentage': len(micro_cap) / total * 100 if total > 0 else 0,
                'avg_cap': sum(micro_cap) / len(micro_cap) if micro_cap else 0
            },
            'small_mid_cap': {
                'count': len(small_mid_cap),
                'percentage': len(small_mid_cap) / total * 100 if total > 0 else 0,
                'avg_cap': sum(small_mid_cap) / len(small_mid_cap) if small_mid_cap else 0
            },
            'large_cap': {
                'count': len(large_cap),
                'percentage': len(large_cap) / total * 100 if total > 0 else 0,
                'avg_cap': sum(large_cap) / len(large_cap) if large_cap else 0
            }
        }
    
    def _analyze_industry(self, cases: List[Dict]) -> Dict:
        """
        åˆ†æè¡Œä¸šåˆ†å¸ƒ
        
        Args:
            cases: æ¡ˆä¾‹åˆ—è¡¨
        
        Returns:
            è¡Œä¸šåˆ†å¸ƒç»Ÿè®¡
        """
        industries = [case.get('industry', 'æœªçŸ¥') for case in cases]
        industry_counter = Counter(industries)
        
        # è·å–Top 3è¡Œä¸š
        top_3 = industry_counter.most_common(3)
        
        total = len(cases)
        
        return {
            'top_3': [
                {
                    'industry': ind,
                    'count': count,
                    'percentage': count / total * 100
                }
                for ind, count in top_3
            ],
            'total_industries': len(industry_counter)
        }
    
    def _analyze_volume_price(self, cases: List[Dict]) -> Dict:
        """
        åˆ†æé‡ä»·ç‰¹å¾
        
        Args:
            cases: æ¡ˆä¾‹åˆ—è¡¨
        
        Returns:
            é‡ä»·ç‰¹å¾ç»Ÿè®¡
        """
        turnover_rates = [case.get('turnover_rate', 0) for case in cases if case.get('turnover_rate')]
        volume_ratios = [case.get('volume_ratio', 0) for case in cases if case.get('volume_ratio')]
        
        return {
            'turnover_rate': {
                'avg': sum(turnover_rates) / len(turnover_rates) if turnover_rates else 0,
                'max': max(turnover_rates) if turnover_rates else 0,
                'min': min(turnover_rates) if turnover_rates else 0
            },
            'volume_ratio': {
                'avg': sum(volume_ratios) / len(volume_ratios) if volume_ratios else 0,
                'max': max(volume_ratios) if volume_ratios else 0,
                'min': min(volume_ratios) if volume_ratios else 0
            }
        }
    
    def _analyze_time_distribution(self, cases: List[Dict]) -> Dict:
        """
        åˆ†ææ—¶é—´åˆ†å¸ƒï¼ˆé¦–æ¿æ—¶é—´ï¼‰
        
        æ³¨æ„ï¼šå½“å‰æ•°æ®ä¸­æ²¡æœ‰é¦–æ¿æ—¶é—´ä¿¡æ¯ï¼Œè¿”å›å ä½ç¬¦
        æœªæ¥å¯ä»¥ä»å†å²Kçº¿æ•°æ®ä¸­æå–é¦–æ¿æ—¶é—´
        
        Args:
            cases: æ¡ˆä¾‹åˆ—è¡¨
        
        Returns:
            æ—¶é—´åˆ†å¸ƒç»Ÿè®¡
        """
        # TODO: ä»å†å²Kçº¿æ•°æ®ä¸­æå–é¦–æ¿æ—¶é—´
        return {
            'note': 'é¦–æ¿æ—¶é—´åˆ†æéœ€è¦å†å²Kçº¿æ•°æ®ï¼Œå½“å‰ç‰ˆæœ¬æš‚ä¸æ”¯æŒ',
            'suggestion': 'å»ºè®®åç»­ç‰ˆæœ¬å¢åŠ é¦–æ¿æ—¶é—´è¿½è¸ªåŠŸèƒ½'
        }
    
    def _analyze_score_distribution(self, cases: List[Dict]) -> Dict:
        """
        åˆ†æç³»ç»Ÿè¯„åˆ†åˆ†å¸ƒ
        
        Args:
            cases: æ¡ˆä¾‹åˆ—è¡¨
        
        Returns:
            è¯„åˆ†åˆ†å¸ƒç»Ÿè®¡
        """
        scores = [case.get('system_score', 0) for case in cases if case.get('system_score') is not None]
        
        if not scores:
            return {'note': 'æ— è¯„åˆ†æ•°æ®'}
        
        return {
            'avg': sum(scores) / len(scores),
            'max': max(scores),
            'min': min(scores),
            'distribution': {
                'very_low': len([s for s in scores if s < 40]),
                'low': len([s for s in scores if 40 <= s < 50]),
                'medium': len([s for s in scores if 50 <= s < 60]),
                'high': len([s for s in scores if s >= 60])
            }
        }
    
    def _generate_insights(self, analysis: Dict) -> Tuple[List[Dict], List[str]]:
        """
        ç”Ÿæˆæ¨¡å¼å‘ç°å’Œä¼˜åŒ–å»ºè®®
        
        Args:
            analysis: åˆ†æç»“æœ
        
        Returns:
            (æ¨¡å¼å‘ç°åˆ—è¡¨, ä¼˜åŒ–å»ºè®®åˆ—è¡¨)
        """
        patterns = []
        recommendations = []
        
        total = analysis['total_cases']
        threshold_ratio = 0.6  # 60%é˜ˆå€¼
        
        # 1. å¸‚å€¼æ¨¡å¼åˆ†æ
        market_cap = analysis['market_cap_distribution']
        if market_cap['micro_cap']['percentage'] > threshold_ratio * 100:
            patterns.append({
                'type': 'å¸‚å€¼',
                'pattern': 'å¾®ç›˜è‚¡åå¥½',
                'description': f"è¸ç©ºæ¡ˆä¾‹ä¸­ {market_cap['micro_cap']['percentage']:.1f}% ä¸ºå¾®ç›˜è‚¡ï¼ˆ<20äº¿ï¼‰"
            })
            recommendations.append(
                f"âš ï¸ å‘ç°æ–°æ¨¡å¼ï¼šå¾®ç›˜è‚¡è¸ç©ºç‡é«˜ ({market_cap['micro_cap']['percentage']:.1f}%)ã€‚"
                f"å»ºè®®é™ä½å°å¸‚å€¼è‚¡ç¥¨çš„èµ„é‡‘æµå‡ºæƒ©ç½šé˜ˆå€¼ã€‚"
            )
        
        if market_cap['large_cap']['percentage'] > threshold_ratio * 100:
            patterns.append({
                'type': 'å¸‚å€¼',
                'pattern': 'å¤§ç›˜è‚¡åå¥½',
                'description': f"è¸ç©ºæ¡ˆä¾‹ä¸­ {market_cap['large_cap']['percentage']:.1f}% ä¸ºå¤§ç›˜è‚¡ï¼ˆ>100äº¿ï¼‰"
            })
            recommendations.append(
                f"âš ï¸ å‘ç°æ–°æ¨¡å¼ï¼šå¤§ç›˜è‚¡è¸ç©ºç‡é«˜ ({market_cap['large_cap']['percentage']:.1f}%)ã€‚"
                f"å»ºè®®å¢åŠ å¤§ç›˜è‚¡çš„è¶‹åŠ¿æƒé‡ã€‚"
            )
        
        # 2. è¡Œä¸šæ¨¡å¼åˆ†æ
        industry = analysis['industry_distribution']
        if industry['top_3']:
            top_industry = industry['top_3'][0]
            if top_industry['percentage'] > threshold_ratio * 100:
                patterns.append({
                    'type': 'è¡Œä¸š',
                    'pattern': f'{top_industry["industry"]}æ¿å—é›†ä¸­',
                    'description': f"è¸ç©ºæ¡ˆä¾‹ä¸­ {top_industry['percentage']:.1f}% å±äº {top_industry['industry']} æ¿å—"
                })
                recommendations.append(
                    f"âš ï¸ å‘ç°æ–°æ¨¡å¼ï¼š{top_industry['industry']}æ¿å—è¸ç©ºç‡é«˜ ({top_industry['percentage']:.1f}%)ã€‚"
                    f"å»ºè®®è°ƒé«˜è¯¥æ¿å—çš„çƒ­åº¦æƒé‡ã€‚"
                )
        
        # 3. é‡ä»·ç‰¹å¾åˆ†æ
        volume_price = analysis['volume_price_features']
        if volume_price['turnover_rate']['avg'] > 10:
            patterns.append({
                'type': 'é‡ä»·',
                'pattern': 'é«˜æ¢æ‰‹ç‡',
                'description': f"è¸ç©ºæ¡ˆä¾‹å¹³å‡æ¢æ‰‹ç‡ä¸º {volume_price['turnover_rate']['avg']:.2f}%"
            })
            recommendations.append(
                f"âš ï¸ å‘ç°æ–°æ¨¡å¼ï¼šè¸ç©ºè‚¡ç¥¨å¹³å‡æ¢æ‰‹ç‡è¾ƒé«˜ ({volume_price['turnover_rate']['avg']:.2f}%)ã€‚"
                f"å»ºè®®å¢åŠ æ¢æ‰‹ç‡å› å­çš„æƒé‡ã€‚"
            )
        
        # 4. è¯„åˆ†åˆ†å¸ƒåˆ†æ
        score_dist = analysis['score_distribution']
        if 'avg' in score_dist:
            if score_dist['avg'] < 45:
                patterns.append({
                    'type': 'è¯„åˆ†',
                    'pattern': 'ä½è¯„åˆ†è¸ç©º',
                    'description': f"è¸ç©ºæ¡ˆä¾‹å¹³å‡ç³»ç»Ÿè¯„åˆ†ä¸º {score_dist['avg']:.1f}"
                })
                recommendations.append(
                    f"âš ï¸ å‘ç°æ–°æ¨¡å¼ï¼šè¸ç©ºè‚¡ç¥¨å¹³å‡è¯„åˆ†è¾ƒä½ ({score_dist['avg']:.1f})ã€‚"
                    f"å»ºè®®æ£€æŸ¥è¯„åˆ†ç®—æ³•æ˜¯å¦è¿‡äºä¿å®ˆã€‚"
                )
        
        # å¦‚æœæ²¡æœ‰å‘ç°æ˜æ˜¾æ¨¡å¼
        if not patterns:
            patterns.append({
                'type': 'é€šç”¨',
                'pattern': 'æ— æ˜æ˜¾æ¨¡å¼',
                'description': f"è¿‡å» {analysis['total_cases']} ä¸ªè¸ç©ºæ¡ˆä¾‹åˆ†å¸ƒè¾ƒä¸ºå‡åŒ€"
            })
            recommendations.append("âœ… æœªå‘ç°æ˜æ˜¾æ¨¡å¼ï¼Œå½“å‰ç­–ç•¥è¾ƒä¸ºå‡è¡¡ã€‚")
        
        return patterns, recommendations
    
    def _save_pattern_analysis(self, analysis: Dict):
        """
        ä¿å­˜æ¨¡å¼åˆ†æç»“æœ
        
        Args:
            analysis: åˆ†æç»“æœ
        """
        try:
            # ä¿å­˜ä¸ºJSON
            import json
            analysis_file = self.base_dir / "pattern_analysis.json"
            with open(analysis_file, 'w', encoding='utf-8') as f:
                json.dump(analysis, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æ¨¡å¼åˆ†æç»“æœå·²ä¿å­˜åˆ° {analysis_file}")
            
            # ç”ŸæˆMarkdownæŠ¥å‘Š
            self._generate_pattern_report(analysis)
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å¼åˆ†æç»“æœå¤±è´¥: {e}")
    
    def _generate_pattern_report(self, analysis: Dict):
        """
        ç”Ÿæˆæ¨¡å¼åˆ†æMarkdownæŠ¥å‘Š
        
        Args:
            analysis: åˆ†æç»“æœ
        """
        report = f"""
# V14.3 æ¨¡å¼æ•è·åˆ†ææŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## ğŸ“Š åˆ†ææ‘˜è¦

- **åˆ†æå‘¨æœŸ**: {analysis['date_range']['start']} è‡³ {analysis['date_range']['end']}
- **è¸ç©ºæ¡ˆä¾‹æ€»æ•°**: {analysis['total_cases']} ä¸ª

---

## ğŸ¯ æ¨¡å¼å‘ç°

"""
        
        for pattern in analysis['patterns']:
            report += f"""
### {pattern['type']} - {pattern['pattern']}
{pattern['description']}

"""
        
        report += """
---

## ğŸ’¡ ä¼˜åŒ–å»ºè®®

"""
        
        for rec in analysis['recommendations']:
            report += f"{rec}\n\n"
        
        report += """
---

## ğŸ“ˆ è¯¦ç»†æ•°æ®

### å¸‚å€¼åˆ†å¸ƒ
- **å¾®ç›˜è‚¡ (<20äº¿)**: {micro_count} ä¸ª ({micro_pct:.1f}%)
- **ä¸­å°ç›˜è‚¡ (20-100äº¿)**: {small_count} ä¸ª ({small_pct:.1f}%)
- **å¤§ç›˜è‚¡ (>100äº¿)**: {large_count} ä¸ª ({large_pct:.1f}%)

### è¡Œä¸šåˆ†å¸ƒ (Top 3)
""".format(
            micro_count=analysis['market_cap_distribution']['micro_cap']['count'],
            micro_pct=analysis['market_cap_distribution']['micro_cap']['percentage'],
            small_count=analysis['market_cap_distribution']['small_mid_cap']['count'],
            small_pct=analysis['market_cap_distribution']['small_mid_cap']['percentage'],
            large_count=analysis['market_cap_distribution']['large_cap']['count'],
            large_pct=analysis['market_cap_distribution']['large_cap']['percentage']
        )
        
        for ind in analysis['industry_distribution']['top_3']:
            report += f"- **{ind['industry']}**: {ind['count']} ä¸ª ({ind['percentage']:.1f}%)\n"
        
        report += f"""
### é‡ä»·ç‰¹å¾
- **å¹³å‡æ¢æ‰‹ç‡**: {analysis['volume_price_features']['turnover_rate']['avg']:.2f}%
- **å¹³å‡é‡æ¯”**: {analysis['volume_price_features']['volume_ratio']['avg']:.2f}

### è¯„åˆ†åˆ†å¸ƒ
"""
        
        if 'avg' in analysis['score_distribution']:
            score_dist = analysis['score_distribution']
            report += f"""
- **å¹³å‡è¯„åˆ†**: {score_dist['avg']:.1f}
- **æœ€é«˜è¯„åˆ†**: {score_dist['max']:.1f}
- **æœ€ä½è¯„åˆ†**: {score_dist['min']:.1f}
"""
        
        report += f"""

---

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
*V14.3 Pattern Hunter v1.0*
"""
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.base_dir / "pattern_analysis_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"æ¨¡å¼åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ° {report_file}")


def run_daily_review(date: str = None):
    """
    è¿è¡Œæ¯æ—¥å¤ç›˜
    
    Args:
        date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼YYYY-MM-DDï¼Œé»˜è®¤ä¸ºä»Šå¤©
    """
    logger.info("="*60)
    logger.info("V14 AutoReviewer æ¯æ—¥å¤ç›˜å¼€å§‹")
    logger.info("="*60)
    
    try:
        reviewer = AutoReviewer()
        report = reviewer.generate_daily_report(date)
        
        print(report)
        logger.info("æ¯æ—¥å¤ç›˜å®Œæˆ")
        
    except Exception as e:
        logger.error(f"æ¯æ—¥å¤ç›˜å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    # è¿è¡Œæ¯æ—¥å¤ç›˜
    run_daily_review()
