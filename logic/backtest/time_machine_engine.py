"""
å…¨æ¯æ—¶é—´æœºå™¨å¼•æ“ - è¿ç»­å¤šæ—¥å›æµ‹
è‡ªåŠ¨æ‰§è¡Œè¿ç»­Nä¸ªäº¤æ˜“æ—¥çš„å›æµ‹ï¼ŒéªŒè¯ç­–ç•¥ç¨³å®šæ€§

Author: iFlow CLI
Date: 2026-02-23
Version: 1.1.0 - æ·»åŠ è®°å¿†è¡°å‡æœºåˆ¶
"""
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple, Set
from pathlib import Path
import json
import logging

from logic.core.path_resolver import PathResolver

# è®°å¿†è¡°å‡å‚æ•°
MEMORY_DECAY_FACTOR = 0.5      # è¡°å‡ç³»æ•°
MEMORY_MIN_SCORE = 10.0        # æœ€ä½åˆ†æ•°é˜ˆå€¼
MEMORY_MAX_ABSENCE_DAYS = 2    # è¿ç»­ä¸ä¸Šæ¦œæœ€å¤§å¤©æ•°
from logic.core.metric_definitions import MetricDefinitions
from logic.core.sanity_guards import SanityGuards
from logic.data_providers.qmt_manager import QmtDataManager
from logic.data_providers.universe_builder import UniverseBuilder

logger = logging.getLogger(__name__)


class TimeMachineEngine:
    """
    å…¨æ¯æ—¶é—´æœºå™¨ - è¿ç»­äº¤æ˜“æ—¥å›æµ‹å¼•æ“
    
    ä½¿ç”¨ç¤ºä¾‹:
        engine = TimeMachineEngine()
        results = engine.run_continuous_backtest(
            start_date='20251231',
            end_date='20260115',
            stock_pool='data/cleaned_candidates_66.csv'
        )
    """
    
    # è®°å¿†æ–‡ä»¶è·¯å¾„
    MEMORY_FILE = Path(__file__).parent.parent.parent / 'data' / 'memory' / 'ShortTermMemory.json'
    
    def __init__(self, initial_capital: float = 20000.0):
        self.initial_capital = initial_capital
        self.data_manager = QmtDataManager()
        self.results_cache: Dict[str, Dict] = {}
        self._ensure_output_dirs()
        
        # CTOä¿®å¤ï¼šå¯åŠ¨VIPæœåŠ¡ç¡®ä¿æ•°æ®è¿æ¥
        self.data_manager.start_vip_service()
        
    def _ensure_output_dirs(self):
        """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
        output_dir = PathResolver.get_data_dir() / 'backtest_out'
        PathResolver.ensure_dir(output_dir)
        PathResolver.ensure_dir(output_dir / 'time_machine')
        
    def get_trade_dates(self, start_date: str, end_date: str) -> List[str]:
        """
        è·å–äº¤æ˜“æ—¥åˆ—è¡¨ï¼ˆè‡ªåŠ¨è·³è¿‡å‘¨æœ«å’ŒèŠ‚å‡æ—¥ï¼‰
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ 'YYYYMMDD'
            end_date: ç»“æŸæ—¥æœŸ 'YYYYMMDD'
        
        Returns:
            äº¤æ˜“æ—¥åˆ—è¡¨
        """
        # ç®€åŒ–ç‰ˆï¼šåªè·³è¿‡å‘¨æœ«
        dates = []
        current = datetime.strptime(start_date, '%Y%m%d')
        end = datetime.strptime(end_date, '%Y%m%d')
        
        while current <= end:
            # è·³è¿‡å‘¨æœ« (5=Saturday, 6=Sunday)
            if current.weekday() < 5:
                dates.append(current.strftime('%Y%m%d'))
            current += timedelta(days=1)
        
        logger.info(f"ã€æ—¶é—´æœºå™¨ã€‘è·å–åˆ° {len(dates)} ä¸ªäº¤æ˜“æ—¥ï¼ˆ{start_date} è‡³ {end_date}ï¼‰")
        return dates
    
    def run_daily_backtest(self, date: str, stock_pool: List[str]) -> Dict:
        """
        å•æ—¥å›æµ‹
        
        æ¨¡æ‹Ÿå®ç›˜æµç¨‹ï¼š
        1. 09:30 å¼€ç›˜å‰å‡†å¤‡
        2. 09:40 è®¡ç®—æ—©ç›˜æ•°æ®
        3. è¾“å‡ºå½“æ—¥Top 20 (CTODict: æ‰©å®¹è§‚å¯Ÿæ¢¯åº¦)
        
        Args:
            date: äº¤æ˜“æ—¥æœŸ 'YYYYMMDD'
            stock_pool: è‚¡ç¥¨ä»£ç åˆ—è¡¨
        
        Returns:
            å½“æ—¥å›æµ‹ç»“æœå­—å…¸
        """
        print(f"\n{'='*60}")
        print(f"ã€æ—¶é—´æœºå™¨ã€‘å›æµ‹æ—¥æœŸ: {date}")
        print(f"{'='*60}")
        
        daily_result = {
            'date': date,
            'status': 'running',
            'top20': [],
            'signals': [],
            'errors': [],
            'total_stocks': len(stock_pool),
            'valid_stocks': 0
        }
        
        try:
            # CTOä¿®å¤ï¼šç¬¬ä¸€æ­¥ - VIPé˜»å¡ä¸‹è½½æ•°æ®ï¼ˆæ‰“é€šä»»ç£äºŒè„‰ï¼ï¼‰
            print(f"  ğŸ“¥ å‘VIPèŠ‚ç‚¹è¯·æ±‚ {date} Tickæ•°æ®å¹¶é˜»å¡ç­‰å¾…...")
            from logic.data_providers.qmt_manager import QmtDataManager
            downloader = QmtDataManager()
            download_results = downloader.download_tick_data(
                stock_list=stock_pool,  # CTOä¿®å¤ï¼šä¸‹è½½å…¨éƒ¨88åª
                trade_date=date,
                use_vip=True,
                check_existing=True
            )
            success_downloads = sum(1 for r in download_results.values() if r.success)
            print(f"  âœ… ä¸‹è½½å®Œæˆ: {success_downloads}/{len(stock_pool)} åª")
            
            # 2. è·å–å½“æ—¥è‚¡ç¥¨æ± æ•°æ®
            print(f"  ğŸ“Š è·å– {len(stock_pool)} åªè‚¡ç¥¨æ•°æ®...")
            
            valid_stocks = []
            for stock in stock_pool:  # CTOä¿®å¤ï¼šæ£€æŸ¥å…¨éƒ¨88åª
                try:
                    # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
                    tick_data = self._get_tick_data(stock, date)
                    if tick_data is not None and len(tick_data) > 100:
                        valid_stocks.append(stock)
                        logger.debug(f"  âœ“ {stock}: {len(tick_data)} æ¡Tickæ•°æ®")
                except Exception as e:
                    error_msg = f"{stock}: {str(e)}"
                    daily_result['errors'].append(error_msg)
                    logger.warning(f"  âš ï¸ {error_msg}")
            
            daily_result['valid_stocks'] = len(valid_stocks)
            print(f"  âœ… æœ‰æ•ˆæ•°æ®: {len(valid_stocks)} åª")
            
            if len(valid_stocks) < 5:
                daily_result['status'] = 'insufficient_data'
                logger.warning(f"  âš ï¸ æ•°æ®ä¸è¶³: ä»… {len(valid_stocks)} åªæœ‰æ•ˆæ•°æ®")
                return daily_result
            
            # 2. è®¡ç®—09:40æŒ‡æ ‡ï¼ˆæ—©ç›˜5åˆ†é’Ÿ+5åˆ†é’Ÿï¼‰
            print(f"  ğŸ§® è®¡ç®—æ—©ç›˜æŒ‡æ ‡...")
            
            stock_scores = []
            for stock in valid_stocks:
                try:
                    score = self._calculate_morning_score(stock, date)
                    if score:
                        stock_scores.append(score)
                except Exception as e:
                    error_msg = f"{stock}è®¡ç®—é”™è¯¯: {str(e)}"
                    daily_result['errors'].append(error_msg)
                    logger.warning(f"  âš ï¸ {error_msg}")
            
            # 3. æ’åºé€‰å‡ºTop 20 (CTODict: æ‰©å®¹è‡³Top 20è§‚å¯Ÿæ¢¯åº¦)
            stock_scores.sort(key=lambda x: x['final_score'], reverse=True)
            top20 = stock_scores[:20]
            
            daily_result['top20'] = top20
            daily_result['status'] = 'success'
            
            # 5. æ‰§è¡Œè®°å¿†è¡°å‡
            self._apply_memory_decay(date, top20)
            
            # 6. æ‰“å°ç»“æœ (ä»…æ˜¾ç¤ºå‰5ï¼Œä½†ä¿å­˜Top 20)
            print(f"\n  ğŸ† å½“æ—¥Top 20 (æ˜¾ç¤ºå‰5):")
            for i, item in enumerate(top20[:5], 1):
                print(f"    {i}. {item['stock_code']} - å¾—åˆ†: {item['final_score']:.2f}")
                print(f"       09:40æ¶¨å¹…: {item.get('change_0940', 0):.2f}%, çŠ¶æ€: {item.get('status', 'N/A')}")
            if len(top20) > 5:
                print(f"    ... å…± {len(top20)} åª (è¯¦è§JSON)")
            
            logger.info(f"ã€æ—¶é—´æœºå™¨ã€‘{date} å›æµ‹æˆåŠŸï¼ŒTop20: {[s['stock_code'] for s in top20[:5]]}...")
            
        except Exception as e:
            daily_result['status'] = 'error'
            error_msg = str(e)
            daily_result['errors'].append(error_msg)
            logger.error(f"  âŒ é”™è¯¯: {error_msg}")
            print(f"  âŒ é”™è¯¯: {error_msg}")
        
        return daily_result
    
    def _get_tick_data(self, stock_code: str, date: str) -> Optional[pd.DataFrame]:
        """
        è·å–Tickæ•°æ®
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            date: æ—¥æœŸ 'YYYYMMDD'
        
        Returns:
            DataFrameåŒ…å«time, priceç­‰å­—æ®µï¼Œæˆ–None
        """
        try:
            from xtquant import xtdata
            
            # æ ‡å‡†åŒ–ä»£ç 
            normalized_code = self._normalize_stock_code(stock_code)
            
            # è·å–æœ¬åœ°æ•°æ®
            data = xtdata.get_local_data(
                field_list=['time', 'lastPrice', 'volume'],
                stock_list=[normalized_code],
                period='tick',
                start_time=date,
                end_time=date
            )
            
            if data and normalized_code in data:
                df = data[normalized_code]
                if not df.empty:
                    # è½¬æ¢æ—¶é—´æ ¼å¼
                    if 'time' in df.columns:
                        df['time'] = df['time'].apply(
                            lambda x: datetime.fromtimestamp(x/1000).strftime('%H:%M:%S') 
                            if isinstance(x, (int, float)) else str(x)
                        )
                    # é‡å‘½åä»·æ ¼åˆ—ä¸ºæ ‡å‡†æ ¼å¼
                    if 'lastPrice' in df.columns:
                        df = df.rename(columns={'lastPrice': 'price'})
                    return df
            
            return None
            
        except Exception as e:
            logger.warning(f"è·å–Tickæ•°æ®å¤±è´¥ {stock_code}: {e}")
            return None
    
    def _get_pre_close(self, stock_code: str, date: str) -> float:
        """
        è·å–æ˜¨æ”¶ä»·
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            date: æ—¥æœŸ 'YYYYMMDD'
        
        Returns:
            æ˜¨æ”¶ä»·ï¼Œå¤±è´¥è¿”å›0
        """
        try:
            from xtquant import xtdata
            
            # æ ‡å‡†åŒ–ä»£ç 
            normalized_code = self._normalize_stock_code(stock_code)
            
            # è®¡ç®—å‰ä¸€å¤©çš„æ—¥æœŸ
            current = datetime.strptime(date, '%Y%m%d')
            prev_date = (current - timedelta(days=1)).strftime('%Y%m%d')
            
            # è·å–æ—¥çº¿æ•°æ®
            data = xtdata.get_local_data(
                field_list=['time', 'close'],
                stock_list=[normalized_code],
                period='1d',
                start_time=prev_date,
                end_time=date
            )
            
            if data and normalized_code in data:
                df = data[normalized_code]
                if not df.empty and len(df) >= 1:
                    # å–å€’æ•°ç¬¬äºŒæ¡ï¼ˆæ˜¨å¤©çš„æ”¶ç›˜ä»·ï¼‰
                    if len(df) >= 2:
                        return float(df.iloc[-2]['close'])
                    else:
                        # åªæœ‰ä¸€æ¡æ•°æ®æ—¶å–ç¬¬ä¸€æ¡
                        return float(df.iloc[0]['close'])
            
            return 0.0
            
        except Exception as e:
            logger.warning(f"è·å–æ˜¨æ”¶ä»·å¤±è´¥ {stock_code}: {e}")
            return 0.0
    
    @staticmethod
    def _normalize_stock_code(code: str) -> str:
        """
        æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç æ ¼å¼ä¸º QMT æ ¼å¼ï¼ˆ######.SH / ######.SZï¼‰
        
        Args:
            code: è‚¡ç¥¨ä»£ç ï¼Œæ”¯æŒå¤šç§æ ¼å¼
        
        Returns:
            QMT æ ‡å‡†æ ¼å¼çš„è‚¡ç¥¨ä»£ç 
        """
        if not code:
            return code
        
        # å¦‚æœå·²ç»åŒ…å«äº¤æ˜“æ‰€åç¼€ï¼Œç›´æ¥è¿”å›
        if code.endswith('.SH') or code.endswith('.SZ'):
            return code
        
        # æå–6ä½æ•°å­—ä»£ç 
        code = code.strip().replace('.', '')
        
        if code.startswith('sh'):
            stock_code = code[2:]
            return f"{stock_code}.SH"
        elif code.startswith('sz'):
            stock_code = code[2:]
            return f"{stock_code}.SZ"
        elif code.startswith('6'):
            return f"{code}.SH"
        elif code.startswith(('0', '3')):
            return f"{code}.SZ"
        else:
            # é»˜è®¤ä¸ºä¸»æ¿
            return f"{code}.SH"
    
    def _calculate_morning_score(self, stock_code: str, date: str) -> Optional[Dict]:
        """
        è®¡ç®—æ—©ç›˜å¾—åˆ†
        ä½¿ç”¨MetricDefinitionsè®¡ç®—çœŸå®æŒ‡æ ‡
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            date: æ—¥æœŸ 'YYYYMMDD'
        
        Returns:
            å¾—åˆ†å­—å…¸æˆ–None
        """
        try:
            # è·å–æ•°æ®
            tick_data = self._get_tick_data(stock_code, date)
            
            if tick_data is None or tick_data.empty or len(tick_data) < 10:
                return None
            
            # è·å–æ˜¨æ”¶ä»·
            pre_close = self._get_pre_close(stock_code, date)
            if pre_close <= 0:
                return None
            
            # ä½¿ç”¨SanityGuardsæ£€æŸ¥æ˜¨æ”¶ä»·
            passed, msg = SanityGuards.check_pre_close_valid(pre_close, stock_code)
            if not passed:
                logger.warning(f"ã€æ—¶é—´æœºå™¨ã€‘{stock_code} æ˜¨æ”¶ä»·æ£€æŸ¥å¤±è´¥: {msg}")
                return None
            
            # CTOä¿®å¤ï¼šæ­£ç¡®å¤„ç†æ—¶é—´æˆ³è·å–09:40ä»·æ ¼
            # ç¡®ä¿timeåˆ—æ˜¯å­—ç¬¦ä¸²æ ¼å¼ HH:MM:SS
            if pd.api.types.is_numeric_dtype(tick_data['time']):
                # å¦‚æœæ˜¯æ•°å€¼ï¼ˆæ¯«ç§’æ—¶é—´æˆ³ï¼‰ï¼Œè½¬æ¢
                tick_data['time_str'] = pd.to_datetime(tick_data['time'], unit='ms') + pd.Timedelta(hours=8)
                tick_data['time_str'] = tick_data['time_str'].dt.strftime('%H:%M:%S')
            else:
                tick_data['time_str'] = tick_data['time'].astype(str)
            
            # æˆªå–æ—©ç›˜æ•°æ®
            tick_0940 = tick_data[tick_data['time_str'] <= '09:40:00']
            if tick_0940.empty:
                logger.warning(f"ã€æ—¶é—´æœºå™¨ã€‘{stock_code} 09:40å‰æ— æ•°æ®")
                return None
            
            price_0940 = float(tick_0940.iloc[-1]['price'])
            
            # ä½¿ç”¨MetricDefinitionsè®¡ç®—çœŸå®æ¶¨å¹…
            try:
                change_pct = MetricDefinitions.TRUE_CHANGE(price_0940, pre_close)
            except (ValueError, TypeError) as e:
                logger.warning(f"ã€æ—¶é—´æœºå™¨ã€‘{stock_code} æ¶¨å¹…è®¡ç®—å¤±è´¥: {e}")
                return None
            
            # Sanity Check - æ¶¨å¹…åˆç†æ€§æ£€æŸ¥
            passed, msg = SanityGuards.check_price_change(change_pct, stock_code)
            if not passed:
                logger.warning(f"ã€æ—¶é—´æœºå™¨ã€‘{stock_code} æ¶¨å¹…æ£€æŸ¥å¤±è´¥: {msg}")
                return None
            
            # ç®€å•è¯„åˆ†ï¼ˆåç»­æ›¿æ¢ä¸ºå®Œæ•´V18è¯„åˆ†ï¼‰
            base_score = min(abs(change_pct) * 5, 100)  # æ¶¨å¹…è¶Šå¤§åˆ†è¶Šé«˜
            
            # ç¡®å®šçŠ¶æ€
            if change_pct > 5:
                status = 'strong'
            elif change_pct > 2:
                status = 'normal'
            else:
                status = 'weak'
            
            return {
                'stock_code': stock_code,
                'final_score': base_score,
                'change_0940': change_pct,
                'price_0940': price_0940,
                'pre_close': pre_close,
                'status': status
            }
            
        except Exception as e:
            logger.error(f"ã€æ—¶é—´æœºå™¨ã€‘è®¡ç®—æ—©ç›˜å¾—åˆ†å¤±è´¥ {stock_code}: {e}")
            return None
    
    def run_continuous_backtest(self, start_date: str, end_date: str, 
                                 stock_pool_path: str = 'TUSHARE',
                                 use_tushare: bool = True) -> List[Dict]:
        """
        è¿ç»­å¤šæ—¥å›æµ‹ - å…¨æ¯æ—¶é—´æœºå™¨æ ¸å¿ƒ
        CTODict: å¼ºåˆ¶ä½¿ç”¨çœŸå®Tushareç²—ç­›ï¼Œç¦æ­¢æ¨¡æ‹Ÿæ•°æ®
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ 'YYYYMMDD'
            end_date: ç»“æŸæ—¥æœŸ 'YYYYMMDD'
            stock_pool_path: è‚¡ç¥¨æ± æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤'TUSHARE'è¡¨ç¤ºå®æ—¶ç²—ç­›
            use_tushare: æ˜¯å¦ä½¿ç”¨Tushareæ¯æ—¥åŠ¨æ€ç²—ç­›
        
        Returns:
            æ¯æ—¥å›æµ‹ç»“æœåˆ—è¡¨
        """
        print(f"\n{'#'*80}")
        print(f"# å…¨æ¯æ—¶é—´æœºå™¨å¯åŠ¨")
        print(f"# å›æµ‹åŒºé—´: {start_date} ~ {end_date}")
        print(f"# åˆå§‹èµ„é‡‘: {self.initial_capital}å…ƒ")
        print(f"# æ•°æ®æº: {'Tushareå®æ—¶ç²—ç­›' if use_tushare else 'CSVæ–‡ä»¶'}")
        print(f"{'#'*80}\n")
        
        logger.info(f"ã€æ—¶é—´æœºå™¨ã€‘å¯åŠ¨è¿ç»­å›æµ‹: {start_date} ~ {end_date}")
        logger.info(f"ã€æ—¶é—´æœºå™¨ã€‘æ•°æ®æº: {'Tushareå®æ—¶ç²—ç­›' if use_tushare else 'CSVæ–‡ä»¶'}")
        
        # 2. è·å–äº¤æ˜“æ—¥
        trade_dates = self.get_trade_dates(start_date, end_date)
        print(f"ğŸ“… äº¤æ˜“æ—¥: {len(trade_dates)} å¤©")
        logger.info(f"äº¤æ˜“æ—¥: {len(trade_dates)} å¤©")
        
        # 3. é€æ—¥å›æµ‹
        all_results = []
        
        for i, date in enumerate(trade_dates, 1):
            print(f"\nğŸ“Œ è¿›åº¦: [{i}/{len(trade_dates)}] {date}")
            
            # CTODict: æ¯æ—¥åŠ¨æ€ç²—ç­› (Tushareæ¨¡å¼)
            if use_tushare:
                try:
                    stock_pool = self._load_stock_pool('TUSHARE', date)
                    print(f"  ğŸ“Š å½“æ—¥ç²—ç­›: {len(stock_pool)} åª")
                except Exception as e:
                    logger.error(f"ã€æ—¶é—´æœºå™¨ã€‘{date} ç²—ç­›å¤±è´¥: {e}")
                    print(f"  âŒ {date} ç²—ç­›å¤±è´¥: {e}")
                    # è®°å½•å¤±è´¥å¹¶ç»§ç»­ä¸‹ä¸€æ—¥
                    all_results.append({
                        'date': date,
                        'status': 'coarse_filter_failed',
                        'error': str(e)
                    })
                    continue
            
            daily_result = self.run_daily_backtest(date, stock_pool)
            all_results.append(daily_result)
            
            # ä¿å­˜æ¯æ—¥ç»“æœ
            self._save_daily_result(date, daily_result)
            
            # æ¸…ç†ç¼“å­˜
            self.results_cache.clear()
        
        # 4. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self._generate_summary_report(all_results, start_date, end_date)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = len([r for r in all_results if r['status'] == 'success'])
        
        print(f"\n{'#'*80}")
        print(f"# å…¨æ¯æ—¶é—´æœºå™¨å®Œæˆ")
        print(f"# æˆåŠŸ: {success_count}/{len(all_results)}")
        print(f"{'#'*80}\n")
        
        logger.info(f"ã€æ—¶é—´æœºå™¨ã€‘è¿ç»­å›æµ‹å®Œæˆ: {success_count}/{len(all_results)} æˆåŠŸ")
        
        return all_results
    
    def _load_stock_pool(self, path: str, date: str = None) -> List[str]:
        """
        åŠ è½½è‚¡ç¥¨æ±  - CTODict: ç¦æ­¢æ¨¡æ‹Ÿæ•°æ®ï¼Œå¼ºåˆ¶çœŸå®ç²—ç­›
        
        Args:
            path: è‚¡ç¥¨æ± æ–‡ä»¶è·¯å¾„ æˆ– 'TUSHARE' è¡¨ç¤ºå®æ—¶ç²—ç­›
            date: æ—¥æœŸ 'YYYYMMDD' (ç”¨äºTushareç²—ç­›)
        
        Returns:
            è‚¡ç¥¨ä»£ç åˆ—è¡¨ (çº¦500åª)
        
        Raises:
            RuntimeError: æ— æ³•è·å–çœŸå®æ•°æ®æ—¶æŠ›å‡ºè‡´å‘½å¼‚å¸¸ (Fail Fast)
        """
        # å¦‚æœä½¿ç”¨Tushareå®æ—¶ç²—ç­›
        if path.upper() == 'TUSHARE' or path == '':
            if not date:
                raise ValueError("ä½¿ç”¨Tushareç²—ç­›æ—¶å¿…é¡»æä¾›dateå‚æ•°")
            
            logger.info(f"ã€æ—¶é—´æœºå™¨ã€‘ä½¿ç”¨Tushareå®æ—¶ç²—ç­›: {date}")
            try:
                builder = UniverseBuilder()
                stock_pool = builder.get_daily_universe(date)
                
                if not stock_pool:
                    raise RuntimeError(f"Tushareç²—ç­›è¿”å›ç©ºè‚¡ç¥¨æ± : {date}")
                
                logger.info(f"ã€æ—¶é—´æœºå™¨ã€‘Tushareç²—ç­›å®Œæˆ: {len(stock_pool)} åª")
                return stock_pool
                
            except Exception as e:
                logger.error(f"ã€æ—¶é—´æœºå™¨ã€‘Tushareç²—ç­›å¤±è´¥: {e}")
                raise RuntimeError(f"æ— æ³•è·å–çœŸå®è‚¡ç¥¨æ± : {e}") from e
        
        # å¦‚æœæä¾›CSVæ–‡ä»¶è·¯å¾„
        full_path = PathResolver.resolve_path(path)
        
        if not full_path.exists():
            logger.error(f"ã€æ—¶é—´æœºå™¨ã€‘è‚¡ç¥¨æ± æ–‡ä»¶ä¸å­˜åœ¨: {path}")
            raise FileNotFoundError(f"è‚¡ç¥¨æ± æ–‡ä»¶ä¸å­˜åœ¨: {path}ã€‚è¯·æä¾›æœ‰æ•ˆCSVæ–‡ä»¶æˆ–ä½¿ç”¨'TUSHARE'è¿›è¡Œå®æ—¶ç²—ç­›")
        
        try:
            df = pd.read_csv(full_path)
            if 'ts_code' in df.columns:
                return df['ts_code'].tolist()
            elif 'stock_code' in df.columns:
                return df['stock_code'].tolist()
            elif 'code' in df.columns:
                return df['code'].tolist()
            else:
                # å‡è®¾ç¬¬ä¸€åˆ—æ˜¯è‚¡ç¥¨ä»£ç 
                return df.iloc[:, 0].tolist()
        except Exception as e:
            logger.error(f"ã€æ—¶é—´æœºå™¨ã€‘åŠ è½½è‚¡ç¥¨æ± å¤±è´¥: {e}")
            raise RuntimeError(f"æ— æ³•åŠ è½½è‚¡ç¥¨æ± æ–‡ä»¶: {e}") from e
    
    def _save_daily_result(self, date: str, result: Dict):
        """
        ä¿å­˜æ¯æ—¥ç»“æœ
        
        Args:
            date: æ—¥æœŸ 'YYYYMMDD'
            result: å½“æ—¥å›æµ‹ç»“æœ
        """
        try:
            output_dir = PathResolver.get_data_dir() / 'backtest_out' / 'time_machine'
            PathResolver.ensure_dir(output_dir)
            
            output_file = output_dir / f'time_machine_{date}.json'
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ã€æ—¶é—´æœºå™¨ã€‘ç»“æœå·²ä¿å­˜: {output_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜æ¯æ—¥ç»“æœå¤±è´¥: {e}")
    
    def _generate_summary_report(self, results: List[Dict], start_date: str, end_date: str):
        """
        ç”Ÿæˆæ€»ç»“æŠ¥å‘Š - CTODict: ä¿®å¤success_daysç»Ÿè®¡ï¼Œæ‰©å®¹è‡³Top 20
        
        Args:
            results: æ‰€æœ‰å›æµ‹ç»“æœ
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
        """
        try:
            # ç»Ÿè®¡å„çŠ¶æ€å¤©æ•°
            success_results = [r for r in results if r.get('status') == 'success']
            insufficient_results = [r for r in results if r.get('status') == 'insufficient_data']
            error_results = [r for r in results if r.get('status') == 'error']
            coarse_failed_results = [r for r in results if r.get('status') == 'coarse_filter_failed']
            
            report = {
                'start_date': start_date,
                'end_date': end_date,
                'total_days': len(results),
                'success_days': len(success_results),
                'insufficient_data_days': len(insufficient_results),
                'error_days': len(error_results),
                'coarse_filter_failed_days': len(coarse_failed_results),
                'statistics_by_date': {
                    r['date']: {
                        'status': r.get('status'),
                        'valid_stocks': r.get('valid_stocks', 0),
                        'top1_score': r.get('top20', [{}])[0].get('final_score', 0) if r.get('top20') else 0
                    }
                    for r in results
                },
                'daily_top20': [
                    {
                        'date': r['date'],
                        'top20': r.get('top20', []),
                        'valid_stocks': r.get('valid_stocks', 0)
                    } 
                    for r in success_results
                ]
            }
            
            output_dir = PathResolver.get_data_dir() / 'backtest_out' / 'time_machine'
            PathResolver.ensure_dir(output_dir)
            
            output_file = output_dir / f'time_machine_summary_{start_date}_{end_date}.json'
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            print(f"\n  ğŸ“„ æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {output_file}")
            logger.info(f"ã€æ—¶é—´æœºå™¨ã€‘æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {output_file}")
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ€»ç»“æŠ¥å‘Šå¤±è´¥: {e}")
    
    def get_backtest_summary(self, start_date: str, end_date: str) -> Optional[Dict]:
        """
        è·å–å›æµ‹æ€»ç»“æŠ¥å‘Š
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
        
        Returns:
            æŠ¥å‘Šå­—å…¸æˆ–None
        """
        try:
            output_file = (
                PathResolver.get_data_dir() / 'backtest_out' / 'time_machine' / 
                f'time_machine_summary_{start_date}_{end_date}.json'
            )
            
            if output_file.exists():
                with open(output_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            
            return None
        except Exception as e:
            logger.error(f"è¯»å–æ€»ç»“æŠ¥å‘Šå¤±è´¥: {e}")
            return None
    
    # ==================== è®°å¿†è¡°å‡æœºåˆ¶ ====================
    
    def _load_memory(self) -> Dict[str, Dict]:
        """
        åŠ è½½çŸ­æœŸè®°å¿† - è‡ªåŠ¨è¡¥å……ç¼ºå¤±å­—æ®µ
        
        Returns:
            è®°å¿†å­—å…¸ {stock_code: memory_item}
        """
        try:
            if self.MEMORY_FILE.exists():
                with open(self.MEMORY_FILE, 'r', encoding='utf-8') as f:
                    memory = json.load(f)
                
                # è‡ªåŠ¨è¡¥å……ç¼ºå¤±çš„å­—æ®µï¼ˆå‘åå…¼å®¹æ—§æ•°æ®ç»“æ„ï¼‰
                for stock_code, mem_item in memory.items():
                    if 'absent_days' not in mem_item:
                        mem_item['absent_days'] = 0
                        logger.debug(f"ã€è®°å¿†è¡°å‡ã€‘{stock_code} è¡¥å…… absent_days=0")
                    if 'last_decay_date' not in mem_item:
                        mem_item['last_decay_date'] = mem_item.get('date', '')
                        logger.debug(f"ã€è®°å¿†è¡°å‡ã€‘{stock_code} è¡¥å…… last_decay_date")
                
                return memory
            return {}
        except Exception as e:
            logger.error(f"ã€è®°å¿†è¡°å‡ã€‘åŠ è½½è®°å¿†å¤±è´¥: {e}")
            return {}
    
    def _save_memory(self, memory: Dict[str, Dict]) -> bool:
        """
        ä¿å­˜çŸ­æœŸè®°å¿†
        
        Args:
            memory: è®°å¿†å­—å…¸
        
        Returns:
            æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            self.MEMORY_FILE.parent.mkdir(parents=True, exist_ok=True)
            
            with open(self.MEMORY_FILE, 'w', encoding='utf-8') as f:
                json.dump(memory, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ã€è®°å¿†è¡°å‡ã€‘è®°å¿†å·²ä¿å­˜: {len(memory)} æ¡")
            return True
        except Exception as e:
            logger.error(f"ã€è®°å¿†è¡°å‡ã€‘ä¿å­˜è®°å¿†å¤±è´¥: {e}")
            return False
    
    def _apply_memory_decay(self, current_date: str, today_top20: List[Dict]) -> Dict[str, Dict]:
        """
        æ‰§è¡Œè®°å¿†è¡°å‡ - æ ¸å¿ƒé€»è¾‘
        
        è§„åˆ™:
        1. æ–°è®°å¿†åˆ† = è€è®°å¿†åˆ† * 0.5
        2. è¿ç»­2æ—¥ä¸ä¸Šæ¦œ -> åˆ é™¤
        3. è¡°å‡åscore < 10 -> åˆ é™¤
        
        Args:
            current_date: å½“å‰æ—¥æœŸ 'YYYYMMDD'
            today_top20: ä»Šæ—¥Top20åˆ—è¡¨ [{'stock_code': str, 'final_score': float, ...}]
        
        Returns:
            æ›´æ–°åçš„è®°å¿†å­—å…¸
        """
        # 1. åŠ è½½æ—§è®°å¿†
        memory = self._load_memory()
        
        # 2. è·å–ä»Šæ—¥ä¸Šæ¦œè‚¡ç¥¨ä»£ç 
        today_top_codes: Set[str] = {item['stock_code'] for item in today_top20}
        
        # 3. æ›´æ–°è®°å¿†ä¸­æ¯åªè‚¡ç¥¨
        new_memory = {}
        decay_stats = {'decayed': 0, 'removed_absent': 0, 'removed_low_score': 0, 'new_added': 0}
        
        for stock_code, mem_item in memory.items():
            # è·å–å½“å‰åˆ†æ•°
            old_score = mem_item.get('score', 0)
            
            # è¡°å‡åˆ†æ•°
            new_score = old_score * MEMORY_DECAY_FACTOR
            
            # æ£€æŸ¥æ˜¯å¦åœ¨ä»Šæ—¥Top20ä¸­
            if stock_code in today_top_codes:
                # ä»Šæ—¥ä¸Šæ¦œï¼Œé‡ç½®ç¼ºå¸­å¤©æ•°
                mem_item['absent_days'] = 0
                decay_stats['decayed'] += 1
                logger.debug(f"ã€è®°å¿†è¡°å‡ã€‘{stock_code} ä»Šæ—¥ä¸Šæ¦œï¼Œé‡ç½®ç¼ºå¸­å¤©æ•°")
            else:
                # æœªä¸Šæ¦œï¼Œå¢åŠ ç¼ºå¸­å¤©æ•°
                absent_days = mem_item.get('absent_days', 0) + 1
                mem_item['absent_days'] = absent_days
                
                # æ£€æŸ¥æ˜¯å¦è¿ç»­ç¼ºå¸­è¶…è¿‡é˜ˆå€¼
                if absent_days >= MEMORY_MAX_ABSENCE_DAYS:
                    decay_stats['removed_absent'] += 1
                    logger.info(f"ã€è®°å¿†è¡°å‡ã€‘{stock_code} è¿ç»­{absent_days}æ—¥ä¸ä¸Šæ¦œï¼Œåˆ é™¤")
                    continue
            
            # æ£€æŸ¥åˆ†æ•°æ˜¯å¦ä½äºé˜ˆå€¼
            if new_score < MEMORY_MIN_SCORE:
                decay_stats['removed_low_score'] += 1
                logger.info(f"ã€è®°å¿†è¡°å‡ã€‘{stock_code} åˆ†æ•°{new_score:.1f} < {MEMORY_MIN_SCORE}ï¼Œåˆ é™¤")
                continue
            
            # æ›´æ–°åˆ†æ•°å’Œæ—¥æœŸ
            mem_item['score'] = round(new_score, 2)
            mem_item['last_decay_date'] = current_date
            new_memory[stock_code] = mem_item
            decay_stats['decayed'] += 1
        
        # 4. æ·»åŠ ä»Šæ—¥æ–°ä¸Šæ¦œè‚¡ç¥¨ï¼ˆä¸åœ¨è®°å¿†ä¸­çš„ï¼‰
        for item in today_top20:
            stock_code = item['stock_code']
            if stock_code not in new_memory:
                new_memory[stock_code] = {
                    'stock_code': stock_code,
                    'date': current_date,
                    'score': item.get('final_score', 70.0),
                    'absent_days': 0,
                    'last_decay_date': current_date,
                    'close_price': item.get('price_0940', 0),
                    'change_pct': item.get('change_0940', 0),
                    'status': item.get('status', 'unknown')
                }
                decay_stats['new_added'] += 1
                logger.debug(f"ã€è®°å¿†è¡°å‡ã€‘{stock_code} æ–°ä¸Šæ¦œï¼ŒåŠ å…¥è®°å¿†")
        
        # 5. ä¿å­˜æ›´æ–°åçš„è®°å¿†
        self._save_memory(new_memory)
        
        # 6. æ‰“å°ç»Ÿè®¡
        print(f"\n  ğŸ“‰ è®°å¿†è¡°å‡ç»Ÿè®¡:")
        print(f"     åŸæœ‰è®°å¿†: {len(memory)} æ¡")
        print(f"     è¡°å‡ä¿ç•™: {decay_stats['decayed']} æ¡")
        print(f"     æ–°å¢è®°å¿†: {decay_stats['new_added']} æ¡")
        print(f"     åˆ é™¤(ç¼ºå¸­): {decay_stats['removed_absent']} æ¡")
        print(f"     åˆ é™¤(ä½åˆ†): {decay_stats['removed_low_score']} æ¡")
        print(f"     å½“å‰è®°å¿†: {len(new_memory)} æ¡")
        
        logger.info(f"ã€è®°å¿†è¡°å‡ã€‘ç»Ÿè®¡: åŸæœ‰{len(memory)}, ä¿ç•™{decay_stats['decayed']}, "
                   f"æ–°å¢{decay_stats['new_added']}, åˆ é™¤ç¼ºå¸­{decay_stats['removed_absent']}, "
                   f"åˆ é™¤ä½åˆ†{decay_stats['removed_low_score']}, å½“å‰{len(new_memory)}")
        
        return new_memory


# CLIå…¥å£
if __name__ == '__main__':
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # åˆ›å»ºå¼•æ“
    engine = TimeMachineEngine(initial_capital=20000.0)
    
    # æµ‹è¯•ï¼šå›æµ‹1.5æ—¥å‰å10å¤©
    results = engine.run_continuous_backtest(
        start_date='20251231',
        end_date='20260110',
        stock_pool_path='data/cleaned_candidates_66.csv'
    )
    
    # æ‰“å°æœ€ç»ˆç»“æœ
    print("\n" + "="*80)
    print("å›æµ‹å®Œæˆ!")
    print(f"æ€»äº¤æ˜“æ—¥: {len(results)}")
    print(f"æˆåŠŸ: {len([r for r in results if r['status']=='success'])}")
    print("="*80)
