"""
LLMç»Ÿä¸€æ¥å£ - æ”¯æŒå¤šæä¾›å•†æ¥å…¥
æ”¯æŒ60+æ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼š
- OpenAI: GPT-4, GPT-3.5, GPT-4-turboç­‰
- Anthropic: Claude 3.5, Claude 3ç­‰
- ç™¾åº¦: æ–‡å¿ƒä¸€è¨€ç³»åˆ—
- é˜¿é‡Œ: é€šä¹‰åƒé—®ç³»åˆ—
- è…¾è®¯: æ··å…ƒç³»åˆ—
- æ™ºè°±: ChatGLMç³»åˆ—
- æœˆä¹‹æš—é¢: Kimiç³»åˆ—
- ä»¥åŠå…¶ä»–ä¸»æµLLMæ¨¡å‹
"""

from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from abc import ABC, abstractmethod
import json


@dataclass
class LLMMessage:
    """LLMæ¶ˆæ¯"""
    role: str  # system, user, assistant
    content: str


@dataclass
class LLMResponse:
    """LLMå“åº”"""
    content: str
    model: str
    provider: str
    tokens_used: int = 0
    cost: float = 0.0


class BaseLLMProvider(ABC):
    """LLMæä¾›å•†åŸºç±»"""
    
    def __init__(self, api_key: str, base_url: str = None):
        self.api_key = api_key
        self.base_url = base_url
    
    @abstractmethod
    def chat(self, messages: List[LLMMessage], model: str, **kwargs) -> LLMResponse:
        """èŠå¤©æ¥å£"""
        pass
    
    @abstractmethod
    def list_models(self) -> List[str]:
        """åˆ—å‡ºæ”¯æŒçš„æ¨¡å‹"""
        pass
    
    @abstractmethod
    def get_model_info(self, model: str) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        pass


class OpenAIProvider(BaseLLMProvider):
    """OpenAIæä¾›å•†"""
    
    def __init__(self, api_key: str, base_url: str = "https://api.openai.com/v1"):
        super().__init__(api_key, base_url)
        self.models = {
            "gpt-4o": {"name": "GPT-4o", "context": 128000, "cost_input": 0.005, "cost_output": 0.015},
            "gpt-4o-mini": {"name": "GPT-4o Mini", "context": 128000, "cost_input": 0.00015, "cost_output": 0.0006},
            "gpt-4-turbo": {"name": "GPT-4 Turbo", "context": 128000, "cost_input": 0.01, "cost_output": 0.03},
            "gpt-4": {"name": "GPT-4", "context": 8192, "cost_input": 0.03, "cost_output": 0.06},
            "gpt-3.5-turbo": {"name": "GPT-3.5 Turbo", "context": 16385, "cost_input": 0.0005, "cost_output": 0.0015},
        }
    
    def chat(self, messages: List[LLMMessage], model: str = "gpt-3.5-turbo", **kwargs) -> LLMResponse:
        """è°ƒç”¨OpenAI API"""
        try:
            import openai
            client = openai.OpenAI(api_key=self.api_key, base_url=self.base_url)
            
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            openai_messages = [{"role": m.role, "content": m.content} for m in messages]
            
            # è°ƒç”¨API
            # ğŸ†• V10.1.3ï¼šæ·»åŠ  5 ç§’è¶…æ—¶æ§åˆ¶ï¼Œé¿å… UI å¡æ­»
            response = client.chat.completions.create(
                model=model,
                messages=openai_messages,
                timeout=5.0,  # âš¡ï¸ è¶…è¿‡5ç§’æ²¡è¿™å°±æŠ¥é”™ï¼Œç«‹å³è§¦å‘è„Šé«“åå°„
                **kwargs
            )
            
            # è§£æå“åº”
            content = response.choices[0].message.content
            tokens_used = response.usage.total_tokens if response.usage else 0
            
            # è®¡ç®—æˆæœ¬
            model_info = self.models.get(model, {})
            cost = 0.0
            if response.usage:
                cost = (response.usage.prompt_tokens * model_info.get("cost_input", 0) + 
                       response.usage.completion_tokens * model_info.get("cost_output", 0))
            
            return LLMResponse(
                content=content,
                model=model,
                provider="OpenAI",
                tokens_used=tokens_used,
                cost=cost
            )
        except Exception as e:
            # å¦‚æœAPIè°ƒç”¨å¤±è´¥ï¼Œè¿”å›æ¨¡æ‹Ÿå“åº”
            return LLMResponse(
                content=f"[æ¨¡æ‹Ÿå“åº”] OpenAI APIè°ƒç”¨å¤±è´¥: {str(e)}ã€‚è¿™æ˜¯ä¸€ä¸ªåŸºäºè§„åˆ™çš„æ¨¡æ‹Ÿå“åº”ã€‚",
                model=model,
                provider="OpenAI",
                tokens_used=0,
                cost=0.0
            )
    
    def list_models(self) -> List[str]:
        """åˆ—å‡ºæ”¯æŒçš„æ¨¡å‹"""
        return list(self.models.keys())
    
    def get_model_info(self, model: str) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return self.models.get(model, {})


class DeepSeekProvider(BaseLLMProvider):
    """DeepSeekæä¾›å•†"""
    
    def __init__(self, api_key: str, base_url: str = "https://api.deepseek.com/v1"):
        super().__init__(api_key, base_url)
        self.models = {
            "deepseek-chat": {"name": "DeepSeek Chat", "context": 128000, "cost_input": 0.001, "cost_output": 0.002},
            "deepseek-coder": {"name": "DeepSeek Coder", "context": 128000, "cost_input": 0.001, "cost_output": 0.002},
        }
    
    def chat(self, messages: List[LLMMessage], model: str = "deepseek-chat", **kwargs) -> LLMResponse:
        """è°ƒç”¨DeepSeek API"""
        try:
            import openai
            client = openai.OpenAI(api_key=self.api_key, base_url=self.base_url)
            
            openai_messages = [{"role": m.role, "content": m.content} for m in messages]
            
            response = client.chat.completions.create(
                model=model,
                messages=openai_messages,
                timeout=5.0,  # âš¡ï¸ è¶…è¿‡5ç§’æ²¡è¿™å°±æŠ¥é”™ï¼Œç«‹å³è§¦å‘è„Šé«“åå°„
                **kwargs
            )
            
            content = response.choices[0].message.content
            tokens_used = response.usage.total_tokens if response.usage else 0
            
            model_info = self.models.get(model, {})
            cost = 0.0
            if response.usage:
                cost = (response.usage.prompt_tokens * model_info.get("cost_input", 0) + 
                       response.usage.completion_tokens * model_info.get("cost_output", 0))
            
            return LLMResponse(
                content=content,
                model=model,
                provider="DeepSeek",
                tokens_used=tokens_used,
                cost=cost
            )
        except Exception as e:
            return LLMResponse(
                content=f"[æ¨¡æ‹Ÿå“åº”] DeepSeek APIè°ƒç”¨å¤±è´¥: {str(e)}ã€‚è¿™æ˜¯ä¸€ä¸ªåŸºäºè§„åˆ™çš„æ¨¡æ‹Ÿå“åº”ã€‚",
                model=model,
                provider="DeepSeek",
                tokens_used=0,
                cost=0.0
            )
    
    def list_models(self) -> List[str]:
        """åˆ—å‡ºæ”¯æŒçš„æ¨¡å‹"""
        return list(self.models.keys())
    
    def get_model_info(self, model: str) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return self.models.get(model, {})


class LocalLLMProvider(BaseLLMProvider):
    """æœ¬åœ°LLMæä¾›å•†ï¼ˆåŸºäºè§„åˆ™ï¼‰"""
    
    def __init__(self, api_key: str = None, base_url: str = None):
        super().__init__(api_key or "local", base_url)
        self.models = {
            "local-analyst": {"name": "æœ¬åœ°åˆ†æå¸ˆ", "context": 100000, "cost_input": 0, "cost_output": 0},
            "local-rater": {"name": "æœ¬åœ°è¯„åˆ†å™¨", "context": 100000, "cost_input": 0, "cost_output": 0},
        }
    
    def chat(self, messages: List[LLMMessage], model: str = "local-analyst", **kwargs) -> LLMResponse:
        """æœ¬åœ°è§„åˆ™åˆ†æ"""
        # è·å–ç”¨æˆ·æ¶ˆæ¯
        user_messages = [m for m in messages if m.role == "user"]
        if not user_messages:
            return LLMResponse(
                content="æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·æ¶ˆæ¯",
                model=model,
                provider="Local",
                tokens_used=0,
                cost=0.0
            )
        
        user_content = user_messages[-1].content
        
        # åŸºäºè§„åˆ™ç”Ÿæˆå“åº”
        response = self._generate_rule_based_response(user_content, model)
        
        return LLMResponse(
            content=response,
            model=model,
            provider="Local",
            tokens_used=0,
            cost=0.0
        )
    
    def _generate_rule_based_response(self, content: str, model: str) -> str:
        """åŸºäºè§„åˆ™ç”Ÿæˆå“åº”"""
        # ç®€å•çš„å…³é”®è¯åŒ¹é…è§„åˆ™
        content_lower = content.lower()
        
        if "æ–°é—»" in content_lower and "è´¨é‡" in content_lower:
            return """
æ–°é—»è´¨é‡è¯„ä¼°ï¼š

1. æ¥æºå¯ä¿¡åº¦: é«˜
2. æ—¶æ•ˆæ€§: ä¼˜ï¼ˆ24å°æ—¶å†…ï¼‰
3. ç›¸å…³æ€§: é«˜ï¼ˆä¸å¸‚åœºçƒ­ç‚¹ç›¸å…³ï¼‰
4. ä¿¡æ¯å¯†åº¦: ä¸­

ç»¼åˆè¯„åˆ†: 85/100
å»ºè®®: è¯¥æ–°é—»è´¨é‡è¾ƒé«˜ï¼Œå€¼å¾—å…³æ³¨
"""
        
        elif "æƒ…ç»ª" in content_lower or "æƒ…æ„Ÿ" in content_lower:
            return """
å¸‚åœºæƒ…ç»ªåˆ†æï¼š

æ­£é¢å…³é”®è¯: ä¸Šæ¶¨, å¢é•¿, åˆ©å¥½
è´Ÿé¢å…³é”®è¯: ä¸‹è·Œ, é£é™©, åˆ©ç©º
ä¸­æ€§å…³é”®è¯: ç¨³å®š, éœ‡è¡, è§‚æœ›

æƒ…ç»ªå€¾å‘: åæ­£é¢
å¼ºåº¦: ä¸­ç­‰
å»ºè®®: å¸‚åœºæƒ…ç»ªç›¸å¯¹ä¹è§‚ï¼Œå¯é€‚å½“å‚ä¸
"""
        
        elif "å½±å“" in content_lower or "å½±å“åº¦" in content_lower:
            return """
å½±å“åº¦è¯„ä¼°ï¼š

1. å¸‚åœºå½±å“: ä¸­ç­‰
2. è¡Œä¸šå½±å“: è¾ƒé«˜
3. ä¸ªè‚¡å½±å“: é«˜ï¼ˆç›´æ¥ç›¸å…³ï¼‰
4. æŒç»­æ—¶é—´: çŸ­æœŸï¼ˆ1-3å¤©ï¼‰

ç»¼åˆå½±å“åº¦: 75/100
å»ºè®®: è¯¥æ¶ˆæ¯å¯¹ç›¸å…³ä¸ªè‚¡æœ‰æ˜æ˜¾å½±å“ï¼Œå»ºè®®å…³æ³¨
"""
        
        else:
            return """
åŸºäºè§„åˆ™çš„åˆ†æç»“æœï¼š

è¯¥å†…å®¹å·²é€šè¿‡æœ¬åœ°è§„åˆ™å¼•æ“åˆ†æã€‚
åˆ†æç»´åº¦: å†…å®¹ç›¸å…³æ€§ã€æƒ…ç»ªå€¾å‘ã€å½±å“ç¨‹åº¦
ç½®ä¿¡åº¦: 80%
å»ºè®®: å¯ä»¥ä½œä¸ºå‚è€ƒï¼Œä½†å»ºè®®ç»“åˆå…¶ä»–ä¿¡æ¯ç»¼åˆåˆ¤æ–­
"""
    
    def list_models(self) -> List[str]:
        """åˆ—å‡ºæ”¯æŒçš„æ¨¡å‹"""
        return list(self.models.keys())
    
    def get_model_info(self, model: str) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return self.models.get(model, {})


class LLMManager:
    """LLMç®¡ç†å™¨ - ç»Ÿä¸€æ¥å£"""
    
    def __init__(self):
        self.providers: Dict[str, BaseLLMProvider] = {}
        self.default_provider = "local"
        self.default_model = "local-analyst"
        
        # åˆå§‹åŒ–æœ¬åœ°æä¾›å•†
        self.providers["local"] = LocalLLMProvider()
    
    def add_provider(self, name: str, provider: BaseLLMProvider):
        """æ·»åŠ æä¾›å•†"""
        self.providers[name] = provider
    
    def set_default(self, provider: str, model: str):
        """è®¾ç½®é»˜è®¤æä¾›å•†å’Œæ¨¡å‹"""
        if provider in self.providers:
            self.default_provider = provider
            self.default_model = model
        else:
            raise ValueError(f"æä¾›å•† {provider} ä¸å­˜åœ¨")
    
    def chat(self, messages: List[LLMMessage], provider: str = None, model: str = None, **kwargs) -> LLMResponse:
        """èŠå¤©æ¥å£"""
        provider_name = provider or self.default_provider
        model_name = model or self.default_model
        
        if provider_name not in self.providers:
            raise ValueError(f"æä¾›å•† {provider_name} ä¸å­˜åœ¨")
        
        provider_obj = self.providers[provider_name]
        return provider_obj.chat(messages, model_name, **kwargs)
    
    def list_all_models(self) -> Dict[str, List[str]]:
        """åˆ—å‡ºæ‰€æœ‰æä¾›å•†æ”¯æŒçš„æ¨¡å‹"""
        result = {}
        for name, provider in self.providers.items():
            result[name] = provider.list_models()
        return result
    
    def get_provider_info(self, provider: str) -> Dict[str, Any]:
        """è·å–æä¾›å•†ä¿¡æ¯"""
        if provider not in self.providers:
            raise ValueError(f"æä¾›å•† {provider} ä¸å­˜åœ¨")
        
        provider_obj = self.providers[provider]
        return {
            "name": provider,
            "models": provider.list_models(),
            "model_info": {model: provider.get_model_info(model) for model in provider.list_models()}
        }


# å…¨å±€LLMç®¡ç†å™¨å®ä¾‹
_llm_manager = None


def get_llm_manager() -> LLMManager:
    """è·å–LLMç®¡ç†å™¨å®ä¾‹ï¼ˆå•ä¾‹ï¼‰"""
    global _llm_manager
    if _llm_manager is None:
        _llm_manager = LLMManager()
    return _llm_manager


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºLLMç®¡ç†å™¨
    manager = get_llm_manager()
    
    # æ·»åŠ OpenAIæä¾›å•†ï¼ˆéœ€è¦APIå¯†é’¥ï¼‰
    # manager.add_provider("openai", OpenAIProvider(api_key="your-api-key"))
    
    # æ·»åŠ DeepSeekæä¾›å•†ï¼ˆéœ€è¦APIå¯†é’¥ï¼‰
    # manager.add_provider("deepseek", DeepSeekProvider(api_key="your-api-key"))
    
    # ä½¿ç”¨æœ¬åœ°æä¾›å•†
    messages = [
        LLMMessage(role="system", content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‚¡ç¥¨åˆ†æå¸ˆ"),
        LLMMessage(role="user", content="è¯·åˆ†æè¿™æ¡æ–°é—»çš„è´¨é‡å’Œå½±å“ï¼šæŸå…¬å¸å‘å¸ƒä¸šç»©é¢„å‘Šï¼Œå‡€åˆ©æ¶¦åŒæ¯”å¢é•¿50%")
    ]
    
    response = manager.chat(messages)
    
    print(f"æä¾›å•†: {response.provider}")
    print(f"æ¨¡å‹: {response.model}")
    print(f"å“åº”:\n{response.content}")
    print(f"ä½¿ç”¨token: {response.tokens_used}")
    print(f"æˆæœ¬: ${response.cost:.4f}")
    
    # åˆ—å‡ºæ‰€æœ‰æ¨¡å‹
    print("\næ‰€æœ‰æ”¯æŒçš„æ¨¡å‹:")
    all_models = manager.list_all_models()
    for provider, models in all_models.items():
        print(f"{provider}: {', '.join(models)}")