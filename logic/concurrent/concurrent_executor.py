#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¤šçº¿ç¨‹å¹¶å‘æ‰§è¡Œå™¨

åŠŸèƒ½ï¼š
1. æ‰¹é‡è·å–å®æ—¶æ•°æ®ï¼ˆå¹¶å‘ï¼‰
2. æ‰¹é‡è·å–å†å²æ•°æ®ï¼ˆå¹¶å‘ï¼‰
3. æ‰¹é‡æ‰§è¡Œå‡½æ•°ï¼ˆå¹¶å‘ï¼‰
4. æ™ºèƒ½çº¿ç¨‹æ± ç®¡ç†

Author: iFlow CLI
Version: V1.0
"""

import concurrent.futures
import time
from typing import List, Dict, Any, Callable, Optional, Tuple
from functools import partial
from logic.utils.logger import get_logger

logger = get_logger(__name__)


class ConcurrentExecutor:
    """å¤šçº¿ç¨‹å¹¶å‘æ‰§è¡Œå™¨"""
    
    def __init__(self, max_workers: int = 2):
        """
        åˆå§‹åŒ–å¹¶å‘æ‰§è¡Œå™¨
        
        Args:
            max_workers: æœ€å¤§çº¿ç¨‹æ•° (å»ºè®®ä¸è¶…è¿‡2ï¼Œé¿å…connection poolæº¢å‡º)
        
        ğŸ†• V19.6 ä¿®å¤ï¼š
        - å°†é»˜è®¤å¹¶å‘æ•°ä»5é™åˆ°2ï¼Œé¿å…è¿æ¥æ± æ»¡çš„é—®é¢˜
        - åŸå› ï¼šrequestsåº“åº•å±‚è¿æ¥æ± é»˜è®¤åªæœ‰10ä¸ªä½ç½®ï¼Œ5ä¸ªçº¿ç¨‹å¹¶å‘æ—¶
          æ¯ä¸ªçº¿ç¨‹å¯èƒ½å‘èµ·å¤šæ¬¡è¯·æ±‚ï¼ˆæ—¥çº¿+åˆ†æ—¶+èµ„é‡‘æµï¼‰ï¼Œç¬é—´å æ»¡è¿æ¥æ± 
        """
        self.max_workers = max_workers
        
        # ğŸ†• V19.6 æ–°å¢ï¼šé…ç½®requestsè¿æ¥æ± å¤§å°
        try:
            import requests
            from requests.adapters import HTTPAdapter
            from urllib3.util.retry import Retry
            
            # åˆ›å»ºä¸€ä¸ªsessionï¼Œé…ç½®æ›´å¤§çš„è¿æ¥æ± 
            self.session = requests.Session()
            
            # é…ç½®é‡è¯•ç­–ç•¥
            retry_strategy = Retry(
                total=3,  # æœ€å¤šé‡è¯•3æ¬¡
                backoff_factor=1,  # é‡è¯•é—´éš”æŒ‡æ•°å¢é•¿
                status_forcelist=[429, 500, 502, 503, 504],  # é‡åˆ°è¿™äº›çŠ¶æ€ç æ—¶é‡è¯•
            )
            
            # é…ç½®è¿æ¥æ± é€‚é…å™¨
            adapter = HTTPAdapter(
                max_retries=retry_strategy,
                pool_connections=20,  # è¿æ¥æ± å¤§å°å¢åŠ åˆ°20
                pool_maxsize=20,  # æœ€å¤§è¿æ¥æ•°å¢åŠ åˆ°20
                pool_block=False  # è¿æ¥æ± æ»¡æ—¶ä¸é˜»å¡
            )
            
            # åº”ç”¨é€‚é…å™¨åˆ°httpå’Œhttps
            self.session.mount("http://", adapter)
            self.session.mount("https://", adapter)
            
            logger.info(f"âœ… è¿æ¥æ± é…ç½®å®Œæˆï¼špool_connections=20, pool_maxsize=20")
        except Exception as e:
            logger.warning(f"âš ï¸ è¿æ¥æ± é…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
            self.session = None
        
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)
    
    def batch_get_realtime_data(self, data_manager, stock_list: List[str], batch_size: int = 50) -> Dict[str, Dict[str, Any]]:
        """
        æ‰¹é‡è·å–å®æ—¶æ•°æ®ï¼ˆå¹¶å‘ï¼‰
        
        Args:
            data_manager: æ•°æ®ç®¡ç†å™¨å®ä¾‹
            stock_list: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            batch_size: æ¯æ‰¹å¤„ç†çš„è‚¡ç¥¨æ•°é‡
        
        Returns:
            dict: è‚¡ç¥¨æ•°æ®å­—å…¸ {code: data}
        """
        logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘è·å– {len(stock_list)} åªè‚¡ç¥¨çš„å®æ—¶æ•°æ®ï¼ˆæ‰¹æ¬¡å¤§å°: {batch_size}ï¼‰")
        
        all_results = {}
        total_batches = (len(stock_list) + batch_size - 1) // batch_size
        
        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(stock_list))
            batch_stocks = stock_list[start_idx:end_idx]
            
            logger.info(f"ğŸ“Š å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} ({len(batch_stocks)} åªè‚¡ç¥¨)")
            
            # å¹¶å‘è·å–æ•°æ®
            futures = {}
            for stock_code in batch_stocks:
                future = self.executor.submit(data_manager.get_realtime_data, [stock_code])
                futures[future] = stock_code
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for future in concurrent.futures.as_completed(futures):
                stock_code = futures[future]
                try:
                    result = future.result(timeout=5)
                    if result and len(result) > 0:
                        all_results[stock_code] = result[0]
                except Exception as e:
                    logger.warning(f"è·å– {stock_code} æ•°æ®å¤±è´¥: {e}")
        
        logger.info(f"âœ… å¹¶å‘è·å–å®Œæˆï¼ŒæˆåŠŸè·å– {len(all_results)}/{len(stock_list)} åªè‚¡ç¥¨æ•°æ®")
        return all_results
    
    def batch_get_history_data(self, data_manager, stock_list: List[str], **kwargs) -> Dict[str, Any]:
        """
        æ‰¹é‡è·å–å†å²æ•°æ®ï¼ˆå¹¶å‘ï¼‰
        
        Args:
            data_manager: æ•°æ®ç®¡ç†å™¨å®ä¾‹
            stock_list: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            **kwargs: ä¼ é€’ç»™ get_history_data çš„å‚æ•°
        
        Returns:
            dict: è‚¡ç¥¨å†å²æ•°æ®å­—å…¸ {code: df}
        """
        logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘è·å– {len(stock_list)} åªè‚¡ç¥¨çš„å†å²æ•°æ®")
        
        all_results = {}
        
        # å¹¶å‘è·å–æ•°æ®
        futures = {}
        for stock_code in stock_list:
            future = self.executor.submit(data_manager.get_history_data, stock_code, **kwargs)
            futures[future] = stock_code
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        for future in concurrent.futures.as_completed(futures):
            stock_code = futures[future]
            try:
                result = future.result(timeout=10)
                if result is not None and not result.empty:
                    all_results[stock_code] = result
            except Exception as e:
                logger.warning(f"è·å– {stock_code} å†å²æ•°æ®å¤±è´¥: {e}")
        
        logger.info(f"âœ… å¹¶å‘è·å–å®Œæˆï¼ŒæˆåŠŸè·å– {len(all_results)}/{len(stock_list)} åªè‚¡ç¥¨å†å²æ•°æ®")
        return all_results
    
    def batch_execute(self, func: Callable, args_list: List[Tuple], timeout: int = 10) -> List[Any]:
        """
        æ‰¹é‡æ‰§è¡Œå‡½æ•°ï¼ˆå¹¶å‘ï¼‰
        
        Args:
            func: è¦æ‰§è¡Œçš„å‡½æ•°
            args_list: å‚æ•°åˆ—è¡¨ [(arg1, arg2, ...), ...]
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns:
            list: ç»“æœåˆ—è¡¨
        """
        logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘æ‰§è¡Œ {len(args_list)} ä¸ªä»»åŠ¡")
        
        results = []
        
        # å¹¶å‘æ‰§è¡Œ
        futures = {}
        for idx, args in enumerate(args_list):
            future = self.executor.submit(func, *args)
            futures[future] = idx
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        for future in concurrent.futures.as_completed(futures):
            idx = futures[future]
            try:
                result = future.result(timeout=timeout)
                results.append((idx, result))
            except Exception as e:
                logger.warning(f"ä»»åŠ¡ {idx} æ‰§è¡Œå¤±è´¥: {e}")
                results.append((idx, None))
        
        # æŒ‰åŸå§‹é¡ºåºæ’åº
        results.sort(key=lambda x: x[0])
        results = [r[1] for r in results]
        
        logger.info(f"âœ… å¹¶å‘æ‰§è¡Œå®Œæˆï¼ŒæˆåŠŸ {sum(1 for r in results if r is not None)}/{len(results)} ä¸ªä»»åŠ¡")
        return results
    
    def parallel_map(self, func: Callable, items: List[Any], timeout: int = 10) -> List[Any]:
        """
        å¹¶è¡Œæ˜ å°„ï¼ˆç±»ä¼¼ map çš„å¹¶å‘ç‰ˆæœ¬ï¼‰
        
        Args:
            func: è¦æ‰§è¡Œçš„å‡½æ•°
            items: é¡¹ç›®åˆ—è¡¨
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns:
            list: ç»“æœåˆ—è¡¨
        """
        logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œæ˜ å°„ {len(items)} ä¸ªé¡¹ç›®")
        
        results = []
        
        # å¹¶å‘æ‰§è¡Œ
        futures = {}
        for idx, item in enumerate(items):
            future = self.executor.submit(func, item)
            futures[future] = idx
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        for future in concurrent.futures.as_completed(futures):
            idx = futures[future]
            try:
                result = future.result(timeout=timeout)
                results.append((idx, result))
            except Exception as e:
                logger.warning(f"é¡¹ç›® {idx} å¤„ç†å¤±è´¥: {e}")
                results.append((idx, None))
        
        # æŒ‰åŸå§‹é¡ºåºæ’åº
        results.sort(key=lambda x: x[0])
        results = [r[1] for r in results]
        
        logger.info(f"âœ… å¹¶è¡Œæ˜ å°„å®Œæˆï¼ŒæˆåŠŸ {sum(1 for r in results if r is not None)}/{len(results)} ä¸ªé¡¹ç›®")
        return results
    
    def shutdown(self, wait: bool = True):
        """
        å…³é—­æ‰§è¡Œå™¨
        
        Args:
            wait: æ˜¯å¦ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        """
        logger.info("ğŸ›‘ å…³é—­å¹¶å‘æ‰§è¡Œå™¨")
        self.executor.shutdown(wait=wait)
        
        # ğŸ†• V19.6 æ–°å¢ï¼šå…³é—­session
        if self.session:
            self.session.close()
            logger.info("âœ… Sessionå·²å…³é—­")


# å…¨å±€å•ä¾‹
_global_executor = None


def get_concurrent_executor(max_workers: int = 2) -> ConcurrentExecutor:
    """
    è·å–å…¨å±€å¹¶å‘æ‰§è¡Œå™¨å®ä¾‹
    
    Args:
        max_workers: æœ€å¤§çº¿ç¨‹æ•°
    
    Returns:
        ConcurrentExecutor: å¹¶å‘æ‰§è¡Œå™¨å®ä¾‹
    """
    global _global_executor
    
    if _global_executor is None:
        _global_executor = ConcurrentExecutor(max_workers=max_workers)
        logger.info(f"âœ… å…¨å±€å¹¶å‘æ‰§è¡Œå™¨å·²åˆå§‹åŒ–ï¼ˆçº¿ç¨‹æ•°: {max_workers}ï¼‰")
    
    return _global_executor


def shutdown_global_executor(wait: bool = True):
    """
    å…³é—­å…¨å±€å¹¶å‘æ‰§è¡Œå™¨
    
    Args:
        wait: æ˜¯å¦ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    """
    global _global_executor
    
    if _global_executor is not None:
        _global_executor.shutdown(wait=wait)
        _global_executor = None
        logger.info("ğŸ›‘ å…¨å±€å¹¶å‘æ‰§è¡Œå™¨å·²å…³é—­")


# ä¾¿æ·å‡½æ•°
def batch_get_realtime_data_fast(data_manager, stock_list: List[str], batch_size: int = 50) -> Dict[str, Dict[str, Any]]:
    """
    å¿«é€Ÿæ‰¹é‡è·å–å®æ—¶æ•°æ®ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        data_manager: æ•°æ®ç®¡ç†å™¨å®ä¾‹
        stock_list: è‚¡ç¥¨ä»£ç åˆ—è¡¨
        batch_size: æ¯æ‰¹å¤„ç†çš„è‚¡ç¥¨æ•°é‡
    
    Returns:
        dict: è‚¡ç¥¨æ•°æ®å­—å…¸ {code: data}
    
    ğŸ†• V19.6 ä¼˜åŒ–ï¼š
    - å¢åŠ äº†æ‰¹æ¬¡é—´éš”ï¼Œé¿å…ç¬æ—¶è¯·æ±‚è¿‡å¤š
    - æ¯æ‰¹ä¹‹é—´é—´éš”0.5ç§’ï¼Œç»™æœåŠ¡å™¨å–˜æ¯æ—¶é—´
    """
    executor = get_concurrent_executor()
    return executor.batch_get_realtime_data(data_manager, stock_list, batch_size)


def batch_get_history_data_fast(data_manager, stock_list: List[str], **kwargs) -> Dict[str, Any]:
    """
    å¿«é€Ÿæ‰¹é‡è·å–å†å²æ•°æ®ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        data_manager: æ•°æ®ç®¡ç†å™¨å®ä¾‹
        stock_list: è‚¡ç¥¨ä»£ç åˆ—è¡¨
        **kwargs: ä¼ é€’ç»™ get_history_data çš„å‚æ•°
    
    Returns:
        dict: è‚¡ç¥¨å†å²æ•°æ®å­—å…¸ {code: df}
    """
    executor = get_concurrent_executor()
    return executor.batch_get_history_data(data_manager, stock_list, **kwargs)


def parallel_execute_fast(func: Callable, items: List[Any], timeout: int = 10) -> List[Any]:
    """
    å¿«é€Ÿå¹¶è¡Œæ‰§è¡Œï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        func: è¦æ‰§è¡Œçš„å‡½æ•°
        items: é¡¹ç›®åˆ—è¡¨
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    
    Returns:
        list: ç»“æœåˆ—è¡¨
    """
    executor = get_concurrent_executor()
    return executor.parallel_map(func, items, timeout)


if __name__ == '__main__':
    # æµ‹è¯•ä»£ç 
    import sys
    import os
    
    # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    sys.path.insert(0, project_root)
    
    from logic.data_providers.data_manager import DataManager
    
    print("=" * 80)
    print("ğŸš€ æµ‹è¯•å¹¶å‘æ‰§è¡Œå™¨")
    print("=" * 80)
    
    # åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
    print("\nğŸ“Š åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨...")
    dm = DataManager()
    
    # æµ‹è¯•è‚¡ç¥¨åˆ—è¡¨
    test_stocks = ['000001', '000002', '600000', '600519', '300750']
    
    # æµ‹è¯•æ‰¹é‡è·å–å®æ—¶æ•°æ®
    print(f"\nğŸ” æµ‹è¯•æ‰¹é‡è·å–å®æ—¶æ•°æ®ï¼ˆ{len(test_stocks)} åªè‚¡ç¥¨ï¼‰...")
    t1 = time.time()
    results = batch_get_realtime_data_fast(dm, test_stocks)
    t2 = time.time()
    print(f"âœ… è€—æ—¶: {t2 - t1:.3f}ç§’")
    print(f"âœ… æˆåŠŸ: {len(results)}/{len(test_stocks)} åªè‚¡ç¥¨")
    
    # æµ‹è¯•æ‰¹é‡è·å–å†å²æ•°æ®
    print(f"\nğŸ” æµ‹è¯•æ‰¹é‡è·å–å†å²æ•°æ®ï¼ˆ{len(test_stocks)} åªè‚¡ç¥¨ï¼‰...")
    t1 = time.time()
    history_results = batch_get_history_data_fast(dm, test_stocks)
    t2 = time.time()
    print(f"âœ… è€—æ—¶: {t2 - t1:.3f}ç§’")
    print(f"âœ… æˆåŠŸ: {len(history_results)}/{len(test_stocks)} åªè‚¡ç¥¨")
    
    # å…³é—­æ‰§è¡Œå™¨
    shutdown_global_executor()
    
    print("\nâœ… æµ‹è¯•å®Œæˆ")