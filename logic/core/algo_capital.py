"""
æ¸¸èµ„å¸­ä½åˆ†ææ¨¡å—
åˆ†æé¾™è™æ¦œæ¸¸èµ„ã€æ¸¸èµ„æ“ä½œæ¨¡å¼ã€è¯†åˆ«çŸ¥åæ¸¸èµ„"""

import pandas as pd
import sqlite3
import json
import time
import os
from datetime import datetime, timedelta
from functools import wraps
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from typing import Any, Callable, Optional
from logic.data.data_manager import DataManager
from logic.utils.logger import get_logger, log_execution_time, performance_context

# è·å–æ—¥å¿—è®°å½•å™¨
logger = get_logger(__name__)

try:
    # diskcache is SQLite-backed persistent cache
    from diskcache import FanoutCache
except ImportError as e:
    raise ImportError("Please install diskcache: pip install diskcache") from e


@dataclass
class CacheResult:
    """ç¼“å­˜ç»“æœ"""
    value: Any
    hit: bool


class DiskCacheManager:
    """
    æœ¬åœ°æŒä¹…åŒ–ç¼“å­˜ï¼ˆSQLite-backedï¼‰ã€‚
    - é€‚åˆç¼“å­˜ pandas.DataFrame / dict / list ç­‰å¯ pickle å¯¹è±¡
    - æ”¯æŒ TTL expire
    - FanoutCache é€‚åˆå¤šçº¿ç¨‹/å¤šè¿›ç¨‹å¹¶å‘è®¿é—®
    """

    _instance = None
    _cache = None

    def __new__(cls, *args, **kwargs):
        """å•ä¾‹æ¨¡å¼"""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(
        self,
        cache_dir: str = ".myquant_cache",
        shards: int = 8,
        size_limit_bytes: int = 5 * 1024**3,  # 5GB
        enabled: bool = True,
    ):
        # é¿å…é‡å¤åˆå§‹åŒ–
        if hasattr(self, '_initialized') and self._initialized:
            return

        self.enabled = enabled
        self.cache_dir = cache_dir

        if not self.enabled:
            self._cache = None
            self._initialized = True
            return

        os.makedirs(cache_dir, exist_ok=True)
        self._cache = FanoutCache(
            directory=cache_dir,
            shards=shards,
            size_limit=size_limit_bytes,
            statistics=True,
        )
        self._initialized = True

    def get(self, key: str, default: Any = None) -> Any:
        """è·å–ç¼“å­˜æ•°æ®"""
        if not self.enabled or self._cache is None:
            return default
        return self._cache.get(key, default=default)

    def set(self, key: str, value: Any, expire: Optional[int] = None, tag: Optional[str] = None) -> bool:
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        if not self.enabled or self._cache is None:
            return False
        # expire: seconds
        self._cache.set(key, value, expire=expire, tag=tag)
        return True

    def get_or_set(
        self,
        key: str,
        loader: Callable[[], Any],
        expire: Optional[int] = None,
        tag: Optional[str] = None,
        cache_none: bool = False,
    ) -> CacheResult:
        """
        è·å–æˆ–è®¾ç½®ç¼“å­˜
        - cache_none=Falseï¼šloader è¿”å› None æ—¶ä¸å†™å…¥ç¼“å­˜ï¼ˆé¿å…æŠŠä¸´æ—¶å¤±è´¥ç¼“å­˜ä½ï¼‰
        """
        cached = self.get(key, default=None)
        if cached is not None:
            return CacheResult(cached, True)

        value = loader()
        if value is None and not cache_none:
            return CacheResult(None, False)

        self.set(key, value, expire=expire, tag=tag)
        return CacheResult(value, False)

    def invalidate_prefix(self, prefix: str) -> int:
        """
        åˆ é™¤æ‰€æœ‰ä»¥ prefix å¼€å¤´çš„ keyï¼ˆç”¨äºæŒ‰æ—¥æœŸ/æ¨¡å—æ‰¹é‡å¤±æ•ˆï¼‰ã€‚
        """
        if not self.enabled or self._cache is None:
            return 0
        keys = list(self._cache.iterkeys())
        removed = 0
        for k in keys:
            if isinstance(k, str) and k.startswith(prefix):
                if self._cache.delete(k):
                    removed += 1
        return removed

    def invalidate_tag(self, tag: str) -> int:
        """
        åˆ é™¤æ‰€æœ‰æŒ‡å®š tag çš„ç¼“å­˜æ¡ç›®
        """
        if not self.enabled or self._cache is None:
            return 0
        return self._cache.evict(tag)

    def clear(self) -> None:
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        if self._cache is not None:
            self._cache.clear()

    def close(self) -> None:
        """å…³é—­ç¼“å­˜"""
        if self._cache is not None:
            self._cache.close()

    def get_stats(self) -> dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if not self.enabled or self._cache is None:
            return {'enabled': False}

        stats = self._cache.stats()
        return {
            'enabled': True,
            'cache_dir': self.cache_dir,
            'size_limit_bytes': self._cache.size_limit,
            'hits': stats.get('hits', 0),
            'misses': stats.get('misses', 0),
            'total_keys': len(list(self._cache.iterkeys())),
            'hit_rate': f"{stats.get('hits', 0) / max(stats.get('hits', 0) + stats.get('misses', 0), 1) * 100:.2f}%"
        }


class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨ - ä½¿ç”¨SQLiteç¼“å­˜APIæ•°æ®"""

    def __init__(self, db_path='data/cache.db'):
        """åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨"""
        self.db_path = db_path
        self._init_db()
        self.stats = {
            'hits': 0,
            'misses': 0,
            'sets': 0,
            'deletes': 0
        }

    def _init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS api_cache (
                key TEXT PRIMARY KEY,
                data TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                expires_at TIMESTAMP NOT NULL,
                access_count INTEGER DEFAULT 0,
                last_accessed TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        conn.commit()
        conn.close()

    def get(self, key):
        """ä»ç¼“å­˜è·å–æ•°æ®"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('''
            SELECT data, expires_at, access_count FROM api_cache
            WHERE key = ? AND expires_at > CURRENT_TIMESTAMP
        ''', (key,))
        result = cursor.fetchone()
        
        if result:
            # æ›´æ–°è®¿é—®ç»Ÿè®¡
            cursor.execute('''
                UPDATE api_cache 
                SET access_count = access_count + 1,
                    last_accessed = CURRENT_TIMESTAMP
                WHERE key = ?
            ''', (key,))
            conn.commit()
            
            self.stats['hits'] += 1
            conn.close()
            return json.loads(result[0])
        
        self.stats['misses'] += 1
        conn.close()
        return None

    def set(self, key, data, ttl=3600):
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        expires_at = datetime.now() + timedelta(seconds=ttl)
        cursor.execute('''
            INSERT OR REPLACE INTO api_cache (key, data, expires_at, access_count, last_accessed)
            VALUES (?, ?, ?, 1, CURRENT_TIMESTAMP)
        ''', (key, json.dumps(data), expires_at.strftime('%Y-%m-%d %H:%M:%S')))
        conn.commit()
        conn.close()
        
        self.stats['sets'] += 1

    def delete(self, key):
        """åˆ é™¤ç¼“å­˜æ•°æ®"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('DELETE FROM api_cache WHERE key = ?', (key,))
        conn.commit()
        conn.close()
        
        self.stats['deletes'] += 1

    def clear_expired(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('DELETE FROM api_cache WHERE expires_at <= CURRENT_TIMESTAMP')
        deleted_count = cursor.rowcount
        conn.commit()
        conn.close()
        
        return deleted_count

    def get_stats(self):
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # è·å–ç¼“å­˜æ€»æ•°
        cursor.execute('SELECT COUNT(*) FROM api_cache')
        total_count = cursor.fetchone()[0]
        
        # è·å–è¿‡æœŸç¼“å­˜æ•°
        cursor.execute('SELECT COUNT(*) FROM api_cache WHERE expires_at <= CURRENT_TIMESTAMP')
        expired_count = cursor.fetchone()[0]
        
        conn.close()
        
        hit_rate = self.stats['hits'] / (self.stats['hits'] + self.stats['misses']) if (self.stats['hits'] + self.stats['misses']) > 0 else 0
        
        return {
            'total_keys': total_count,
            'expired_keys': expired_count,
            'active_keys': total_count - expired_count,
            'hits': self.stats['hits'],
            'misses': self.stats['misses'],
            'hit_rate': hit_rate,
            'sets': self.stats['sets'],
            'deletes': self.stats['deletes']
        }

    def clear_all(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('DELETE FROM api_cache')
        conn.commit()
        conn.close()
        
        # é‡ç½®ç»Ÿè®¡
        self.stats = {
            'hits': 0,
            'misses': 0,
            'sets': 0,
            'deletes': 0
        }


def retry_with_backoff(max_retries=3, backoff_factor=2):
    """
    æŒ‡æ•°é€€é¿é‡è¯•è£…é¥°å™¨

    Args:
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        backoff_factor: é€€é¿å› å­
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            retries = 0
            while retries < max_retries:
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    retries += 1
                    if retries >= max_retries:
                        print(f"å‡½æ•° {func.__name__} é‡è¯• {max_retries} æ¬¡åä»ç„¶å¤±è´¥: {e}")
                        raise

                    wait_time = backoff_factor ** retries
                    print(f"å‡½æ•° {func.__name__} æ‰§è¡Œå¤±è´¥ï¼Œ{wait_time} ç§’åè¿›è¡Œç¬¬ {retries + 1} æ¬¡é‡è¯•...")
                    time.sleep(wait_time)

            return None
        return wrapper
    return decorator


class CapitalAnalyzer:
    """æ¸¸èµ„å¸­ä½åˆ†ææ¨¡å—"""

    # çŸ¥åæ¸¸èµ„å¸­ä½åˆ—è¡¨ï¼ˆåŒ…å«å¸¸è§å˜ä½“ï¼‰
    FAMOUS_CAPITALISTS = {
        "ç« ç›Ÿä¸»": [
            "ä¸­ä¿¡è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸æ­å·å»¶å®‰è·¯è¯åˆ¸è¥ä¸šéƒ¨",
            "å›½æ³°å›å®‰è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸ä¸Šæµ·åˆ†å…¬å¸",
            "å›½æ³°å›å®‰è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸ä¸Šæµ·æ±Ÿè‹è·¯è¯åˆ¸è¥ä¸šéƒ¨",
            "å›½æ³°å›å®‰è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸ä¸Šæµ·æ±Ÿè‹",
            "ä¸­ä¿¡è¯åˆ¸æ­å·å»¶å®‰è·¯",
            "å›½æ³°å›å®‰ä¸Šæµ·æ±Ÿè‹è·¯",
            "å›½æ³°å›å®‰ä¸Šæµ·åˆ†å…¬å¸"
        ],
        "æ–¹æ–°ä¾ ": [
            "å…´ä¸šè¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸è¥¿å®‰åˆ†å…¬å¸",
            "ä¸­ä¿¡è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸è¥¿å®‰æœ±é›€å¤§è¡—è¯åˆ¸è¥ä¸šéƒ¨",
            "å…´ä¸šè¯åˆ¸è¥¿å®‰åˆ†å…¬å¸",
            "ä¸­ä¿¡è¯åˆ¸è¥¿å®‰æœ±é›€å¤§è¡—",
            "å…´ä¸šè¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸"
        ],
        "å¾ç¿”": [
            "å›½æ³°å›å®‰è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸ä¸Šæµ·ç¦å±±è·¯è¯åˆ¸è¥ä¸šéƒ¨",
            "å…‰å¤§è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸å®æ³¢è§£æ”¾å—è·¯è¯åˆ¸è¥ä¸šéƒ¨",
            "å›½æ³°å›å®‰ä¸Šæµ·ç¦å±±è·¯",
            "å…‰å¤§è¯åˆ¸å®æ³¢è§£æ”¾å—è·¯",
            "å›½æ³°å›å®‰ä¸Šæµ·ç¦å»ºè·¯"
        ],
        "èµµè€å“¥": [
            "ä¸­å›½é“¶æ²³è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸ç»å…´è¯åˆ¸è¥ä¸šéƒ¨",
            "åæ³°è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸æµ™æ±Ÿåˆ†å…¬å¸",
            "é“¶æ²³è¯åˆ¸ç»å…´",
            "åæ³°è¯åˆ¸æµ™æ±Ÿåˆ†å…¬å¸",
            "ä¸­å›½é“¶æ²³è¯åˆ¸ç»å…´"
        ],
        "ç‚’è‚¡å…»å®¶": [
            "ä¸­ä¿¡è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸ä¸Šæµ·æ·®æµ·ä¸­è·¯è¯åˆ¸è¥ä¸šéƒ¨",
            "å›½æ³°å›å®‰è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸ä¸Šæµ·åˆ†å…¬å¸",
            "ä¸­ä¿¡è¯åˆ¸ä¸Šæµ·æ·®æµ·ä¸­è·¯",
            "å›½æ³°å›å®‰ä¸Šæµ·åˆ†å…¬å¸"
        ],
        "æˆéƒ½": [
            "åæ³°è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸æˆéƒ½èœ€é‡‘è·¯è¯åˆ¸è¥ä¸šéƒ¨",
            "å›½æ³°å›å®‰è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸æˆéƒ½åˆ†å…¬å¸",
            "ä¸­ä¿¡è¯åˆ¸æˆéƒ½èœ€é‡‘è·¯",
            "å›½æ³°å›å®‰æˆéƒ½åˆ†å…¬å¸"
        ],
        "æ·±åœ³": [
            "å…‰å¤§è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸æ·±åœ³é‡‘ç”°è·¯è¯åˆ¸è¥ä¸šéƒ¨",
            "é•¿æ±Ÿè¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸æ·±åœ³ç§‘è‹‘è·¯è¯åˆ¸è¥ä¸šéƒ¨",
            "å…‰å¤§è¯åˆ¸æ·±åœ³é‡‘ç”°è·¯",
            "é•¿æ±Ÿè¯åˆ¸æ·±åœ³ç§‘è‹‘è·¯",
            "å…‰å¤§è¯åˆ¸æ·±åœ³"
        ],
        "ä¹”å¸®ä¸»": [
            "ä¸­å›½ä¸­é‡‘è´¢å¯Œè¯åˆ¸æœ‰é™å…¬å¸æ·±åœ³åˆ†å…¬å¸",
            "åæ³°è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸æ·±åœ³å½©ç”°è·¯è¶…ç®—ä¸­å¿ƒè¯åˆ¸è¥ä¸šéƒ¨",
            "ä¸­é‡‘è´¢å¯Œæ·±åœ³",
            "åæ³°è¯åˆ¸æ·±åœ³å½©ç”°è·¯",
            "ä¸­å›½ä¸­é‡‘è´¢å¯Œæ·±åœ³"
        ],
        "ä½œæ‰‹æ–°ä¸€": [
            "å›½æ³°å›å®‰è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸å—äº¬å¤ªå¹³å—è·¯è¯åˆ¸è¥ä¸šéƒ¨",
            "åæ³°è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸å—äº¬æ±Ÿä¸œä¸­è·¯è¯åˆ¸è¥ä¸šéƒ¨",
            "å›½æ³°å›å®‰å—äº¬å¤ªå¹³å—è·¯",
            "åæ³°è¯åˆ¸å—äº¬æ±Ÿä¸œä¸­è·¯",
            "å›½æ³°å›å®‰å—äº¬"
        ],
        "å°é³„é±¼": [
            "ä¸­å›½é“¶æ²³è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸åŒ—äº¬ä¸­å…³æ‘å¤§è¡—è¯åˆ¸è¥ä¸šéƒ¨",
            "ä¸­ä¿¡è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸åŒ—äº¬æ€»éƒ¨è¯åˆ¸è¥ä¸šéƒ¨",
            "é“¶æ²³è¯åˆ¸åŒ—äº¬ä¸­å…³æ‘å¤§è¡—",
            "ä¸­ä¿¡è¯åˆ¸åŒ—äº¬æ€»éƒ¨",
            "é“¶æ²³è¯åˆ¸åŒ—äº¬"
        ],
        "æ‹‰è¨å¸®": [
            "ä¸œæ–¹è´¢å¯Œè¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸æ‹‰è¨ä¸œç¯è·¯ç¬¬äºŒè¯åˆ¸è¥ä¸šéƒ¨",
            "ä¸œæ–¹è´¢å¯Œè¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸æ‹‰è¨å›¢ç»“è·¯ç¬¬ä¸€è¯åˆ¸è¥ä¸šéƒ¨",
            "ä¸œæ–¹è´¢å¯Œè¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸æ‹‰è¨ä¸œç¯è·¯ç¬¬ä¸€è¯åˆ¸è¥ä¸šéƒ¨",
            "ä¸œæ–¹è´¢å¯Œè¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸æ‹‰è¨å›¢ç»“è·¯ç¬¬äºŒè¯åˆ¸è¥ä¸šéƒ¨",
            "ä¸œæ–¹è´¢å¯Œæ‹‰è¨ä¸œç¯è·¯ç¬¬äºŒ",
            "ä¸œæ–¹è´¢å¯Œæ‹‰è¨å›¢ç»“è·¯ç¬¬ä¸€",
            "ä¸œæ–¹è´¢å¯Œæ‹‰è¨"
        ],
        "æœºæ„": [
            "æ·±è‚¡é€šä¸“ç”¨",
            "æ²ªè‚¡é€šä¸“ç”¨",
            "æœºæ„ä¸“ç”¨",
            "æœºæ„ä¸“ç”¨å¸­ä½"
        ],
        "åæ³°": [
            "åæ³°è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸å—äº¬åºå±±è·¯è¯åˆ¸è¥ä¸šéƒ¨",
            "åæ³°è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸æµ™æ±Ÿåˆ†å…¬å¸",
            "åæ³°è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸æˆéƒ½èœ€é‡‘è·¯è¯åˆ¸è¥ä¸šéƒ¨",
            "åæ³°è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸æ·±åœ³å½©ç”°è·¯è¶…ç®—ä¸­å¿ƒè¯åˆ¸è¥ä¸šéƒ¨",
            "åæ³°è¯åˆ¸å—äº¬åºå±±è·¯",
            "åæ³°è¯åˆ¸æµ™æ±Ÿåˆ†å…¬å¸",
            "åæ³°è¯åˆ¸æˆéƒ½èœ€é‡‘è·¯",
            "åæ³°è¯åˆ¸æ·±åœ³å½©ç”°è·¯"
        ],
        "å›½ç››": [
            "å›½ç››è¯åˆ¸æœ‰é™è´£ä»»å…¬å¸å®æ³¢æ¡‘ç”°è·¯è¯åˆ¸è¥ä¸šéƒ¨",
            "å›½ç››è¯åˆ¸å®æ³¢æ¡‘ç”°è·¯"
        ],
        "å¼€æº": [
            "å¼€æºè¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸è¥¿å®‰è¥¿å¤§è¡—è¯åˆ¸è¥ä¸šéƒ¨",
            "å¼€æºè¯åˆ¸è¥¿å®‰è¥¿å¤§è¡—"
        ],
        "å›½ä¿¡": [
            "å›½ä¿¡è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸æµ™æ±Ÿäº’è”ç½‘åˆ†å…¬å¸",
            "å›½ä¿¡è¯åˆ¸æµ™æ±Ÿäº’è”ç½‘åˆ†å…¬å¸"
        ],
        "çˆ±å»º": [
            "çˆ±å»ºè¯åˆ¸æœ‰é™è´£ä»»å…¬å¸ä¸Šæµ·æµ¦ä¸œæ–°åŒºå‰æ»©å¤§é“è¯åˆ¸è¥ä¸šéƒ¨",
            "çˆ±å»ºè¯åˆ¸ä¸Šæµ·æµ¦ä¸œæ–°åŒºå‰æ»©å¤§é“"
        ]
    }

    # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
    cache = CacheManager()

    @staticmethod
    @log_execution_time
    def analyze_longhubu_capital(date=None):
        """
        åˆ†æé¾™è™æ¦œæ¸¸èµ„
        è¿”å›å½“æ—¥é¾™è™æ¦œä¸­çš„æ¸¸èµ„å¸­ä½åˆ†æ

        æ•°æ®æºç­–ç•¥ï¼ˆä¸‰å±‚ï¼‰ï¼š
        1. ç¬¬ä¸€å±‚ï¼šä¸œæ–¹è´¢å¯Œæ¥å£ - ä½¿ç”¨ stock_lhb_detail_em è·å–é¾™è™æ¦œè‚¡ç¥¨ï¼Œç„¶åä½¿ç”¨ stock_lhb_yyb_detail_em æŒ‰è¥ä¸šéƒ¨ä»£ç æŸ¥è¯¢è¯¦ç»†æ•°æ®
        2. ç¬¬äºŒå±‚ï¼šæ–°æµªæ¥å£ - ä½¿ç”¨ stock_lhb_yytj_sina è·å–ç´¯ç§¯ç»Ÿè®¡æ•°æ®
        3. ç¬¬ä¸‰å±‚ï¼šæœ¬åœ°ç¼“å­˜ - å¦‚æœå‰ä¸¤å±‚éƒ½å¤±è´¥ï¼Œè¿”å›å†å²æ•°æ®
        """
        logger.info(f"å¼€å§‹åˆ†æé¾™è™æ¦œæ¸¸èµ„ï¼Œæ—¥æœŸ: {date or 'æœ€æ–°'}")

        try:
            import akshare as ak
            from datetime import datetime
            import time

            # æ£€æŸ¥æ—§ç¼“å­˜ï¼ˆå…¼å®¹æ€§ï¼‰
            cache_key = f"lhb_capital_{date or 'latest'}"
            cached_data = CapitalAnalyzer.cache.get(cache_key)
            if cached_data:
                logger.info(f"ä»æ—§ç¼“å­˜è·å–æ•°æ®: {cache_key}")
                return cached_data

            # ===== ç¬¬ä¸€å±‚ï¼šä¸œæ–¹è´¢å¯Œæ¥å£ =====
            logger.info("=" * 60)
            logger.info("ç¬¬ä¸€å±‚æ•°æ®æºï¼šä¸œæ–¹è´¢å¯Œæ¥å£")
            logger.info("=" * 60)

            # ä½¿ç”¨ diskcache ç¼“å­˜é¾™è™æ¦œåˆ—è¡¨
            disk_cache = DiskCacheManager()

            # è·å–é¾™è™æ¦œæ•°æ®
            try:
                if date:
                    if isinstance(date, str):
                        # æ”¯æŒå¤šç§æ—¥æœŸæ ¼å¼
                        if '-' in date:
                            # %Y-%m-%d æ ¼å¼
                            date_obj = pd.to_datetime(date)
                            date_str = date_obj.strftime("%Y%m%d")
                        else:
                            date_str = date
                    else:
                        date_str = date.strftime("%Y%m%d")

                    # ä½¿ç”¨ diskcache ç¼“å­˜é¾™è™æ¦œåˆ—è¡¨
                    lhb_cache_key = f"lhb:list:{date_str}"
                    cache_result = disk_cache.get_or_set(
                        lhb_cache_key,
                        loader=lambda: ak.stock_lhb_detail_em(start_date=date_str, end_date=date_str),
                        expire=86400,  # 24å°æ—¶
                        tag=f"date:{date_str}"
                    )

                    if cache_result.hit:
                        logger.info(f"[ç¼“å­˜å‘½ä¸­] ä» diskcache è·å– {date_str} çš„é¾™è™æ¦œæ•°æ®")
                    else:
                        logger.info(f"[ç¼“å­˜æœªå‘½ä¸­] ä» API è·å– {date_str} çš„é¾™è™æ¦œæ•°æ®")

                    lhb_df = cache_result.value
                    logger.info(f"è·å– {date_str} çš„é¾™è™æ¦œæ•°æ®ï¼Œå…± {len(lhb_df)} æ¡è®°å½•")
                else:
                    # è·å–æœ€è¿‘å‡ å¤©çš„æ•°æ®
                    today = datetime.now()
                    date_str = today.strftime("%Y%m%d")

                    # ä½¿ç”¨ diskcache ç¼“å­˜é¾™è™æ¦œåˆ—è¡¨
                    lhb_cache_key = f"lhb:list:{date_str}"
                    cache_result = disk_cache.get_or_set(
                        lhb_cache_key,
                        loader=lambda: ak.stock_lhb_detail_em(start_date=date_str, end_date=date_str),
                        expire=86400,  # 24å°æ—¶
                        tag=f"date:{date_str}"
                    )

                    if cache_result.hit:
                        logger.info(f"[ç¼“å­˜å‘½ä¸­] ä» diskcache è·å–ä»Šæ—¥é¾™è™æ¦œæ•°æ®")
                    else:
                        logger.info(f"[ç¼“å­˜æœªå‘½ä¸­] ä» API è·å–ä»Šæ—¥é¾™è™æ¦œæ•°æ®")

                    lhb_df = cache_result.value
                    logger.info(f"è·å–ä»Šæ—¥é¾™è™æ¦œæ•°æ®ï¼Œå…± {len(lhb_df)} æ¡è®°å½•")

                    # å¦‚æœä»Šæ—¥æ— æ•°æ®ï¼Œå°è¯•è·å–æ˜¨å¤©
                    if lhb_df.empty:
                        yesterday = today - pd.Timedelta(days=1)
                        yesterday_str = yesterday.strftime("%Y%m%d")

                        # ä½¿ç”¨ diskcache ç¼“å­˜æ˜¨æ—¥é¾™è™æ¦œåˆ—è¡¨
                        lhb_cache_key = f"lhb:list:{yesterday_str}"
                        cache_result = disk_cache.get_or_set(
                            lhb_cache_key,
                            loader=lambda: ak.stock_lhb_detail_em(start_date=yesterday_str, end_date=yesterday_str),
                            expire=86400,  # 24å°æ—¶
                            tag=f"date:{yesterday_str}"
                        )

                        if cache_result.hit:
                            logger.info(f"[ç¼“å­˜å‘½ä¸­] ä» diskcache è·å–æ˜¨æ—¥é¾™è™æ¦œæ•°æ®")
                        else:
                            logger.info(f"[ç¼“å­˜æœªå‘½ä¸­] ä» API è·å–æ˜¨æ—¥é¾™è™æ¦œæ•°æ®")

                        lhb_df = cache_result.value
                        logger.info(f"ä»Šæ—¥æ— æ•°æ®ï¼Œè·å–æ˜¨æ—¥é¾™è™æ¦œæ•°æ®ï¼Œå…± {len(lhb_df)} æ¡è®°å½•")

                        date_str = yesterday_str
            except Exception as e:
                logger.error(f"è·å–é¾™è™æ¦œæ•°æ®å¤±è´¥: {e}", exc_info=True)
                lhb_df = None

            # å¦‚æœé¾™è™æ¦œæ•°æ®ä¸ºç©ºï¼Œå°è¯•ç¬¬äºŒå±‚æ•°æ®æº
            if lhb_df is None or lhb_df.empty:
                logger.warning("é¾™è™æ¦œæ•°æ®ä¸ºç©ºï¼Œåˆ‡æ¢åˆ°ç¬¬äºŒå±‚æ•°æ®æº")
                return CapitalAnalyzer._get_sina_data()

            logger.info(f"[OK] è·å– {len(lhb_df)} åªé¾™è™æ¦œè‚¡ç¥¨")

            # ===== æ–¹æ¡ˆ1ï¼šæŒ‰è‚¡ç¥¨é€ä¸ªæŸ¥è¯¢è¥ä¸šéƒ¨æ˜ç»†ï¼ˆå¹¶å‘ä¼˜åŒ–ï¼‰ =====
            logger.info("=" * 60)
            logger.info("æ–¹æ¡ˆ1ï¼šæŒ‰è‚¡ç¥¨é€ä¸ªæŸ¥è¯¢è¥ä¸šéƒ¨æ˜ç»†ï¼ˆå¹¶å‘æŸ¥è¯¢ï¼‰")
            logger.info("=" * 60)

            # ä½¿ç”¨å¹¶å‘æŸ¥è¯¢è·å–è¥ä¸šéƒ¨æ˜ç»†
            seat_detail_result = CapitalAnalyzer._get_seat_detail_by_stock_concurrent(lhb_df, date_str)
            if seat_detail_result is not None:
                return seat_detail_result

            # å¦‚æœå¹¶å‘æŸ¥è¯¢å¤±è´¥ï¼Œåˆ‡æ¢åˆ°ç¬¬äºŒå±‚æ•°æ®æº
            logger.warning("å¹¶å‘æŸ¥è¯¢è¥ä¸šéƒ¨æ˜ç»†å¤±è´¥ï¼Œåˆ‡æ¢åˆ°ç¬¬äºŒå±‚æ•°æ®æº")
            return CapitalAnalyzer._get_sina_data()

        except Exception as e:
            logger.error(f"åˆ†æé¾™è™æ¦œæ¸¸èµ„å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'æ•°æ®çŠ¶æ€': 'åˆ†æå¤±è´¥',
                'é”™è¯¯ä¿¡æ¯': str(e),
                'è¯´æ˜': 'åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œè¯·ç¨åé‡è¯•'
            }

    @staticmethod
    @retry_with_backoff(max_retries=3, backoff_factor=2)
    def _get_seat_detail_by_stock_concurrent(lhb_df, date_str, max_workers=20):
        """
        âœ… ä¼˜åŒ–ç‰ˆæœ¬ï¼šæŒ‰è‚¡ç¥¨é€ä¸ªæŸ¥è¯¢è¥ä¸šéƒ¨æ˜ç»†ï¼ˆå¹¶å‘ä¼˜åŒ–ï¼‰
        
        æ ¸å¿ƒä¼˜åŒ–ï¼š
        1. max_workers ä» 10 æå‡åˆ° 20 (+100% ååé‡)
        2. æ·»åŠ è¶…æ—¶ä¿æŠ¤ (æ€» 30sï¼Œå• 5s)
        3. ä¼˜åŒ–å¼‚å¸¸å¤„ç† (åˆ†ç±»å¤„ç†ï¼Œä¸ä¸­æ–­æµç¨‹)
        4. æ·»åŠ æ€§èƒ½æ—¥å¿—

        Args:
            lhb_df: é¾™è™æ¦œè‚¡ç¥¨åˆ—è¡¨
            date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆæ ¼å¼ï¼šYYYYMMDDï¼‰
            max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•° (ä¼˜åŒ–å€¼ï¼š20)

        Returns:
            æ¸¸èµ„åˆ†æç»“æœ
        """
        try:
            import akshare as ak
            from concurrent.futures import ThreadPoolExecutor, as_completed
            from datetime import datetime as dt

            all_seats = []
            success_count = 0
            fail_count = 0
            timeout_count = 0
            
            start_time = dt.now()
            logger.info(f"ğŸ“ å¼€å§‹æŸ¥è¯¢ {len(lhb_df)} åªè‚¡ç¥¨çš„è¥ä¸šéƒ¨æ˜ç»† (max_workers={max_workers})")

            def fetch_seat_detail(stock_info):
                """è·å–å•ä¸ªè‚¡ç¥¨çš„è¥ä¸šéƒ¨æ˜ç»†"""
                code = stock_info['ä»£ç ']
                name = stock_info['åç§°']

                try:
                    # ä½¿ç”¨ diskcache ç¼“å­˜å•è‚¡è¥ä¸šéƒ¨æ˜ç»†
                    disk_cache = DiskCacheManager()
                    seat_cache_key = f"lhb:seat_detail:{date_str}:{code}"

                    # å°è¯•ä»ç¼“å­˜è·å–
                    cached_seats = disk_cache.get(seat_cache_key)
                    if cached_seats is not None:
                        logger.debug(f"  [ç¼“å­˜å‘½ä¸­] {name}({code}) è¥ä¸šéƒ¨æ˜ç»†")
                        return cached_seats, True

                    # ç¼“å­˜æœªå‘½ä¸­ï¼Œè°ƒç”¨ API
                    seats = ak.stock_lhb_stock_detail_em(
                        symbol=code,
                        date=date_str
                    )

                    if not seats.empty:
                        # æ·»åŠ è‚¡ç¥¨ä¿¡æ¯
                        seats['è‚¡ç¥¨ä»£ç '] = code
                        seats['è‚¡ç¥¨åç§°'] = name
                        seats['ä¸Šæ¦œæ—¥'] = date_str

                        # ç¼“å­˜ç»“æœ
                        disk_cache.set(seat_cache_key, seats, expire=86400, tag=f"date:{date_str}")
                        logger.debug(f"  [ç¼“å­˜æœªå‘½ä¸­] {name}({code}) è¥ä¸šéƒ¨æ˜ç»†ï¼Œå·²ç¼“å­˜")

                        return seats, True
                    else:
                        return None, False
                except Exception as e:
                    logger.debug(f"  [WARN] {name}({code}) æŸ¥è¯¢å¤±è´¥: {e}")
                    return None, False

            # å¹¶å‘æŸ¥è¯¢ï¼ˆæå‡åˆ° 20 workersï¼‰
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                futures = {
                    executor.submit(fetch_seat_detail, row): idx
                    for idx, row in lhb_df.iterrows()
                }

                # è®¾ç½®æ€»è¶…æ—¶ä¸º 30 ç§’
                try:
                    for future in as_completed(futures, timeout=30):
                        try:
                            result, success = future.result(timeout=5)  # å•ä¸ª5ç§’
                            if success and result is not None:
                                all_seats.append(result)
                                success_count += 1
                                
                                # æ¯ 5 ä¸ªæˆåŠŸæ‰“å°ä¸€æ¬¡è¿›åº¦
                                if success_count % 5 == 0:
                                    logger.info(f"  âœ… è¿›åº¦: {success_count} æˆåŠŸï¼Œ{fail_count} å¤±è´¥ï¼Œ{timeout_count} è¶…æ—¶")
                            else:
                                fail_count += 1
                                
                        except TimeoutError:
                            timeout_count += 1
                        except Exception as e:
                            fail_count += 1
                            logger.debug(f"  å¤„ç†ç»“æœæ—¶å‡ºé”™: {e}")
                            
                except TimeoutError:
                    logger.warning(f"  âš ï¸  æ€»æŸ¥è¯¢è¶…æ—¶ (30ç§’)ï¼Œåœæ­¢ç­‰å¾…æ›´å¤šç»“æœ")
                    
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            elapsed = (dt.now() - start_time).total_seconds()
            total_records = sum(len(df) for df in all_seats if df is not None)
            
            logger.info(f"""
âœ… å¹¶å‘æŸ¥è¯¢å®Œæˆ
   - æŸ¥è¯¢ç»“æœ: {success_count} æˆåŠŸï¼Œ{fail_count} å¤±è´¥ï¼Œ{timeout_count} è¶…æ—¶
   - è·å–è®°å½•: {total_records} æ¡è¥ä¸šéƒ¨æ•°æ®
   - è€—æ—¶: {elapsed:.1f}ç§’
   - é€Ÿåº¦: {len(lhb_df)/elapsed:.2f} è‚¡ç¥¨/ç§’
           - ç›®æ ‡: < 15ç§’ {'[SUCCESS]' if elapsed < 15 else '[WARNING]' if elapsed < 20 else '[ERROR]'}""")
            
            if not all_seats:
                logger.error("[ERROR] æ‰€æœ‰è‚¡ç¥¨çš„è¥ä¸šéƒ¨æ˜ç»†æŸ¥è¯¢å‡å¤±è´¥")
                return None

            # åˆå¹¶æ‰€æœ‰è¥ä¸šéƒ¨æ•°æ®
            df_all = pd.concat(all_seats, ignore_index=True)
            logger.info(f"[OK] æ€»è®¡è·å– {len(df_all)} æ¡è¥ä¸šéƒ¨æ˜ç»†æ•°æ®")
            logger.info(f"[OK] è¥ä¸šéƒ¨æ˜ç»†åˆ—å: {df_all.columns.tolist()}")

            # åˆ†æè¥ä¸šéƒ¨æ•°æ®
            return CapitalAnalyzer._analyze_seat_data_from_stock_detail(df_all, date_str)

        except Exception as e:
            logger.error(f"[ERROR] å¹¶å‘æŸ¥è¯¢è¥ä¸šéƒ¨æ˜ç»†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None

    @staticmethod
    def _analyze_seat_data_from_stock_detail(df_all, date_str=None):
        """
        åˆ†æä»è‚¡ç¥¨æ˜ç»†è·å–çš„è¥ä¸šéƒ¨æ•°æ®

        Args:
            df_all: è¥ä¸šéƒ¨æ˜ç»†æ•°æ®
            date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆç”¨äºç¼“å­˜ï¼‰

        Returns:
            æ¸¸èµ„åˆ†æç»“æœ
        """
        try:
            capital_analysis = []
            capital_stats = {}
            matched_count = 0

            # âœ… å…³é”®ä¿®å¤ï¼šæ£€æŸ¥æ­£ç¡®çš„åˆ—å
            seat_col = None
            if 'äº¤æ˜“è¥ä¸šéƒ¨åç§°' in df_all.columns:
                seat_col = 'äº¤æ˜“è¥ä¸šéƒ¨åç§°'
            elif 'è¥ä¸šéƒ¨åç§°' in df_all.columns:
                seat_col = 'è¥ä¸šéƒ¨åç§°'
            else:
                logger.error(f"è¥ä¸šéƒ¨æ˜ç»†æ•°æ®ä¸­æ²¡æœ‰'è¥ä¸šéƒ¨åç§°'åˆ—ï¼Œå¯ç”¨åˆ—: {df_all.columns.tolist()}")
                return None

            logger.info(f"[OK] ä½¿ç”¨åˆ—å: {seat_col}")
            logger.info(f"[OK] è¥ä¸šéƒ¨æ˜ç»†åˆ—å: {df_all.columns.tolist()}")

            # åˆ†ææ¯æ¡è¥ä¸šéƒ¨è®°å½•
            for _, row in df_all.iterrows():
                seat_name = str(row.get(seat_col, ''))

                if not seat_name or seat_name == 'nan':
                    continue

                # ä½¿ç”¨æ™ºèƒ½åŒ¹é…ç®—æ³•
                capital_name, match_score = CapitalAnalyzer._match_capital_seat(seat_name)

                # åªä¿ç•™åŒ¹é…åº¦è¾ƒé«˜çš„ç»“æœï¼ˆ> 0.3ï¼‰
                if capital_name and match_score > 0.3:
                    matched_count += 1

                    # ç»Ÿè®¡æ¸¸èµ„æ“ä½œ
                    if capital_name not in capital_stats:
                        capital_stats[capital_name] = {
                            'ä¹°å…¥æ¬¡æ•°': 0,
                            'å–å‡ºæ¬¡æ•°': 0,
                            'ä¹°å…¥é‡‘é¢': 0,
                            'å–å‡ºé‡‘é¢': 0,
                            'æ“ä½œè‚¡ç¥¨': []
                        }

                    # è·å–ä¹°å…¥å’Œå–å‡ºé‡‘é¢
                    try:
                        buy_amount = float(row.get('ä¹°å…¥é‡‘é¢', 0) or row.get('ä¹°å…¥', 0) or 0)
                    except:
                        buy_amount = 0
                    
                    try:
                        sell_amount = float(row.get('å–å‡ºé‡‘é¢', 0) or row.get('å–å‡º', 0) or 0)
                    except:
                        sell_amount = 0

                    # è·å–ä¹°å…¥å’Œå–å‡ºæ¬¡æ•°
                    buy_count = 1 if buy_amount > 0 else 0
                    sell_count = 1 if sell_amount > 0 else 0

                    if buy_amount > 0:
                        capital_stats[capital_name]['ä¹°å…¥æ¬¡æ•°'] += buy_count
                        capital_stats[capital_name]['ä¹°å…¥é‡‘é¢'] += buy_amount
                    if sell_amount > 0:
                        capital_stats[capital_name]['å–å‡ºæ¬¡æ•°'] += sell_count
                        capital_stats[capital_name]['å–å‡ºé‡‘é¢'] += sell_amount

                    # è®°å½•æ“ä½œè‚¡ç¥¨
                    stock_info = {
                        'ä»£ç ': row.get('è‚¡ç¥¨ä»£ç ', ''),
                        'åç§°': row.get('è‚¡ç¥¨åç§°', ''),
                        'æ—¥æœŸ': row.get('ä¸Šæ¦œæ—¥', ''),
                        'ä¹°å…¥é‡‘é¢': buy_amount,
                        'å–å‡ºé‡‘é¢': sell_amount,
                        'å‡€ä¹°å…¥': buy_amount - sell_amount
                    }
                    capital_stats[capital_name]['æ“ä½œè‚¡ç¥¨'].append(stock_info)

                    capital_analysis.append({
                        'æ¸¸èµ„åç§°': capital_name,
                        'è¥ä¸šéƒ¨åç§°': seat_name,
                        'è‚¡ç¥¨ä»£ç ': row.get('è‚¡ç¥¨ä»£ç ', ''),
                        'è‚¡ç¥¨åç§°': row.get('è‚¡ç¥¨åç§°', ''),
                        'ä¸Šæ¦œæ—¥': row.get('ä¸Šæ¦œæ—¥', ''),
                        'ä¹°å…¥é‡‘é¢': buy_amount,
                        'å–å‡ºé‡‘é¢': sell_amount,
                        'å‡€ä¹°å…¥': buy_amount - sell_amount
                    })

            # è®¡ç®—æ¸¸èµ„ç»Ÿè®¡
            capital_summary = []
            for capital_name, stats in capital_stats.items():
                net_flow = stats['ä¹°å…¥é‡‘é¢'] - stats['å–å‡ºé‡‘é¢']
                total_trades = stats['ä¹°å…¥æ¬¡æ•°'] + stats['å–å‡ºæ¬¡æ•°']

                # åˆ¤æ–­æ“ä½œé£æ ¼
                if stats['ä¹°å…¥é‡‘é¢'] > stats['å–å‡ºé‡‘é¢'] * 2:
                    style = "æ¿€è¿›ä¹°å…¥"
                elif stats['å–å‡ºé‡‘é¢'] > stats['ä¹°å…¥é‡‘é¢'] * 2:
                    style = "æ¿€è¿›å–å‡º"
                elif net_flow > 0:
                    style = "åå¤š"
                else:
                    style = "åç©º"

                capital_summary.append({
                    'æ¸¸èµ„åç§°': capital_name,
                    'ä¹°å…¥æ¬¡æ•°': stats['ä¹°å…¥æ¬¡æ•°'],
                    'å–å‡ºæ¬¡æ•°': stats['å–å‡ºæ¬¡æ•°'],
                    'æ€»æ“ä½œæ¬¡æ•°': total_trades,
                    'ä¹°å…¥é‡‘é¢': stats['ä¹°å…¥é‡‘é¢'],
                    'å–å‡ºé‡‘é¢': stats['å–å‡ºé‡‘é¢'],
                    'å‡€æµå…¥': net_flow,
                    'æ“ä½œé£æ ¼': style,
                    'æ“ä½œè‚¡ç¥¨æ•°': len(stats['æ“ä½œè‚¡ç¥¨'])
                })

            # æŒ‰å‡€æµå…¥æ’åº
            capital_summary.sort(key=lambda x: x['å‡€æµå…¥'], reverse=True)

            logger.info(f"[OK] åˆ†æå®Œæˆï¼šåŒ¹é…åˆ° {matched_count} æ¡æ¸¸èµ„æ“ä½œè®°å½•ï¼Œæ¶‰åŠ {len(capital_stats)} ä¸ªæ¸¸èµ„")

            result = {
                'æ•°æ®çŠ¶æ€': 'æ­£å¸¸',
                'æ¸¸èµ„ç»Ÿè®¡': capital_summary,
                'æ¸¸èµ„æ“ä½œè®°å½•': capital_analysis,
                'åŒ¹é…è®°å½•æ•°': matched_count,
                'æ¸¸èµ„æ•°é‡': len(capital_stats),
                'é¾™è™æ¦œæ€»è®°å½•æ•°': len(df_all),
                'è¯´æ˜': f'é€šè¿‡å¹¶å‘æŸ¥è¯¢è·å–è¥ä¸šéƒ¨æ˜ç»†ï¼Œåœ¨ {len(df_all)} æ¡è®°å½•ä¸­æ‰¾åˆ° {matched_count} æ¡æ¸¸èµ„æ“ä½œè®°å½•'
            }

            # ä¿å­˜åˆ°ç¼“å­˜
            if date_str:
                cache_key = f"lhb_capital_concurrent_{date_str}"
                CapitalAnalyzer.cache.set(cache_key, result, ttl=3600)  # ç¼“å­˜1å°æ—¶

            return result

        except Exception as e:
            logger.error(f"[ERROR] åˆ†æè¥ä¸šéƒ¨æ˜ç»†æ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None

    @staticmethod
    @retry_with_backoff(max_retries=3, backoff_factor=2)
    def _get_sina_data():
        """
        ç¬¬äºŒå±‚æ•°æ®æºï¼šæ–°æµªæ¥å£ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        è·å–è¥ä¸šéƒ¨ç»Ÿè®¡æ•°æ®
        """
        try:
            import akshare as ak

            logger.info("=" * 60)
            logger.info("ç¬¬äºŒå±‚æ•°æ®æºï¼šæ–°æµªæ¥å£")
            logger.info("=" * 60)

            # æ£€æŸ¥ç¼“å­˜
            cache_key = "sina_yyb_stats_latest"
            cached_data = CapitalAnalyzer.cache.get(cache_key)
            if cached_data:
                logger.info(f"ä»ç¼“å­˜è·å–æ–°æµªè¥ä¸šéƒ¨æ•°æ®")
                yyb_stats = pd.DataFrame(cached_data)
            else:
                # ä½¿ç”¨æ–°æµªæ¥å£è·å–è¥ä¸šéƒ¨ç»Ÿè®¡æ•°æ®
                yyb_stats = ak.stock_lhb_yytj_sina(symbol='5')  # è·å–æœ€è¿‘5å¤©çš„æ•°æ®
                if not yyb_stats.empty:
                    # ç¼“å­˜æ•°æ®ï¼ŒTTLä¸º1å°æ—¶
                    CapitalAnalyzer.cache.set(cache_key, yyb_stats.to_dict('records'), ttl=3600)

            if yyb_stats.empty:
                logger.warning("æ–°æµªæ¥å£è¿”å›ç©ºæ•°æ®ï¼Œåˆ‡æ¢åˆ°ç¬¬ä¸‰å±‚æ•°æ®æº")
                return CapitalAnalyzer._get_historical_data()

            logger.info(f"è·å–åˆ° {len(yyb_stats)} æ¡è¥ä¸šéƒ¨ç»Ÿè®¡æ•°æ®")
            logger.info(f"æ–°æµªæ•°æ®åˆ—å: {yyb_stats.columns.tolist()}")

            return CapitalAnalyzer._analyze_sina_seat_data(yyb_stats)

        except Exception as e:
            logger.error(f"è·å–æ–°æµªè¥ä¸šéƒ¨æ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return CapitalAnalyzer._get_historical_data()

    @staticmethod
    def _analyze_sina_seat_data(yyb_stats, date=None):
        """
        åˆ†ææ–°æµªæ¥å£çš„è¥ä¸šéƒ¨æ•°æ®
        """
        try:
            # æ„å»ºå¸­ä½æ•°æ®
            all_seat_data = []
            for _, row in yyb_stats.head(50).iterrows():  # å–å‰50æ¡
                # æ‰¾åˆ°è¥ä¸šéƒ¨åç§°åˆ—
                seat_name = ''
                for col in yyb_stats.columns:
                    if 'è¥ä¸šéƒ¨' in col or 'å¸­ä½' in col:
                        seat_name = str(row.get(col, ''))
                        break

                if not seat_name:
                    continue

                # å¤„ç†é‡‘é¢æ•°æ®ï¼ˆç´¯ç§¯è´­ä¹°é¢å’Œç´¯ç§¯å–å‡ºé¢ï¼‰
                buy_amount = row.get('ç´¯ç§¯è´­ä¹°é¢', 0)
                sell_amount = row.get('ç´¯ç§¯å–å‡ºé¢', 0)

                # ç¡®ä¿é‡‘é¢æ˜¯æ•°å€¼ç±»å‹
                if pd.notna(buy_amount):
                    try:
                        buy_amount = float(buy_amount)
                    except:
                        buy_amount = 0
                else:
                    buy_amount = 0

                if pd.notna(sell_amount):
                    try:
                        sell_amount = float(sell_amount)
                    except:
                        sell_amount = 0
                else:
                    sell_amount = 0

                # è·å–ä¹°å…¥å’Œå–å‡ºæ¬¡æ•°
                buy_count = row.get('ä¹°å…¥å¸­ä½æ•°', 0)
                sell_count = row.get('å–å‡ºå¸­ä½æ•°', 0)

                if pd.notna(buy_count):
                    try:
                        buy_count = int(buy_count)
                    except:
                        buy_count = 0
                else:
                    buy_count = 0

                if pd.notna(sell_count):
                    try:
                        sell_count = int(sell_count)
                    except:
                        sell_count = 0
                else:
                    sell_count = 0

                all_seat_data.append({
                    'ä»£ç ': '',
                    'åç§°': str(row.get('ä¹°å…¥å‰ä¸‰è‚¡ç¥¨', '')),
                    'ä¸Šæ¦œæ—¥': date if date else '2026-01-06',
                    'æ”¶ç›˜ä»·': 0,
                    'æ¶¨è·Œå¹…': 0,
                    'è¥ä¸šéƒ¨åç§°': seat_name,
                    'ä¹°å…¥é¢': buy_amount,
                    'å–å‡ºé¢': sell_amount,
                    'å‡€ä¹°å…¥': buy_amount - sell_amount,
                    'ä¹°å…¥æ¬¡æ•°': buy_count,
                    'å–å‡ºæ¬¡æ•°': sell_count
                })

            if all_seat_data:
                seat_df = pd.DataFrame(all_seat_data)
                logger.info(f"æˆåŠŸæ„å»ºå¸­ä½æ•°æ®ï¼Œå…± {len(seat_df)} æ¡è®°å½•")
                return CapitalAnalyzer._analyze_seat_data(seat_df, 'è¥ä¸šéƒ¨åç§°', is_sina=True)
            else:
                logger.info("æ–°æµªæ•°æ®ä¸­æ²¡æœ‰æœ‰æ•ˆçš„å¸­ä½ä¿¡æ¯")
                return CapitalAnalyzer._get_historical_data()

        except Exception as e:
            logger.error(f"åˆ†ææ–°æµªè¥ä¸šéƒ¨æ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return CapitalAnalyzer._get_historical_data()

    @staticmethod
    def _get_historical_data():
        """
        ç¬¬ä¸‰å±‚æ•°æ®æºï¼šæœ¬åœ°ç¼“å­˜/å†å²æ•°æ®
        """
        try:
            import akshare as ak

            logger.info("=" * 60)
            logger.info("ç¬¬ä¸‰å±‚æ•°æ®æºï¼šå†å²è¥ä¸šéƒ¨æ•°æ®")
            logger.info("=" * 60)

            # å°è¯•è·å–å†å²è¥ä¸šéƒ¨æ•°æ®
            active_yyb = ak.stock_lhb_yyb_detail_em()
            if not active_yyb.empty:
                logger.info(f"è·å–åˆ° {len(active_yyb)} æ¡å†å²è¥ä¸šéƒ¨æ•°æ®")
                # è¿”å›å†å²è¥ä¸šéƒ¨æ•°æ®
                return {
                    'æ•°æ®çŠ¶æ€': 'æ­£å¸¸',
                    'è¯´æ˜': 'å½“å‰æ•°æ®æºä¸æä¾›å½“æ—¥è¥ä¸šéƒ¨æ˜ç»†ï¼Œæ˜¾ç¤ºå†å²æ´»è·ƒè¥ä¸šéƒ¨æ•°æ®ï¼ˆæ•°æ®å¯èƒ½è¿‡æ—¶ï¼‰',
                    'æ´»è·ƒè¥ä¸šéƒ¨': active_yyb,
                    'è¥ä¸šéƒ¨æ•°é‡': len(active_yyb)
                }
            else:
                logger.info("å†å²è¥ä¸šéƒ¨æ•°æ®ä¸ºç©º")
                return {
                    'æ•°æ®çŠ¶æ€': 'æ— æ•°æ®',
                    'è¯´æ˜': 'æ‰€æœ‰æ•°æ®æºå‡æ— æ³•è·å–åˆ°æœ‰æ•ˆæ•°æ®ï¼Œè¯·ç¨åé‡è¯•'
                }

        except Exception as e:
            logger.error(f"è·å–å†å²è¥ä¸šéƒ¨æ•°æ®å¤±è´¥: {e}")
            return {
                'æ•°æ®çŠ¶æ€': 'è·å–æ•°æ®å¤±è´¥',
                'é”™è¯¯ä¿¡æ¯': str(e),
                'è¯´æ˜': 'æ‰€æœ‰æ•°æ®æºå‡æ— æ³•è·å–åˆ°æœ‰æ•ˆæ•°æ®ï¼Œè¯·ç¨åé‡è¯•'
            }

    @staticmethod
    def _match_capital_seat(seat_name):
        """
        æ™ºèƒ½åŒ¹é…æ¸¸èµ„å¸­ä½

        ä½¿ç”¨å¤šçº§åŒ¹é…ç­–ç•¥ï¼š
        1. ç²¾ç¡®åŒ¹é…ï¼šå®Œå…¨åŒ¹é…
        2. å…³é”®è¯åŒ¹é…ï¼šåŒ…å«å…³é”®è¯
        3. æ¨¡ç³ŠåŒ¹é…ï¼šå»é™¤ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦ååŒ¹é…

        Returns:
            tuple: (capital_name, match_score) æˆ– (None, 0)
        """
        # æ ‡å‡†åŒ–è¥ä¸šéƒ¨åç§°ï¼šå»é™¤ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
        normalized_name = seat_name.replace(' ', '').replace('ã€€', '').replace('ï¼ˆ', '(').replace('ï¼‰', ')')

        for capital_name, seats in CapitalAnalyzer.FAMOUS_CAPITALISTS.items():
            # 1. ç²¾ç¡®åŒ¹é…
            if seat_name in seats or normalized_name in seats:
                return capital_name, 1.0

            # 2. å…³é”®è¯åŒ¹é…
            for seat_pattern in seats:
                if seat_pattern in seat_name or seat_pattern in normalized_name:
                    # è®¡ç®—åŒ¹é…åº¦ï¼šå…³é”®è¯é•¿åº¦ / æ€»é•¿åº¦
                    match_score = len(seat_pattern) / len(seat_name)
                    return capital_name, match_score

            # 3. æ¨¡ç³ŠåŒ¹é…ï¼šå»é™¤"è¯åˆ¸è¥ä¸šéƒ¨"ã€"è‚¡ä»½æœ‰é™å…¬å¸"ç­‰åç¼€
            simplified_name = normalized_name.replace('è¯åˆ¸è¥ä¸šéƒ¨', '').replace('è‚¡ä»½æœ‰é™å…¬å¸', '').replace('è¯åˆ¸', '')
            simplified_pattern = [s.replace('è¯åˆ¸è¥ä¸šéƒ¨', '').replace('è‚¡ä»½æœ‰é™å…¬å¸', '').replace('è¯åˆ¸', '') for s in seats]

            for i, pattern in enumerate(simplified_pattern):
                if pattern in simplified_name:
                    match_score = len(pattern) / len(simplified_name)
                    return capital_name, match_score * 0.9  # é™ä½åŒ¹é…åº¦

        return None, 0.0

    @staticmethod
    def _analyze_seat_data(lhb_df, seat_col, is_sina=False):
        """
        åˆ†æè¥ä¸šéƒ¨æ•°æ®ï¼ˆä¼˜åŒ–æ¸¸èµ„è¯†åˆ«ç²¾åº¦ï¼‰
        """
        try:
            # åˆ†ææ¸¸èµ„å¸­ä½
            capital_analysis = []
            capital_stats = {}
            matched_count = 0

            unique_seats = lhb_df[seat_col].unique()
            logger.info(f"å…±æ‰¾åˆ° {len(unique_seats)} ä¸ªä¸åŒçš„è¥ä¸šéƒ¨")
            logger.info(f"è¥ä¸šéƒ¨åˆ—è¡¨: {unique_seats[:10]}...")  # åªæ‰“å°å‰10ä¸ª

            for _, row in lhb_df.iterrows():
                seat_name = str(row[seat_col])

                # ä½¿ç”¨æ™ºèƒ½åŒ¹é…ç®—æ³•
                capital_name, match_score = CapitalAnalyzer._match_capital_seat(seat_name)

                # åªä¿ç•™åŒ¹é…åº¦è¾ƒé«˜çš„ç»“æœï¼ˆ> 0.3ï¼‰
                if capital_name and match_score > 0.3:
                        matched_count += 1
                        # ç»Ÿè®¡æ¸¸èµ„æ“ä½œ
                        if capital_name not in capital_stats:
                            capital_stats[capital_name] = {
                                'ä¹°å…¥æ¬¡æ•°': 0,
                                'å–å‡ºæ¬¡æ•°': 0,
                                'ä¹°å…¥é‡‘é¢': 0,
                                'å–å‡ºé‡‘é¢': 0,
                                'æ“ä½œè‚¡ç¥¨': []
                            }

                        # åˆ¤æ–­ä¹°å–æ–¹å‘
                        # æ–°æµªæ•°æ®ä½¿ç”¨ç´¯ç§¯è´­ä¹°é¢å’Œç´¯ç§¯å–å‡ºé¢
                        buy_amount = row.get('ä¹°å…¥é¢', 0)
                        sell_amount = row.get('å–å‡ºé¢', 0)

                        # è·å–ä¹°å…¥å’Œå–å‡ºæ¬¡æ•°
                        buy_count = row.get('ä¹°å…¥æ¬¡æ•°', 0)
                        sell_count = row.get('å–å‡ºæ¬¡æ•°', 0)

                        if buy_amount > 0 or buy_count > 0:
                            capital_stats[capital_name]['ä¹°å…¥æ¬¡æ•°'] += buy_count if buy_count > 0 else 1
                            capital_stats[capital_name]['ä¹°å…¥é‡‘é¢'] += buy_amount
                        if sell_amount > 0 or sell_count > 0:
                            capital_stats[capital_name]['å–å‡ºæ¬¡æ•°'] += sell_count if sell_count > 0 else 1
                            capital_stats[capital_name]['å–å‡ºé‡‘é¢'] += sell_amount

                        # è®°å½•æ“ä½œè‚¡ç¥¨
                        stock_info = {
                            'ä»£ç ': row['ä»£ç '],
                            'åç§°': row['åç§°'],
                            'æ—¥æœŸ': row['ä¸Šæ¦œæ—¥'],
                            'ä¹°å…¥é‡‘é¢': buy_amount,
                            'å–å‡ºé‡‘é¢': sell_amount,
                            'å‡€ä¹°å…¥': buy_amount - sell_amount
                        }
                        capital_stats[capital_name]['æ“ä½œè‚¡ç¥¨'].append(stock_info)

                        capital_analysis.append({
                            'æ¸¸èµ„åç§°': capital_name,
                            'è¥ä¸šéƒ¨åç§°': row[seat_col],
                            'è‚¡ç¥¨ä»£ç ': row['ä»£ç '],
                            'è‚¡ç¥¨åç§°': row['åç§°'],
                            'ä¸Šæ¦œæ—¥': row['ä¸Šæ¦œæ—¥'],
                            'ä¹°å…¥é‡‘é¢': buy_amount,
                            'å–å‡ºé‡‘é¢': sell_amount,
                            'å‡€ä¹°å…¥': buy_amount - sell_amount
                        })

            # è®¡ç®—æ¸¸èµ„ç»Ÿè®¡
            capital_summary = []
            for capital_name, stats in capital_stats.items():
                net_flow = stats['ä¹°å…¥é‡‘é¢'] - stats['å–å‡ºé‡‘é¢']
                total_trades = stats['ä¹°å…¥æ¬¡æ•°'] + stats['å–å‡ºæ¬¡æ•°']

                # åˆ¤æ–­æ“ä½œé£æ ¼
                if stats['ä¹°å…¥é‡‘é¢'] > stats['å–å‡ºé‡‘é¢'] * 2:
                    style = "æ¿€è¿›ä¹°å…¥"
                elif stats['å–å‡ºé‡‘é¢'] > stats['ä¹°å…¥é‡‘é¢'] * 2:
                    style = "æ¿€è¿›å–å‡º"
                elif net_flow > 0:
                    style = "åå¤š"
                else:
                    style = "åç©º"

                capital_summary.append({
                    'æ¸¸èµ„åç§°': capital_name,
                    'ä¹°å…¥æ¬¡æ•°': stats['ä¹°å…¥æ¬¡æ•°'],
                    'å–å‡ºæ¬¡æ•°': stats['å–å‡ºæ¬¡æ•°'],
                    'æ€»æ“ä½œæ¬¡æ•°': total_trades,
                    'ä¹°å…¥é‡‘é¢': stats['ä¹°å…¥é‡‘é¢'],
                    'å–å‡ºé‡‘é¢': stats['å–å‡ºé‡‘é¢'],
                    'å‡€æµå…¥': net_flow,
                    'æ“ä½œé£æ ¼': style,
                    'æ“ä½œè‚¡ç¥¨æ•°': len(stats['æ“ä½œè‚¡ç¥¨'])
                })

            # æŒ‰å‡€æµå…¥æ’åº
            capital_summary.sort(key=lambda x: x['å‡€æµå…¥'], reverse=True)

            logger.info(f"åˆ†æå®Œæˆï¼šåŒ¹é…åˆ° {matched_count} æ¡æ¸¸èµ„æ“ä½œè®°å½•ï¼Œæ¶‰åŠ {len(capital_stats)} ä¸ªæ¸¸èµ„")

            result = {
                'æ•°æ®çŠ¶æ€': 'æ­£å¸¸',
                'æ¸¸èµ„ç»Ÿè®¡': capital_summary,
                'æ¸¸èµ„æ“ä½œè®°å½•': capital_analysis,
                'åŒ¹é…è®°å½•æ•°': matched_count,
                'æ¸¸èµ„æ•°é‡': len(capital_stats),
                'é¾™è™æ¦œæ€»è®°å½•æ•°': len(lhb_df),
                'è¯´æ˜': f'åœ¨ {len(lhb_df)} æ¡é¾™è™æ¦œè®°å½•ä¸­ï¼Œæ‰¾åˆ° {matched_count} æ¡æ¸¸èµ„æ“ä½œè®°å½•'
            }

            # ä¿å­˜åˆ°ç¼“å­˜
            cache_key = f"lhb_capital_{is_sina and 'sina' or 'latest'}"
            CapitalAnalyzer.cache.set(cache_key, result, ttl=3600)  # ç¼“å­˜1å°æ—¶

            return result

        except Exception as e:
            return {
                'æ•°æ®çŠ¶æ€': 'è·å–å¤±è´¥',
                'é”™è¯¯ä¿¡æ¯': str(e),
                'è¯´æ˜': 'å¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–æ•°æ®æºé™åˆ¶'
            }

    @staticmethod
    def track_capital_pattern(capital_name, days=30):
        """
        è¿½è¸ªæ¸¸èµ„æ“ä½œæ¨¡å¼ï¼ˆä¿®å¤ç‰ˆï¼‰
        åˆ†æç‰¹å®šæ¸¸èµ„åœ¨æŒ‡å®šæ—¶é—´å†…çš„æ“ä½œè§„å¾‹

        æ ¸å¿ƒä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„åˆ—å 'äº¤æ˜“è¥ä¸šéƒ¨åç§°'
        """
        try:
            import akshare as ak
            from concurrent.futures import ThreadPoolExecutor, as_completed

            if capital_name not in CapitalAnalyzer.FAMOUS_CAPITALISTS:
                return {
                    'æ•°æ®çŠ¶æ€': 'æœªçŸ¥æ¸¸èµ„',
                    'è¯´æ˜': f'æœªæ‰¾åˆ°æ¸¸èµ„: {capital_name}'
                }

            # ä½¿ç”¨ diskcache ç¼“å­˜æ¸¸èµ„è¿½è¸ªç»“æœ
            disk_cache = DiskCacheManager()
            end_date = pd.Timestamp.now()
            track_cache_key = f"lhb:track_pattern:{capital_name}:{days}:{end_date.strftime('%Y%m%d')}"

            # å°è¯•ä»ç¼“å­˜è·å–
            cache_result = disk_cache.get_or_set(
                track_cache_key,
                loader=lambda: CapitalAnalyzer._fetch_capital_pattern_data(capital_name, days),
                expire=3600,  # 1å°æ—¶
                tag=f"capital:{capital_name}"
            )

            if cache_result.hit:
                logger.info(f"[ç¼“å­˜å‘½ä¸­] ä» diskcache è·å– {capital_name} çš„æ¸¸èµ„è¿½è¸ªç»“æœ")
            else:
                logger.info(f"[ç¼“å­˜æœªå‘½ä¸­] ä» API è·å– {capital_name} çš„æ¸¸èµ„è¿½è¸ªç»“æœ")

            return cache_result.value

        except Exception as e:
            logger.error(f"è¿½è¸ªæ¸¸èµ„æ“ä½œæ¨¡å¼å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'æ•°æ®çŠ¶æ€': 'åˆ†æå¤±è´¥',
                'é”™è¯¯ä¿¡æ¯': str(e),
                'è¯´æ˜': 'å¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–æ•°æ®æºé™åˆ¶'
            }

    @staticmethod
    def _fetch_capital_pattern_data(capital_name, days):
        """
        å†…éƒ¨æ–¹æ³•ï¼šè·å–æ¸¸èµ„æ“ä½œæ¨¡å¼æ•°æ®ï¼ˆä¸åŒ…å«ç¼“å­˜é€»è¾‘ï¼‰
        """
        try:
            import akshare as ak
            from concurrent.futures import ThreadPoolExecutor, as_completed

            # è·å–è¯¥æ¸¸èµ„çš„å¸­ä½åˆ—è¡¨
            seats = CapitalAnalyzer.FAMOUS_CAPITALISTS[capital_name]
            logger.info(f"ğŸ¯ æ¸¸èµ„ {capital_name} çš„å¸­ä½åˆ—è¡¨: {seats}")

            # è·å–å†å²é¾™è™æ¦œæ•°æ®
            end_date = pd.Timestamp.now()
            start_date = end_date - pd.Timedelta(days=days)

            all_operations = []
            checked_dates = 0
            matched_dates = 0

            # è·å–æ¯æ—¥é¾™è™æ¦œæ•°æ®
            current_date = start_date
            while current_date <= end_date:
                date_str = current_date.strftime("%Y%m%d")
                checked_dates += 1

                try:
                    # Step 1: è·å–é¾™è™æ¦œè‚¡ç¥¨åˆ—è¡¨
                    lhb_df = ak.stock_lhb_detail_em(start_date=date_str, end_date=date_str)

                    if not lhb_df.empty:
                        logger.info(f"ğŸ“… {date_str}: è·å– {len(lhb_df)} æ¡é¾™è™æ¦œè‚¡ç¥¨")

                        # Step 2: é€ä¸ªæŸ¥è¯¢è¥ä¸šéƒ¨æ˜ç»†ï¼ˆå¹¶å‘ä¼˜åŒ–ï¼‰
                        def fetch_seat_detail(stock_info):
                            """è·å–å•ä¸ªè‚¡ç¥¨çš„è¥ä¸šéƒ¨æ˜ç»†"""
                            code = stock_info['ä»£ç ']
                            name = stock_info['åç§°']

                            try:
                                # ä½¿ç”¨ diskcache ç¼“å­˜å•è‚¡è¥ä¸šéƒ¨æ˜ç»†
                                disk_cache = DiskCacheManager()
                                seat_cache_key = f"lhb:seat_detail:{date_str}:{code}"

                                # å°è¯•ä»ç¼“å­˜è·å–
                                cached_seats = disk_cache.get(seat_cache_key)
                                if cached_seats is not None:
                                    logger.debug(f"  [ç¼“å­˜å‘½ä¸­] {name}({code}) è¥ä¸šéƒ¨æ˜ç»†")
                                    cached_seats['æ—¥æœŸ'] = date_str
                                    return cached_seats, True

                                # ç¼“å­˜æœªå‘½ä¸­ï¼Œè°ƒç”¨ API
                                seats_df = ak.stock_lhb_stock_detail_em(
                                    symbol=code,
                                    date=date_str
                                )

                                if not seats_df.empty:
                                    # æ·»åŠ è‚¡ç¥¨ä¿¡æ¯
                                    seats_df['è‚¡ç¥¨ä»£ç '] = code
                                    seats_df['è‚¡ç¥¨åç§°'] = name
                                    seats_df['æ—¥æœŸ'] = date_str

                                    # ç¼“å­˜ç»“æœ
                                    disk_cache.set(seat_cache_key, seats_df, expire=86400, tag=f"date:{date_str}")
                                    logger.debug(f"  [ç¼“å­˜æœªå‘½ä¸­] {name}({code}) è¥ä¸šéƒ¨æ˜ç»†ï¼Œå·²ç¼“å­˜")

                                    return seats_df, True
                                else:
                                    return None, False
                            except Exception as e:
                                logger.debug(f"  [WARN] {name}({code}) æŸ¥è¯¢å¤±è´¥: {e}")
                                return None, False

                        # å¹¶å‘æŸ¥è¯¢è¥ä¸šéƒ¨æ˜ç»†
                        all_seats = []
                        success_count = 0

                        with ThreadPoolExecutor(max_workers=10) as executor:
                            # æäº¤æ‰€æœ‰ä»»åŠ¡
                            futures = {
                                executor.submit(fetch_seat_detail, row): idx
                                for idx, row in lhb_df.iterrows()
                            }

                            # æ”¶é›†ç»“æœ
                            for future in as_completed(futures):
                                result, success = future.result()
                                if success and result is not None:
                                    all_seats.append(result)
                                    success_count += 1

                        logger.info(f"  âœ… å¹¶å‘æŸ¥è¯¢å®Œæˆï¼šæˆåŠŸ {success_count} åªè‚¡ç¥¨")

                        if all_seats:
                            # åˆå¹¶æ‰€æœ‰è¥ä¸šéƒ¨æ•°æ®
                            df_all = pd.concat(all_seats, ignore_index=True)
                            logger.info(f"  è·å–åˆ° {len(df_all)} æ¡è¥ä¸šéƒ¨æ˜ç»†")
                            logger.info(f"  è¥ä¸šéƒ¨æ˜ç»†åˆ—å: {df_all.columns.tolist()}")

                            # âœ… å…³é”®ä¿®å¤ï¼šæ£€æŸ¥æ­£ç¡®çš„åˆ—å
                            seat_col = None
                            if 'äº¤æ˜“è¥ä¸šéƒ¨åç§°' in df_all.columns:
                                seat_col = 'äº¤æ˜“è¥ä¸šéƒ¨åç§°'
                            elif 'è¥ä¸šéƒ¨åç§°' in df_all.columns:
                                seat_col = 'è¥ä¸šéƒ¨åç§°'
                            else:
                                logger.error(f"  è¥ä¸šéƒ¨æ˜ç»†æ•°æ®ä¸­æ²¡æœ‰'è¥ä¸šéƒ¨åç§°'åˆ—ï¼Œå¯ç”¨åˆ—: {df_all.columns.tolist()}")
                                current_date += pd.Timedelta(days=1)
                                continue

                            # ç­›é€‰è¯¥æ¸¸èµ„çš„æ“ä½œ
                            for _, row in df_all.iterrows():
                                seat_name = str(row[seat_col])

                                # ç²¾ç¡®åŒ¹é…æˆ–æ¨¡ç³ŠåŒ¹é…
                                if seat_name in seats or any(keyword in seat_name for keyword in seats):
                                    matched_dates += 1
                                    
                                    try:
                                        buy_amt = float(row.get('ä¹°å…¥é‡‘é¢', 0) or 0)
                                    except:
                                        buy_amt = 0
                                    
                                    try:
                                        sell_amt = float(row.get('å–å‡ºé‡‘é¢', 0) or 0)
                                    except:
                                        sell_amt = 0
                                    
                                    all_operations.append({
                                        'æ—¥æœŸ': row['æ—¥æœŸ'],
                                        'è‚¡ç¥¨ä»£ç ': row['è‚¡ç¥¨ä»£ç '],
                                        'è‚¡ç¥¨åç§°': row['è‚¡ç¥¨åç§°'],
                                        'ä¹°å…¥é‡‘é¢': buy_amt,
                                        'å–å‡ºé‡‘é¢': sell_amt,
                                        'å‡€ä¹°å…¥': buy_amt - sell_amt,
                                        'è¥ä¸šéƒ¨åç§°': seat_name
                                    })
                                    logger.info(f"  âœ… åŒ¹é…: {seat_name} - {row['è‚¡ç¥¨åç§°']}({row['è‚¡ç¥¨ä»£ç ']})")

                except Exception as e:
                    logger.error(f"{date_str}: è·å–æ•°æ®å¤±è´¥ - {e}")
                    pass

                current_date += pd.Timedelta(days=1)

            logger.info(f"æ£€æŸ¥äº† {checked_dates} å¤©ï¼Œ{matched_dates} å¤©æ‰¾åˆ°æ“ä½œè®°å½•ï¼Œå…± {len(all_operations)} æ¡æ“ä½œ")

            # å¦‚æœæ²¡æœ‰æ“ä½œè®°å½•ï¼Œè¿”å›æç¤ºä¿¡æ¯
            if not all_operations:
                return {
                    'æ•°æ®çŠ¶æ€': 'æ— æ“ä½œè®°å½•',
                    'è¯´æ˜': f'{capital_name} åœ¨æœ€è¿‘ {days} å¤©å†…æ— æ“ä½œè®°å½•ã€‚å¯èƒ½åŸå› ï¼š1) è¯¥æ¸¸èµ„è¿‘æœŸæœªä¸Šæ¦œ 2) å¸­ä½åç§°ä¸åŒ¹é… 3) æ•°æ®æºé™åˆ¶ã€‚',
                    'æ£€æŸ¥å¤©æ•°': checked_dates,
                    'åŒ¹é…å¤©æ•°': matched_dates,
                    'æ¸¸èµ„å¸­ä½': seats
                }

            # åˆ†ææ“ä½œæ¨¡å¼
            df_ops = pd.DataFrame(all_operations)

            # 1. æ“ä½œé¢‘ç‡
            operation_frequency = len(all_operations) / days

            # 2. ä¹°å–åå¥½
            total_buy = df_ops['ä¹°å…¥é‡‘é¢'].sum()
            total_sell = df_ops['å–å‡ºé‡‘é¢'].sum()
            buy_ratio = total_buy / (total_buy + total_sell) if (total_buy + total_sell) > 0 else 0

            # 3. å•æ¬¡æ“ä½œé‡‘é¢
            avg_operation_amount = df_ops['å‡€ä¹°å…¥'].abs().mean()

            # 4. æ“ä½œæˆåŠŸç‡ï¼ˆåç»­3å¤©æ¶¨å¹…ï¼‰
            success_count = 0
            total_count = 0

            db = DataManager()
            for op in all_operations:
                try:
                    symbol = op['è‚¡ç¥¨ä»£ç ']
                    op_date = op['æ—¥æœŸ']

                    # è·å–å†å²æ•°æ®
                    start_date_str = op_date
                    end_date_str = (pd.Timestamp(op_date) + pd.Timedelta(days=5)).strftime("%Y-%m-%d")

                    df = db.get_history_data(symbol, start_date=start_date_str, end_date=end_date_str)

                    if not df.empty and len(df) > 3:
                        # è®¡ç®—æ“ä½œå3å¤©çš„æ¶¨å¹…
                        op_price = df.iloc[0]['close']
                        future_price = df.iloc[3]['close']
                        future_return = (future_price - op_price) / op_price * 100

                        if future_return > 0:
                            success_count += 1
                        total_count += 1
                except:
                    pass

            db.close()

            success_rate = (success_count / total_count * 100) if total_count > 0 else 0

            # 5. åˆ¤æ–­æ“ä½œé£æ ¼
            if buy_ratio > 0.7:
                style = "æ¿€è¿›ä¹°å…¥å‹"
            elif buy_ratio < 0.3:
                style = "æ¿€è¿›å–å‡ºå‹"
            elif avg_operation_amount > 50000000:
                style = "å¤§èµ„é‡‘æ“ä½œå‹"
            else:
                style = "å¹³å‡æ“ä½œ"

            return {
                'æ•°æ®çŠ¶æ€': 'æ­£å¸¸',
                'æ¸¸èµ„åç§°': capital_name,
                'åˆ†æå¤©æ•°': days,
                'æ“ä½œæ¬¡æ•°': len(all_operations),
                'æ“ä½œé¢‘ç‡': round(operation_frequency, 2),
                'æ€»ä¹°å…¥é‡‘é¢': total_buy,
                'æ€»å–å‡ºé‡‘é¢': total_sell,
                'ä¹°å…¥æ¯”ä¾‹': round(buy_ratio * 100, 1),
                'å¹³å‡æ“ä½œé‡‘é¢': round(avg_operation_amount, 0),
                'æ“ä½œæˆåŠŸç‡': round(success_rate, 1),
                'æ“ä½œé£æ ¼': style,
                'æ“ä½œè®°å½•': all_operations
            }

        except Exception as e:
            logger.error(f"è¿½è¸ªæ¸¸èµ„æ“ä½œæ¨¡å¼å¤±è´¥: {e}", exc_info=True)
            return {
                'æ•°æ®çŠ¶æ€': 'åˆ†æå¤±è´¥',
                'é”™è¯¯ä¿¡æ¯': str(e),
                'è¯´æ˜': 'å¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–æ•°æ®æºé™åˆ¶'
            }

    @staticmethod
    def predict_capital_next_move(capital_name):
        """
        é¢„æµ‹æ¸¸èµ„ä¸‹ä¸€æ­¥æ“ä½œ
        åŸºäºå†å²æ“ä½œæ¨¡å¼é¢„æµ‹
        """
        try:
            # è·å–æ¸¸èµ„æ“ä½œæ¨¡å¼
            pattern_result = CapitalAnalyzer.track_capital_pattern(capital_name, days=30)

            if pattern_result['æ•°æ®çŠ¶æ€'] != 'æ­£å¸¸':
                return pattern_result

            # è·å–æœ€è¿‘æ“ä½œ
            recent_operations = pattern_result['æ“ä½œè®°å½•'][-5:]  # æœ€è¿‘5æ¬¡æ“ä½œ

            # åˆ†ææœ€è¿‘æ“ä½œæ–¹å‘
            recent_buy = sum(op['ä¹°å…¥é‡‘é¢'] for op in recent_operations)
            recent_sell = sum(op['å–å‡ºé‡‘é¢'] for op in recent_operations)

            # é¢„æµ‹
            predictions = []

            if recent_buy > recent_sell * 2:
                predictions.append({
                    'é¢„æµ‹ç±»å‹': 'ç»§ç»­ä¹°å…¥',
                    'æ¦‚ç‡': 'é«˜',
                    'è¯´æ˜': f'{capital_name} æœ€è¿‘å¤§ä¸¾ä¹°å…¥ï¼Œå¯èƒ½ç»§ç»­åŠ ä»“'
                })
            elif recent_sell > recent_buy * 2:
                predictions.append({
                    'é¢„æµ‹ç±»å‹': 'ç»§ç»­å–å‡º',
                    'æ¦‚ç‡': 'é«˜',
                    'è¯´æ˜': f'{capital_name} æœ€è¿‘å¤§ä¸¾å–å‡ºï¼Œå¯èƒ½ç»§ç»­å‡ä»“'
                })
            else:
                predictions.append({
                    'é¢„æµ‹ç±»å‹': 'è§‚æœ›æˆ–å°é‡æ“ä½œ',
                    'æ¦‚ç‡': 'ä¸­',
                    'è¯´æ˜': f'{capital_name} æœ€è¿‘æ“ä½œå‡è¡¡ï¼Œå¯èƒ½è§‚æœ›'
                })

            # æ ¹æ®æˆåŠŸç‡é¢„æµ‹
            if pattern_result['æ“ä½œæˆåŠŸç‡'] > 60:
                predictions.append({
                    'é¢„æµ‹ç±»å‹': 'å…³æ³¨å…¶æ“ä½œ',
                    'æ¦‚ç‡': 'ä¸­',
                    'è¯´æ˜': f'{capital_name} å†å²æˆåŠŸç‡é«˜ï¼Œå»ºè®®å…³æ³¨å…¶æ“ä½œ'
                })

            return {
                'æ•°æ®çŠ¶æ€': 'æ­£å¸¸',
                'æ¸¸èµ„åç§°': capital_name,
                'é¢„æµ‹åˆ—è¡¨': predictions
            }

        except Exception as e:
            logger.error(f"é¢„æµ‹æ¸¸èµ„ä¸‹ä¸€æ­¥æ“ä½œå¤±è´¥: {e}", exc_info=True)
            return {
                'æ•°æ®çŠ¶æ€': 'é¢„æµ‹å¤±è´¥',
                'é”™è¯¯ä¿¡æ¯': str(e),
                'è¯´æ˜': 'å¯èƒ½æ˜¯æ•°æ®é—®é¢˜'
            }

    @staticmethod
    def get_performance_stats():
        """
è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            åŒ…å«ç¼“å­˜ç»Ÿè®¡å’Œæ€§èƒ½æŒ‡æ ‡çš„å­—å…¸
        """
        cache_stats = CapitalAnalyzer.cache.get_stats()
        
        return {
            'ç¼“å­˜ç»Ÿè®¡': cache_stats,
            'ç¼“å­˜å‘½ä¸­ç‡': f"{cache_stats['hit_rate'] * 100:.2f}%",
            'æ´»è·ƒç¼“å­˜æ•°': cache_stats['active_keys'],
            'è¿‡æœŸç¼“å­˜æ•°': cache_stats['expired_keys'],
            'æ€»ç¼“å­˜æ•°': cache_stats['total_keys']
        }

    @staticmethod
    def clear_cache():
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        CapitalAnalyzer.cache.clear_all()
        logger.info("å·²æ¸…ç©ºæ‰€æœ‰ç¼“å­˜")
        return {'çŠ¶æ€': 'æˆåŠŸ', 'è¯´æ˜': 'å·²æ¸…ç©ºæ‰€æœ‰ç¼“å­˜'}

    @staticmethod
    def cleanup_expired_cache():
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        deleted_count = CapitalAnalyzer.cache.clear_expired()
        logger.info(f"å·²æ¸…ç† {deleted_count} æ¡è¿‡æœŸç¼“å­˜")
        return {'çŠ¶æ€': 'æˆåŠŸ', 'è¯´æ˜': f'å·²æ¸…ç† {deleted_count} æ¡è¿‡æœŸç¼“å­˜'}
