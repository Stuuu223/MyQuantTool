#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨å¸‚åœºæ‰«æå¼•æ“ï¼ˆä¸‰é˜¶æ®µæ¸è¿›å¼ç­›é€‰ï¼‰

æ ¸å¿ƒæ¶æ„ï¼š
  é˜¶æ®µ1ï¼šé¢„ç­›é€‰ï¼ˆç¡¬æ€§æ¡ä»¶ï¼Œ1åˆ†é’Ÿï¼‰ â†’ 5000è‚¡ â†’ 200-400è‚¡
  é˜¶æ®µ2ï¼šå››ç»´åˆç­›ï¼ˆç®€åŒ–QPSTï¼Œ3åˆ†é’Ÿï¼‰ â†’ 200-400è‚¡ â†’ 50-100è‚¡
  é˜¶æ®µ3ï¼šç²¾å‡†QPSTï¼ˆå®Œæ•´åˆ†æï¼Œ5åˆ†é’Ÿï¼‰ â†’ 50-100è‚¡ â†’ 20-50è‚¡

ä½¿ç”¨åœºæ™¯ï¼š
  - æ¯å¤©2-3æ¬¡æ‰«æï¼ˆ09:35, 10:00, 14:00ï¼‰
  - è¾“å‡ºTOP 20-50è¯±å¤šæ¦œå•
  - æ—¥æŠ¥/å‘¨æŠ¥ç”Ÿæˆ

Author: MyQuantTool Team
Date: 2026-02-11
Version: Phase 2
"""

import time
import json
import pandas as pd
from typing import List, Dict, Optional
from datetime import datetime, timedelta
from pathlib import Path
from multiprocessing import Pool, cpu_count

try:
    from xtquant import xtdata
    QMT_AVAILABLE = True
except ImportError:
    QMT_AVAILABLE = False

from logic.batch_qpst_analyzer import BatchQPSTAnalyzer
from logic.logger import get_logger

logger = get_logger(__name__)


class MarketScanner:
    """
    å…¨å¸‚åœºæ‰«æå™¨ï¼ˆä¸‰é˜¶æ®µæ¸è¿›å¼ç­›é€‰ï¼‰
    
    åŠŸèƒ½ï¼š
    1. é˜¶æ®µ1ï¼šé¢„ç­›é€‰ï¼ˆç¡¬æ€§æ¡ä»¶ï¼‰
    2. é˜¶æ®µ2ï¼šå››ç»´åˆç­›ï¼ˆç®€åŒ–QPSTï¼‰
    3. é˜¶æ®µ3ï¼šç²¾å‡†QPSTï¼ˆå®Œæ•´åˆ†æï¼‰
    4. æ™ºèƒ½å¹¶è¡Œï¼ˆè‡ªåŠ¨é€‰æ‹©å•/å¤šè¿›ç¨‹ï¼‰
    5. æ•°æ®ç¼“å­˜ï¼ˆæœ¬åœ°CSVå¤‡ä»½ï¼‰
    """
    
    def __init__(self, 
                 equity_info: dict,
                 cache_dir: str = 'data/kline_cache',
                 enable_cache: bool = True,
                 parallel_threshold: int = 100):
        """
        åˆå§‹åŒ–å¸‚åœºæ‰«æå™¨
        
        Args:
            equity_info: è‚¡æœ¬ä¿¡æ¯å­—å…¸ {code: {float_shares, total_shares}}
            cache_dir: æœ¬åœ°ç¼“å­˜ç›®å½•
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
            parallel_threshold: å¹¶è¡Œé˜ˆå€¼ï¼ˆå€™é€‰è‚¡ç¥¨æ•°>æ­¤å€¼æ—¶ä½¿ç”¨å¤šè¿›ç¨‹ï¼‰
        """
        if not QMT_AVAILABLE:
            raise RuntimeError("âš ï¸ xtquant æœªå®‰è£…ï¼ŒMarketScanner ä¸å¯ç”¨")
        
        self.equity_info = equity_info
        self.cache_dir = Path(cache_dir)
        self.enable_cache = enable_cache
        self.parallel_threshold = parallel_threshold
        
        # åˆå§‹åŒ–åˆ†æå™¨
        self.analyzer = BatchQPSTAnalyzer(equity_info)
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        if enable_cache:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("âœ… MarketScanner åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   - è‚¡æœ¬ä¿¡æ¯: {len(equity_info)} åªè‚¡ç¥¨")
        logger.info(f"   - ç¼“å­˜ç›®å½•: {cache_dir}")
        logger.info(f"   - å¹¶è¡Œé˜ˆå€¼: {parallel_threshold}")
    
    
    def scan(self, stock_list: List[str], scan_time: str = None) -> List[dict]:
        """
        æ‰§è¡Œå…¨å¸‚åœºæ‰«æ
        
        Args:
            stock_list: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆå¦‚ ['300997.SZ', '603697.SH', ...]ï¼‰
            scan_time: æ‰«ææ—¶é—´èŠ‚ç‚¹ï¼ˆ'09:35' | '10:00' | '14:00'ï¼Œé»˜è®¤å½“å‰æ—¶é—´ï¼‰
        
        Returns:
            TOP 50 è¯±å¤šæ¦œå•
        """
        scan_time = scan_time or datetime.now().strftime('%H:%M')
        logger.info("="*80)
        logger.info(f"ğŸš€ å¯åŠ¨å…¨å¸‚åœºæ‰«æ - {scan_time}")
        logger.info(f"ğŸ“Š æ‰«æèŒƒå›´: {len(stock_list)} åªè‚¡ç¥¨")
        logger.info("="*80)
        
        start_time = time.time()
        
        # ===== é˜¶æ®µ1ï¼šé¢„ç­›é€‰ï¼ˆç¡¬æ€§æ¡ä»¶ï¼‰ =====
        logger.info("\nğŸ” é˜¶æ®µ1ï¼šé¢„ç­›é€‰ï¼ˆç¡¬æ€§æ¡ä»¶ï¼‰...")
        phase1_start = time.time()
        candidates = self._phase1_pre_filter(stock_list)
        phase1_time = time.time() - phase1_start
        logger.info(f"âœ… é˜¶æ®µ1å®Œæˆ: {len(stock_list)} â†’ {len(candidates)} åªè‚¡ç¥¨ (è€—æ—¶: {phase1_time:.1f}ç§’)")
        
        if not candidates:
            logger.warning("âš ï¸ é˜¶æ®µ1æœªå‘ç°å€™é€‰è‚¡ç¥¨ï¼Œæ‰«æç»“æŸ")
            return []
        
        # ===== é˜¶æ®µ2ï¼šå››ç»´åˆç­›ï¼ˆç®€åŒ–QPSTï¼‰ =====
        logger.info("\nğŸ” é˜¶æ®µ2ï¼šå››ç»´åˆç­›ï¼ˆç®€åŒ–QPSTï¼‰...")
        phase2_start = time.time()
        potentials = self._phase2_qpst_lite(candidates)
        phase2_time = time.time() - phase2_start
        logger.info(f"âœ… é˜¶æ®µ2å®Œæˆ: {len(candidates)} â†’ {len(potentials)} åªè‚¡ç¥¨ (è€—æ—¶: {phase2_time:.1f}ç§’)")
        
        if not potentials:
            logger.warning("âš ï¸ é˜¶æ®µ2æœªå‘ç°æ½œåœ¨è‚¡ç¥¨ï¼Œæ‰«æç»“æŸ")
            return []
        
        # ===== é˜¶æ®µ3ï¼šç²¾å‡†QPSTï¼ˆå®Œæ•´åˆ†æï¼‰ =====
        logger.info("\nğŸ” é˜¶æ®µ3ï¼šç²¾å‡†QPSTï¼ˆå®Œæ•´åˆ†æï¼‰...")
        phase3_start = time.time()
        trap_list = self._phase3_qpst_full(potentials)
        phase3_time = time.time() - phase3_start
        logger.info(f"âœ… é˜¶æ®µ3å®Œæˆ: {len(potentials)} â†’ {len(trap_list)} åªè‚¡ç¥¨ (è€—æ—¶: {phase3_time:.1f}ç§’)")
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        trap_list.sort(key=lambda x: x['confidence'], reverse=True)
        top_50 = trap_list[:50]
        
        total_time = time.time() - start_time
        logger.info("\n" + "="*80)
        logger.info(f"ğŸ‰ æ‰«æå®Œæˆï¼å…±å‘ç° {len(top_50)} åªç–‘ä¼¼è¯±å¤šè‚¡ç¥¨")
        logger.info(f"â±ï¸  æ€»è€—æ—¶: {total_time:.1f}ç§’")
        logger.info("="*80)
        
        return top_50
    
    
    def _phase1_pre_filter(self, stock_list: List[str]) -> List[str]:
        """
        é˜¶æ®µ1ï¼šé¢„ç­›é€‰ï¼ˆç¡¬æ€§æ¡ä»¶ï¼‰
        
        ç›®æ ‡: 5000è‚¡ â†’ 200-400è‚¡ï¼ˆ1åˆ†é’Ÿå†…å®Œæˆï¼‰
        
        æ¡ä»¶:
        1. æ¶¨å¹… > 2%
        2. æ¢æ‰‹ç‡ > 3%ï¼ˆ10åˆ†é’Ÿï¼‰
        3. æ”¾é‡ > 1.3å€
        """
        candidates = []
        
        try:
            # æ‰¹é‡è·å–æœ€æ–°æ•°æ®ï¼ˆä¸€æ¬¡æ€§è·å–æ‰€æœ‰è‚¡ç¥¨ï¼‰
            logger.debug("  æ­£åœ¨æ‰¹é‡è·å–åˆ†é’ŸKæ•°æ®...")
            kline_data = xtdata.get_market_data_ex(
                field_list=['close', 'volume'],
                stock_list=stock_list,
                period='1m',
                count=10,  # æœ€è¿‘10æ ¹åˆ†é’ŸK
                dividend_type='none',
                fill_data=False
            )
            
            logger.debug(f"  æˆåŠŸè·å– {len(kline_data)} åªè‚¡ç¥¨æ•°æ®")
            
            for code in stock_list:
                if code not in kline_data:
                    continue
                
                df = kline_data[code]
                if len(df) < 10:
                    continue
                
                # è®¡ç®—æ¶¨å¹…
                price_change = (df['close'].iloc[-1] - df['close'].iloc[0]) / df['close'].iloc[0]
                
                # è®¡ç®—é‡æ¯”
                recent_vol = df['volume'].iloc[-3:].mean()
                earlier_vol = df['volume'].iloc[:-3].mean()
                volume_ratio = recent_vol / earlier_vol if earlier_vol > 0 else 0
                
                # è®¡ç®—æ¢æ‰‹ç‡
                float_shares = self.equity_info.get(code, {}).get('float_shares', 0)
                if float_shares > 0:
                    turnover = df['volume'].sum() / float_shares
                else:
                    turnover = 0
                
                # ç¡¬æ€§ç­›é€‰æ¡ä»¶
                if price_change > 0.02 and turnover > 0.03 and volume_ratio > 1.3:
                    candidates.append(code)
            
        except Exception as e:
            logger.error(f"âŒ é˜¶æ®µ1é¢„ç­›é€‰å¤±è´¥: {e}")
            import traceback
            logger.debug(traceback.format_exc())
        
        return candidates
    
    
    def _phase2_qpst_lite(self, candidates: List[str]) -> List[str]:
        """
        é˜¶æ®µ2ï¼šå››ç»´åˆç­›ï¼ˆç®€åŒ–QPSTï¼‰
        
        ç›®æ ‡: 200-400è‚¡ â†’ 50-100è‚¡ï¼ˆ3åˆ†é’Ÿå†…å®Œæˆï¼‰
        
        åªè®¡ç®—å…³é”®æŒ‡æ ‡ï¼Œä¸åšå®Œæ•´åˆ†æ
        """
        potentials = []
        
        try:
            # æ‰¹é‡è·å–åˆ†é’ŸKæ•°æ®
            logger.debug("  æ­£åœ¨æ‰¹é‡è·å–åˆ†é’ŸKæ•°æ®...")
            kline_data = xtdata.get_market_data_ex(
                field_list=['open', 'high', 'low', 'close', 'volume', 'amount'],
                stock_list=candidates,
                period='1m',
                count=10,
                dividend_type='none',
                fill_data=False
            )
            
            for code in candidates:
                if code not in kline_data:
                    continue
                
                df = kline_data[code]
                if len(df) < 10:
                    continue
                
                # æ·»åŠ timeåˆ—ï¼ˆç”¨äºæ—¶é—´åˆ¤æ–­ï¼‰
                if 'time' not in df.columns:
                    df['time'] = pd.date_range(end=datetime.now(), periods=len(df), freq='1min').time
                
                # å¿«é€Ÿå››ç»´åˆ¤æ–­
                dims = self.analyzer.analyze_lite(df, code)
                
                # è‡³å°‘2ä¸ªç»´åº¦å¼‚å¸¸æ‰è¿›å…¥ä¸‹ä¸€é˜¶æ®µ
                abnormal_count = sum(1 for v in dims.values() if v == 'ABNORMAL')
                
                if abnormal_count >= 2:
                    potentials.append(code)
            
        except Exception as e:
            logger.error(f"âŒ é˜¶æ®µ2åˆç­›å¤±è´¥: {e}")
            import traceback
            logger.debug(traceback.format_exc())
        
        return potentials
    
    
    def _phase3_qpst_full(self, potentials: List[str]) -> List[dict]:
        """
        é˜¶æ®µ3ï¼šç²¾å‡†QPSTï¼ˆå®Œæ•´åˆ†æï¼‰
        
        ç›®æ ‡: 50-100è‚¡ â†’ 20-50è‚¡ï¼ˆ5åˆ†é’Ÿå†…å®Œæˆï¼‰
        
        æ‰§è¡Œå®Œæ•´çš„å››ç»´åˆ†æ + åè¯±å¤šæ£€æµ‹
        """
        trap_list = []
        
        # æ™ºèƒ½é€‰æ‹©å•è¿›ç¨‹æˆ–å¤šè¿›ç¨‹
        use_parallel = len(potentials) > self.parallel_threshold
        
        if use_parallel:
            logger.info(f"  ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œï¼ˆ{len(potentials)}åªè‚¡ç¥¨ > {self.parallel_threshold}ï¼‰")
            trap_list = self._analyze_parallel(potentials)
        else:
            logger.info(f"  ä½¿ç”¨å•è¿›ç¨‹ä¸²è¡Œï¼ˆ{len(potentials)}åªè‚¡ç¥¨ â‰¤ {self.parallel_threshold}ï¼‰")
            trap_list = self._analyze_serial(potentials)
        
        # è¿‡æ»¤å‡ºè¯±å¤šé¢„è­¦
        trap_list = [r for r in trap_list if r and r['final_signal'] == 'TRAP_WARNING']
        
        return trap_list
    
    
    def _analyze_serial(self, stock_list: List[str]) -> List[dict]:
        """
        å•è¿›ç¨‹ä¸²è¡Œåˆ†æ
        """
        results = []
        
        try:
            # æ‰¹é‡è·å–åˆ†é’ŸKæ•°æ®
            kline_data = xtdata.get_market_data_ex(
                field_list=['open', 'high', 'low', 'close', 'volume', 'amount'],
                stock_list=stock_list,
                period='1m',
                count=30,  # å®Œæ•´åˆ†æéœ€è¦æ›´å¤šæ•°æ®
                dividend_type='none',
                fill_data=False
            )
            
            for code in stock_list:
                if code not in kline_data:
                    continue
                
                df = kline_data[code]
                if len(df) < 20:
                    continue
                
                # æ·»åŠ timeåˆ—
                if 'time' not in df.columns:
                    df['time'] = pd.date_range(end=datetime.now(), periods=len(df), freq='1min').time
                
                # æ‰§è¡Œå®Œæ•´QPSTåˆ†æ
                result = self.analyzer.analyze_full(df, code)
                if result:
                    results.append(result)
        
        except Exception as e:
            logger.error(f"âŒ ä¸²è¡Œåˆ†æå¤±è´¥: {e}")
            import traceback
            logger.debug(traceback.format_exc())
        
        return results
    
    
    def _analyze_parallel(self, stock_list: List[str]) -> List[dict]:
        """
        å¤šè¿›ç¨‹å¹¶è¡Œåˆ†æ
        """
        results = []
        
        try:
            # æ‰¹é‡è·å–åˆ†é’ŸKæ•°æ®ï¼ˆä¸»è¿›ç¨‹ï¼‰
            kline_data = xtdata.get_market_data_ex(
                field_list=['open', 'high', 'low', 'close', 'volume', 'amount'],
                stock_list=stock_list,
                period='1m',
                count=30,
                dividend_type='none',
                fill_data=False
            )
            
            # å‡†å¤‡åˆ†æä»»åŠ¡
            tasks = []
            for code in stock_list:
                if code not in kline_data:
                    continue
                
                df = kline_data[code]
                if len(df) < 20:
                    continue
                
                # æ·»åŠ timeåˆ—
                if 'time' not in df.columns:
                    df['time'] = pd.date_range(end=datetime.now(), periods=len(df), freq='1min').time
                
                tasks.append((df, code, self.equity_info))
            
            # å¤šè¿›ç¨‹æ‰§è¡Œ
            num_processes = min(cpu_count(), 8)  # æœ€å¤š8ä¸ªè¿›ç¨‹
            logger.debug(f"  å¯åŠ¨ {num_processes} ä¸ªè¿›ç¨‹...")
            
            with Pool(processes=num_processes) as pool:
                results = pool.starmap(self._analyze_single_stock_worker, tasks)
            
            # è¿‡æ»¤ç©ºç»“æœ
            results = [r for r in results if r is not None]
        
        except Exception as e:
            logger.error(f"âŒ å¹¶è¡Œåˆ†æå¤±è´¥: {e}")
            import traceback
            logger.debug(traceback.format_exc())
        
        return results
    
    
    @staticmethod
    def _analyze_single_stock_worker(df: pd.DataFrame, code: str, equity_info: dict) -> Optional[dict]:
        """
        å•è‚¡ç¥¨åˆ†æå·¥ä½œå‡½æ•°ï¼ˆä¾›å¤šè¿›ç¨‹è°ƒç”¨ï¼‰
        
        æ³¨æ„ï¼šæ­¤å‡½æ•°åœ¨å­è¿›ç¨‹ä¸­æ‰§è¡Œï¼Œéœ€è¦é‡æ–°åˆå§‹åŒ–analyzer
        """
        try:
            # åœ¨å­è¿›ç¨‹ä¸­åˆ›å»ºanalyzer
            analyzer = BatchQPSTAnalyzer(equity_info)
            result = analyzer.analyze_full(df, code)
            return result
        except Exception as e:
            # å­è¿›ç¨‹ä¸­çš„å¼‚å¸¸ä¸ä¼šä¼ é€’åˆ°ä¸»è¿›ç¨‹ï¼Œéœ€è¦è®°å½•
            return None
    
    
    def save_results(self, results: List[dict], output_dir: str = 'data/scan_results'):
        """
        ä¿å­˜æ‰«æç»“æœ
        
        Args:
            results: æ‰«æç»“æœåˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M')
        
        # ä¿å­˜JSON
        json_file = output_path / f'scan_{timestamp}.json'
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ æ‰«æç»“æœå·²ä¿å­˜: {json_file}")
        
        return str(json_file)
