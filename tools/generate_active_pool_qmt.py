#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çŸ­çº¿æ´»è·ƒè‚¡ç”Ÿæˆè„šæœ¬ (QMT æ•°æ®ç‰ˆæœ¬)

ä½¿ç”¨å·²ä¸‹è½½çš„ 1åˆ†é’Ÿ Kçº¿æ•°æ®ï¼Œç­›é€‰å‡º 500 åªçŸ­çº¿æœ€æ´»è·ƒçš„è‚¡ç¥¨

ç­›é€‰æ¡ä»¶ï¼š
- æ—¥å‡æ¢æ‰‹ç‡ > 3% (æ´»è·ƒ)
- è¿‘ 60 å¤©æœ‰æ¶¨åœ (æœ‰å¦–æ°”)
- æ—¥å‡æˆäº¤é¢ < 50äº¿ (å‰”é™¤è¶…çº§å¤§è±¡)
- å‰”é™¤åœç‰Œè‚¡ç¥¨

æ•°æ®æºï¼šå·²ä¸‹è½½çš„ 1åˆ†é’Ÿ Kçº¿æ•°æ®ï¼ˆ19.28 GBï¼Œ5190 åªè‚¡ç¥¨ï¼‰
"""

import sys
import os
import json
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
import time

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from logic.utils.logger import get_logger

logger = get_logger("generate_active_pool_qmt")

# VIP Token
VIP_TOKEN = '6b1446e317ed67596f13d2e808291a01e0dd9839'


def start_token_service():
    """å¯åŠ¨ xtdatacenter è¡Œæƒ…æœåŠ¡ (Token æ¨¡å¼)"""
    try:
        from xtquant import xtdatacenter as xtdc
    except ImportError:
        logger.error("âŒ æ— æ³•å¯¼å…¥ xtquantï¼Œè¯·æ£€æŸ¥ç¯å¢ƒ")
        return None

    data_dir = PROJECT_ROOT / 'data' / 'qmt_data'
    data_dir.mkdir(parents=True, exist_ok=True)
    xtdc.set_data_home_dir(str(data_dir))
    xtdc.set_token(VIP_TOKEN)
    xtdc.init()
    listen_port = xtdc.listen(port=(58623, 58625))
    logger.info(f"ğŸš€ è¡Œæƒ…æœåŠ¡å·²å¯åŠ¨ï¼Œç›‘å¬ç«¯å£: {listen_port}")
    return listen_port


def generate_active_pool():
    """
    ä½¿ç”¨ QMT 1åˆ†é’Ÿ Kçº¿æ•°æ®ç”ŸæˆçŸ­çº¿æ´»è·ƒè‚¡åå•
    """
    logger.info("=" * 80)
    logger.info("ğŸ” å¼€å§‹ç”ŸæˆçŸ­çº¿æ´»è·ƒè‚¡åå• (QMT æ•°æ®ç‰ˆæœ¬)")
    logger.info("=" * 80)
    
    # å¯åŠ¨ Token æœåŠ¡
    port = start_token_service()
    if not port:
        logger.error("âŒ Token æœåŠ¡å¯åŠ¨å¤±è´¥")
        return []
    
    # è¿æ¥è¡Œæƒ…æœåŠ¡
    from xtquant import xtdata
    ip, port_num = port
    xtdata.connect(ip='127.0.0.1', port=port_num, remember_if_success=False)
    time.sleep(2)
    
    # è·å–å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨ï¼ˆåŸºäºå·²ä¸‹è½½æ•°æ®ï¼‰
    data_dir = PROJECT_ROOT / 'data' / 'qmt_data' / 'datadir'
    
    stock_list = []
    for market in ['SH', 'SZ']:
        kline_dir = data_dir / market / '60'
        if kline_dir.exists():
            files = list(kline_dir.glob('*.DAT'))
            for f in files:
                code = f"{f.stem}.{market}"
                stock_list.append(code)
    
    logger.info(f"   å‘ç°å·²ä¸‹è½½æ•°æ®çš„è‚¡ç¥¨: {len(stock_list)} åª")
    
    # è¯»å– 1åˆ†é’Ÿ Kçº¿æ•°æ®å¹¶åˆæˆæ—¥çº¿
    valid_stocks = []
    skipped_reasons = {
        'æ•°æ®ä¸è¶³': 0,
        'æ¢æ‰‹ç‡ä½': 0,
        'æ— æ¶¨åœ': 0,
        'æˆäº¤é¢è¿‡å¤§': 0,
        'å…¶ä»–': 0
    }
    
    logger.info(f"   å¼€å§‹å¤„ç† {len(stock_list)} åªè‚¡ç¥¨...")
    
    for idx, code in enumerate(stock_list):
        try:
            # è¯»å–è¿‘ 60 å¤©çš„ 1åˆ†é’Ÿ Kçº¿æ•°æ®
            data = xtdata.get_local_data(
                stock_list=[code],
                period='1m',
                count=60 * 390  # 60 å¤© * 390 åˆ†é’Ÿ/å¤©
            )
            
            if code not in data or data[code].empty:
                skipped_reasons['æ•°æ®ä¸è¶³'] += 1
                continue
            
            df = data[code]
            
            if len(df) < 390:  # å°‘äº 1 å¤©çš„æ•°æ®
                skipped_reasons['æ•°æ®ä¸è¶³'] += 1
                continue
            
            # åˆæˆæ—¥çº¿æ•°æ®
            df.index = pd.to_datetime(df.index, unit='s')
            df = df.resample('D').agg({
                'open': 'first',
                'high': 'max',
                'low': 'min',
                'close': 'last',
                'volume': 'sum',
                'amount': 'sum'
            }).dropna()
            
            if len(df) < 20:
                skipped_reasons['æ•°æ®ä¸è¶³'] += 1
                continue
            
            # è®¡ç®—æ¶¨è·Œå¹…
            df['pct_change'] = df['close'].pct_change() * 100
            
            # è®¡ç®—æ¢æ‰‹ç‡ï¼ˆç”¨æˆäº¤é¢ä¼°ç®—ï¼‰
            # å‡è®¾å¹³å‡æˆäº¤ä»· = æˆäº¤é¢ / æˆäº¤é‡
            # æ¢æ‰‹ç‡ = æˆäº¤é‡ / æµé€šè‚¡æœ¬
            # ç”±äºæ²¡æœ‰æµé€šè‚¡æœ¬æ•°æ®ï¼Œæˆ‘ä»¬ç”¨æˆäº¤é¢ / å‡ä»·æ¥ä¼°ç®—æ´»è·ƒåº¦
            df['turnover_est'] = (df['amount'] / df['close']) / 100000000  # äº¿å…ƒ
            
            # 1. æ—¥å‡æ¢æ‰‹ç‡ä¼°ç®— (æœ€è¿‘ 20 å¤©)
            avg_turnover = df['turnover_est'].tail(20).mean()
            
            # 2. æ£€æŸ¥æ˜¯å¦æœ‰æ¶¨åœ
            # ç®€å•åˆ¤æ–­ï¼šæ¶¨å¹… >= 9.5%
            has_limit = (df['pct_change'] >= 9.5).any()
            
            # 3. æ—¥å‡æˆäº¤é¢
            avg_amount = df['amount'].tail(20).mean()
            
            # ç­›é€‰é€»è¾‘ï¼š
            # - æ¢æ‰‹ç‡ä¼°ç®— > 1 äº¿ï¼ˆæ´»è·ƒï¼‰
            # - è¿‘ 60 å¤©æœ‰æ¶¨åœ (æœ‰å¦–æ°”)
            # - æ—¥å‡æˆäº¤é¢ < 50äº¿ (å‰”é™¤è¶…çº§å¤§è±¡)
            
            if avg_turnover > 1.0 and has_limit and avg_amount < 50e8:
                valid_stocks.append({
                    'code': code,
                    'avg_turnover': round(avg_turnover, 2),
                    'has_limit': has_limit,
                    'avg_amount': round(avg_amount / 1e8, 2),  # äº¿å…ƒ
                    'last_price': round(df['close'].iloc[-1], 2),
                    'pct_change': round(df['pct_change'].iloc[-1], 2) if len(df['pct_change']) > 1 else 0
                })
            else:
                if avg_turnover <= 1.0:
                    skipped_reasons['æ¢æ‰‹ç‡ä½'] += 1
                elif not has_limit:
                    skipped_reasons['æ— æ¶¨åœ'] += 1
                elif avg_amount >= 50e8:
                    skipped_reasons['æˆäº¤é¢è¿‡å¤§'] += 1
        
        except Exception as e:
            logger.debug(f"   è·³è¿‡ {code}: {e}")
            skipped_reasons['å…¶ä»–'] += 1
            continue
        
        # æ˜¾ç¤ºè¿›åº¦
        if (idx + 1) % 500 == 0:
            logger.info(f"   è¿›åº¦: {idx + 1}/{len(stock_list)}, å·²ç­›é€‰: {len(valid_stocks)} åª")
    
    # æ’åºï¼šæŒ‰æ¢æ‰‹ç‡å€’åº
    valid_stocks.sort(key=lambda x: x['avg_turnover'], reverse=True)
    
    # å– Top 500
    top_500 = valid_stocks[:500]
    
    logger.info("=" * 80)
    logger.info(f"âœ… ç­›é€‰å®Œæˆï¼å…¥é€‰ {len(top_500)} åª")
    logger.info("=" * 80)
    logger.info(f"è·³è¿‡åŸå› ç»Ÿè®¡:")
    for reason, count in skipped_reasons.items():
        logger.info(f"   - {reason}: {count} åª")
    
    logger.info(f"\næ¦œå–¾ç¤ºä¾‹ (Top 20):")
    for i, stock in enumerate(top_500[:20]):
        logger.info(f"   {i+1}. {stock['code']} | æ¢æ‰‹ç‡: {stock['avg_turnover']}äº¿ | æ¶¨åœ: {'æ˜¯' if stock['has_limit'] else 'å¦'} | æˆäº¤é¢: {stock['avg_amount']}äº¿ | æ¶¨å¹…: {stock['pct_change']}%")
    
    # ä¿å­˜ä¸º JSON (ä¸¤ç§æ ¼å¼)
    output_dir = PROJECT_ROOT / 'config'
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æ ¼å¼ 1ï¼šä»…ä»£ç åˆ—è¡¨
    code_list_file = output_dir / 'active_stocks.json'
    code_list = [stock['code'] for stock in top_500]
    with open(code_list_file, 'w', encoding='utf-8') as f:
        json.dump(code_list, f, ensure_ascii=False, indent=2)
    logger.info(f"ğŸ’¾ ä»£ç åˆ—è¡¨å·²ä¿å­˜è‡³: {code_list_file}")
    
    # æ ¼å¼ 2ï¼šè¯¦ç»†ä¿¡æ¯
    detail_file = output_dir / 'active_stocks_detail.json'
    with open(detail_file, 'w', encoding='utf-8') as f:
        json.dump(top_500, f, ensure_ascii=False, indent=2)
    logger.info(f"ğŸ’¾ è¯¦ç»†ä¿¡æ¯å·²ä¿å­˜è‡³: {detail_file}")
    
    return code_list


if __name__ == "__main__":
    result = generate_active_pool()
    
    if result:
        logger.info("=" * 80)
        logger.info(f"ğŸ‰ æˆåŠŸç”Ÿæˆ {len(result)} åªçŸ­çº¿æ´»è·ƒè‚¡ï¼")
        logger.info("=" * 80)
        logger.info("è¯·æŸ¥çœ‹ä»¥ä¸‹æ–‡ä»¶:")
        logger.info("  - config/active_stocks.json (ä»£ç åˆ—è¡¨)")
        logger.info("  - config/active_stocks_detail.json (è¯¦ç»†ä¿¡æ¯)")
    else:
        logger.error("âŒ ç”Ÿæˆå¤±è´¥")
