import os
import shutil
import json
import time
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Tuple, Set
import logging
import argparse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥é¡¹ç›®ç°æœ‰æ¨¡å—
try:
    from logic.utils.logger import get_logger
    logger = get_logger(__name__)
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨æ ‡å‡†logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    logger = logging.getLogger(__name__)


class ArchiveCleanupAnalyzer:
    """å½’æ¡£æ–‡ä»¶æ¸…ç†åˆ†æå™¨"""

    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root) if project_root else Path(__file__).parent.parent
        self.data_dir = self.project_root / "data"
        self.backup_dir = self.project_root / "temp" / "cleanup_backup"
        self.log_dir = self.project_root / "logs"

        # é…ç½®
        self.max_age_hours = 24  # æœ€å¤§ä¿ç•™æ—¶é—´ï¼ˆå°æ—¶ï¼‰
        self.protected_patterns = [
            # ç»å¯¹ä¿æŠ¤çš„ç›®å½•å’Œæ–‡ä»¶
            "qmt_data",
            "datadir",
            "log",
            "quoter",
            "cache",
            "data/stock_names.json",
            "data/stock_sector_map.json",
            "data/monitor_state.json",
            "data/scheduled_alerts.json",
            "*.db",
            "*.sqlite",
            "*.lock",
        ]

        # é«˜ä»·å€¼æ¡ˆä¾‹ï¼ˆä¿ç•™ï¼‰
        self.preserve_cases = [
            "300997",  # æ¬¢ä¹å®¶è¯±å¤šæ¡ˆä¾‹
            "603697",  # æœ‰å‹é£Ÿå“æ¸¸èµ„æ¡ˆä¾‹
        ]

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_files': 0,
            'total_size': 0,
            'expired_files': 0,
            'expired_size': 0,
            'protected_files': 0,
            'preserved_files': 0,
            'deleted_files': 0,
            'deleted_size': 0,
            'backed_up_files': 0,
            'backed_up_size': 0,
        }

        logger.info(f"âœ… [å½’æ¡£æ¸…ç†] åˆå§‹åŒ–å®Œæˆï¼Œé¡¹ç›®æ ¹ç›®å½•: {self.project_root}")

    def scan_data_directory(self) -> List[Dict]:
        """æ‰«ædataç›®å½•ï¼Œåˆ†ææ–‡ä»¶æƒ…å†µ"""
        logger.info(f"ğŸ“Š [å½’æ¡£æ¸…ç†] å¼€å§‹æ‰«ædataç›®å½•: {self.data_dir}")

        if not self.data_dir.exists():
            logger.warning(f"âš ï¸ [å½’æ¡£æ¸…ç†] dataç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
            return []

        files_info = []
        current_time = time.time()

        # éå†dataç›®å½•
        for item in self.data_dir.rglob("*"):
            if not item.is_file():
                continue

            try:
                # è·å–æ–‡ä»¶ä¿¡æ¯
                file_stat = item.stat()
                file_size = file_stat.st_size
                mod_time = file_stat.st_mtime
                age_hours = (current_time - mod_time) / 3600

                # åˆ¤æ–­æ˜¯å¦è¿‡æœŸ
                is_expired = age_hours > self.max_age_hours

                # åˆ¤æ–­æ˜¯å¦å—ä¿æŠ¤
                is_protected = self._is_protected(item)

                # åˆ¤æ–­æ˜¯å¦æ˜¯é«˜ä»·å€¼æ¡ˆä¾‹
                is_preserve = self._is_preserve_case(item)

                file_info = {
                    'path': str(item.relative_to(self.project_root)),
                    'full_path': str(item),
                    'size': file_size,
                    'age_hours': age_hours,
                    'mod_time': datetime.fromtimestamp(mod_time).strftime('%Y-%m-%d %H:%M:%S'),
                    'is_expired': is_expired,
                    'is_protected': is_protected,
                    'is_preserve': is_preserve,
                    'extension': item.suffix,
                    'directory': str(item.parent.relative_to(self.project_root)),
                }

                files_info.append(file_info)

                # æ›´æ–°ç»Ÿè®¡
                self.stats['total_files'] += 1
                self.stats['total_size'] += file_size

                if is_expired:
                    self.stats['expired_files'] += 1
                    self.stats['expired_size'] += file_size

                if is_protected:
                    self.stats['protected_files'] += 1

                if is_preserve:
                    self.stats['preserved_files'] += 1

            except Exception as e:
                logger.error(f"âŒ [å½’æ¡£æ¸…ç†] åˆ†ææ–‡ä»¶å¤±è´¥: {item} - {e}")
                continue

        logger.info(
            f"âœ… [å½’æ¡£æ¸…ç†] æ‰«æå®Œæˆ: "
            f"æ€»æ–‡ä»¶{self.stats['total_files']}ä¸ª, "
            f"æ€»å¤§å°{self._format_size(self.stats['total_size'])}, "
            f"è¿‡æœŸ{self.stats['expired_files']}ä¸ª "
            f"({self._format_size(self.stats['expired_size'])})"
        )

        return files_info

    def analyze_cleanup_candidates(self, files_info: List[Dict], force: bool = False) -> List[Dict]:
        """åˆ†ææ¸…ç†å€™é€‰æ–‡ä»¶"""
        logger.info(f"ğŸ” [å½’æ¡£æ¸…ç†] åˆ†ææ¸…ç†å€™é€‰æ–‡ä»¶ (force={force})")

        candidates = []

        for file_info in files_info:
            # è·³è¿‡å—ä¿æŠ¤çš„æ–‡ä»¶
            if file_info['is_protected']:
                continue

            # è·³è¿‡é«˜ä»·å€¼æ¡ˆä¾‹
            if file_info['is_preserve']:
                continue

            # åˆ¤æ–­æ˜¯å¦éœ€è¦æ¸…ç†
            if force:
                # å¼ºåˆ¶æ¨¡å¼ï¼šåˆ é™¤æ‰€æœ‰è¿‡æœŸæ–‡ä»¶
                if file_info['is_expired']:
                    candidates.append(file_info)
            else:
                # ä¿å®ˆæ¨¡å¼ï¼šåªæ¸…ç†æ˜æ˜¾æ— ç”¨çš„æ–‡ä»¶
                # 1. å·²è¿‡æœŸä¸”æ— ä¾èµ–çš„æ—¥å¿—æ–‡ä»¶
                # 2. å·²è¿‡æœŸä¸”æ— ä¾èµ–çš„ä¸´æ—¶æ–‡ä»¶
                # 3. å·²è¿‡æœŸä¸”æ— ä¾èµ–çš„åˆ†æç»“æœæ–‡ä»¶
                if file_info['is_expired']:
                    # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
                    ext = file_info['extension'].lower()
                    directory = file_info['directory']

                    # ä¿å®ˆæ¸…ç†æ¡ä»¶
                    if (
                        ext in ['.log', '.txt', '.tmp', '.bak'] or  # æ—¥å¿—å’Œä¸´æ—¶æ–‡ä»¶
                        'stock_analysis' in directory or  # è‚¡ç¥¨åˆ†æç»“æœ
                        'scan_results' in directory or  # æ‰«æç»“æœ
                        'rebuild_snapshots' in directory  # é‡å»ºå¿«ç…§
                    ):
                        candidates.append(file_info)

        logger.info(
            f"âœ… [å½’æ¡£æ¸…ç†] åˆ†æå®Œæˆ: "
            f"æ¸…ç†å€™é€‰{len(candidates)}ä¸ª, "
            f"å¤§å°{self._format_size(sum(f['size'] for f in candidates))}"
        )

        return candidates

    def backup_files(self, candidates: List[Dict]) -> bool:
        """å¤‡ä»½æ–‡ä»¶åˆ°temp/cleanup_backup/"""
        if not candidates:
            return True

        logger.info(f"ğŸ’¾ [å½’æ¡£æ¸…ç†] å¼€å§‹å¤‡ä»½{len(candidates)}ä¸ªæ–‡ä»¶...")

        try:
            # åˆ›å»ºå¤‡ä»½ç›®å½•
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_path = self.backup_dir / timestamp
            backup_path.mkdir(parents=True, exist_ok=True)

            # å¤‡ä»½æ–‡ä»¶
            for file_info in candidates:
                src_path = Path(file_info['full_path'])
                rel_path = Path(file_info['path'])
                dst_path = backup_path / rel_path

                # åˆ›å»ºç›®æ ‡ç›®å½•
                dst_path.parent.mkdir(parents=True, exist_ok=True)

                # å¤åˆ¶æ–‡ä»¶
                shutil.copy2(src_path, dst_path)

                self.stats['backed_up_files'] += 1
                self.stats['backed_up_size'] += file_info['size']

            logger.info(
                f"âœ… [å½’æ¡£æ¸…ç†] å¤‡ä»½å®Œæˆ: "
                f"{self.stats['backed_up_files']}ä¸ªæ–‡ä»¶, "
                f"{self._format_size(self.stats['backed_up_size'])}, "
                f"å¤‡ä»½ç›®å½•: {backup_path}"
            )

            return True

        except Exception as e:
            logger.error(f"âŒ [å½’æ¡£æ¸…ç†] å¤‡ä»½å¤±è´¥: {e}")
            return False

    def cleanup_files(self, candidates: List[Dict], dry_run: bool = False) -> bool:
        """æ¸…ç†æ–‡ä»¶"""
        if not candidates:
            logger.info("â„¹ï¸ [å½’æ¡£æ¸…ç†] æ²¡æœ‰æ–‡ä»¶éœ€è¦æ¸…ç†")
            return True

        if dry_run:
            logger.info(f"ğŸ” [å½’æ¡£æ¸…ç†] æ¨¡æ‹Ÿè¿è¡Œæ¨¡å¼ï¼Œä¸ä¼šå®é™…åˆ é™¤æ–‡ä»¶")
            for file_info in candidates:
                logger.info(f"  å°†åˆ é™¤: {file_info['path']} ({self._format_size(file_info['size'])})")
            return True

        logger.info(f"ğŸ—‘ï¸ [å½’æ¡£æ¸…ç†] å¼€å§‹æ¸…ç†{len(candidates)}ä¸ªæ–‡ä»¶...")

        try:
            for file_info in candidates:
                file_path = Path(file_info['full_path'])

                # åˆ é™¤æ–‡ä»¶
                file_path.unlink()

                self.stats['deleted_files'] += 1
                self.stats['deleted_size'] += file_info['size']

                logger.debug(f"âœ… å·²åˆ é™¤: {file_info['path']}")

            logger.info(
                f"âœ… [å½’æ¡£æ¸…ç†] æ¸…ç†å®Œæˆ: "
                f"{self.stats['deleted_files']}ä¸ªæ–‡ä»¶, "
                f"{self._format_size(self.stats['deleted_size'])}"
            )

            return True

        except Exception as e:
            logger.error(f"âŒ [å½’æ¡£æ¸…ç†] æ¸…ç†å¤±è´¥: {e}")
            return False

    def generate_report(self, candidates: List[Dict], dry_run: bool = False) -> Tuple[str, str]:
        """ç”Ÿæˆæ¸…ç†æŠ¥å‘Šï¼ˆJSON + TXTï¼‰"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # JSONæŠ¥å‘Š
        json_report = {
            'timestamp': timestamp,
            'dry_run': dry_run,
            'stats': self.stats,
            'protected_patterns': self.protected_patterns,
            'preserve_cases': self.preserve_cases,
            'candidates': candidates,
        }

        json_path = self.log_dir / f"cleanup_report_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_report, f, indent=2, ensure_ascii=False)

        # TXTæŠ¥å‘Š
        txt_lines = [
            "=" * 80,
            "å½’æ¡£æ–‡ä»¶æ¸…ç†æŠ¥å‘Š",
            "=" * 80,
            f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"é¡¹ç›®æ ¹ç›®å½•: {self.project_root}",
            f"è¿è¡Œæ¨¡å¼: {'æ¨¡æ‹Ÿè¿è¡Œ' if dry_run else 'å®é™…æ¸…ç†'}",
            "",
            "-" * 80,
            "ç»Ÿè®¡ä¿¡æ¯",
            "-" * 80,
            f"æ€»æ–‡ä»¶æ•°: {self.stats['total_files']}",
            f"æ€»å¤§å°: {self._format_size(self.stats['total_size'])}",
            f"è¿‡æœŸæ–‡ä»¶æ•°: {self.stats['expired_files']}",
            f"è¿‡æœŸå¤§å°: {self._format_size(self.stats['expired_size'])}",
            f"å—ä¿æŠ¤æ–‡ä»¶æ•°: {self.stats['protected_files']}",
            f"é«˜ä»·å€¼æ¡ˆä¾‹æ•°: {self.stats['preserved_files']}",
            f"å¤‡ä»½æ–‡ä»¶æ•°: {self.stats['backed_up_files']}",
            f"å¤‡ä»½å¤§å°: {self._format_size(self.stats['backed_up_size'])}",
            f"åˆ é™¤æ–‡ä»¶æ•°: {self.stats['deleted_files']}",
            f"åˆ é™¤å¤§å°: {self._format_size(self.stats['deleted_size'])}",
            "",
            "-" * 80,
            "æ¸…ç†å€™é€‰æ–‡ä»¶",
            "-" * 80,
        ]

        for i, file_info in enumerate(candidates, 1):
            txt_lines.append(
                f"{i}. {file_info['path']} "
                f"({self._format_size(file_info['size'])}, "
                f"è¿‡æœŸ{file_info['age_hours']:.1f}å°æ—¶)"
            )

        txt_lines.extend([
            "",
            "-" * 80,
            "å—ä¿æŠ¤æ¨¡å¼",
            "-" * 80,
        ])

        for pattern in self.protected_patterns:
            txt_lines.append(f"  - {pattern}")

        txt_lines.extend([
            "",
            "-" * 80,
            "é«˜ä»·å€¼æ¡ˆä¾‹",
            "-" * 80,
        ])

        for case in self.preserve_cases:
            txt_lines.append(f"  - {case}")

        txt_lines.extend([
            "",
            "=" * 80,
            f"JSONæŠ¥å‘Š: {json_path}",
            "=" * 80,
        ])

        txt_path = self.log_dir / f"cleanup_report_{timestamp}.txt"
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(txt_lines))

        logger.info(f"âœ… [å½’æ¡£æ¸…ç†] æŠ¥å‘Šå·²ç”Ÿæˆ: {txt_path}")

        return str(json_path), str(txt_path)

    def _is_protected(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦å—ä¿æŠ¤"""
        relative_path = str(file_path.relative_to(self.project_root))

        for pattern in self.protected_patterns:
            if pattern in relative_path:
                return True

        return False

    def _is_preserve_case(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦æ˜¯é«˜ä»·å€¼æ¡ˆä¾‹"""
        relative_path = str(file_path.relative_to(self.project_root))

        for case in self.preserve_cases:
            if case in relative_path:
                return True

        return False

    def _format_size(self, size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.2f}{unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.2f}TB"


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='å½’æ¡£æ–‡ä»¶è‡ªåŠ¨åŒ–æ¸…ç†è„šæœ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # æ¨¡æ‹Ÿè¿è¡Œï¼ˆæ¨èå…ˆæ‰§è¡Œï¼‰
  python tools/cleanup_archives.py --dry-run

  # å®é™…æ¸…ç†ï¼ˆä¿å®ˆæ–¹æ¡ˆï¼‰
  python tools/cleanup_archives.py

  # å®é™…æ¸…ç†ï¼ˆæ¿€è¿›æ–¹æ¡ˆï¼‰
  python tools/cleanup_archives.py --force

  # ä¸åˆ›å»ºå¤‡ä»½ï¼ˆä¸æ¨èï¼‰
  python tools/cleanup_archives.py --no-backup
        """
    )

    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='æ¨¡æ‹Ÿè¿è¡Œï¼Œä¸å®é™…åˆ é™¤æ–‡ä»¶'
    )

    parser.add_argument(
        '--force',
        action='store_true',
        help='å¼ºåˆ¶æ¸…ç†ï¼Œåˆ é™¤æ‰€æœ‰è¿‡æœŸæ–‡ä»¶'
    )

    parser.add_argument(
        '--no-backup',
        action='store_true',
        help='ä¸åˆ›å»ºå¤‡ä»½ï¼ˆä¸æ¨èï¼‰'
    )

    args = parser.parse_args()

    # åˆ›å»ºåˆ†æå™¨
    analyzer = ArchiveCleanupAnalyzer()

    # æ‰«ædataç›®å½•
    files_info = analyzer.scan_data_directory()

    if not files_info:
        logger.info("â„¹ï¸ [å½’æ¡£æ¸…ç†] æ²¡æœ‰å‘ç°éœ€è¦æ¸…ç†çš„æ–‡ä»¶")
        return

    # åˆ†ææ¸…ç†å€™é€‰æ–‡ä»¶
    candidates = analyzer.analyze_cleanup_candidates(files_info, force=args.force)

    if not candidates:
        logger.info("â„¹ï¸ [å½’æ¡£æ¸…ç†] æ²¡æœ‰å‘ç°éœ€è¦æ¸…ç†çš„æ–‡ä»¶")
        return

    # å¤‡ä»½æ–‡ä»¶
    if not args.no_backup:
        if not analyzer.backup_files(candidates):
            logger.error("âŒ [å½’æ¡£æ¸…ç†] å¤‡ä»½å¤±è´¥ï¼Œç»ˆæ­¢æ¸…ç†")
            return

    # æ¸…ç†æ–‡ä»¶
    if not analyzer.cleanup_files(candidates, dry_run=args.dry_run):
        logger.error("âŒ [å½’æ¡£æ¸…ç†] æ¸…ç†å¤±è´¥")
        return

    # ç”ŸæˆæŠ¥å‘Š
    json_path, txt_path = analyzer.generate_report(candidates, dry_run=args.dry_run)

    # è¾“å‡ºæ€»ç»“
    print("\n" + "=" * 80)
    print("å½’æ¡£æ–‡ä»¶æ¸…ç†å®Œæˆ")
    print("=" * 80)
    print(f"æ‰«ææ–‡ä»¶æ•°: {analyzer.stats['total_files']}")
    print(f"æ‰«æå¤§å°: {analyzer._format_size(analyzer.stats['total_size'])}")
    print(f"æ¸…ç†æ–‡ä»¶æ•°: {analyzer.stats['deleted_files']}")
    print(f"æ¸…ç†å¤§å°: {analyzer._format_size(analyzer.stats['deleted_size'])}")
    print(f"å¤‡ä»½æ–‡ä»¶æ•°: {analyzer.stats['backed_up_files']}")
    print(f"å¤‡ä»½å¤§å°: {analyzer._format_size(analyzer.stats['backed_up_size'])}")
    print(f"\næŠ¥å‘Šæ–‡ä»¶:")
    print(f"  - {txt_path}")
    print(f"  - {json_path}")
    print("=" * 80)


if __name__ == "__main__":
    main()