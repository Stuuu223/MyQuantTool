#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é¡½ä¸»æ¯150ç¥¨æ± æ‰¹é‡ç‰¹å¾æå–è„šæœ¬
CTOæŒ‡ä»¤ï¼šæ‰¹é‡æå–é¡½ä¸»ç¥¨å†å²èµ·çˆ†ç‰¹å¾ï¼Œä¸ºå‚æ•°ä¼˜åŒ–æä¾›ä¾æ®

ç³»ç»Ÿå“²å­¦ï¼šèµ„é‡‘ä¸ºç‹ é¡ºåŠ¿è€Œä¸º è¿½éšå¸‚åœºçŸ­çº¿å¤§å“¥
ç ”ç©¶æ¨¡å‹ï¼šAè‚¡ T+1 è§„åˆ™ä¸‹çš„å³ä¾§èµ·çˆ†æ¨¡å‹ä½“ç³»
å›æµ‹ç³»ç»Ÿï¼šTick/åˆ†K å›æ”¾ + å‚æ•°ä¼˜åŒ–

ä¿®æ”¹è®°å½•ï¼šä½¿ç”¨è€æ¿æŒ‡å®šCSVè·¯å¾„è¯»å–tickæ•°æ®ï¼Œåˆ é™¤QMTä¾èµ–
data/wanzhu_data/samples/{code}_{date}_{label}.csv
"""

import sys
import os
import pandas as pd
from pathlib import Path
from datetime import datetime
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from logic.strategies.unified_warfare_core import UnifiedWarfareCore
from logic.data_providers.dongcai_provider import DongCaiT1Provider
from logic.services.event_lifecycle_service import EventLifecycleService


def load_tick_from_csv(code, date, sample_dir="data/wanzhu_data/samples"):
    """
    ä»CSVåŠ è½½tickæ•°æ®ï¼ˆè€æ¿æŒ‡å®šè·¯å¾„ï¼‰
    è·¯å¾„æ ¼å¼ï¼šdata/wanzhu_data/samples/{code}_{date}_{label}.csv
    
    Args:
        code: è‚¡ç¥¨ä»£ç ï¼Œå¦‚ '000592'
        date: æ—¥æœŸï¼Œå¦‚ '2026-01-20'
        sample_dir: CSVæ–‡ä»¶ç›®å½•
    
    Returns:
        (ticksåˆ—è¡¨, æ•°æ®è¡Œæ•°)
    """
    # å°è¯•åŒ¹é…æ–‡ä»¶
    pattern = f"{code}_{date}_*.csv"
    sample_path = Path(PROJECT_ROOT) / sample_dir
    
    matching_files = list(sample_path.glob(pattern))
    if not matching_files:
        return None, 0
    
    # è¯»å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ–‡ä»¶
    csv_file = matching_files[0]
    
    try:
        df = pd.read_csv(csv_file)
    except Exception as e:
        print(f"   âŒ è¯»å–CSVå¤±è´¥: {csv_file}, é”™è¯¯: {e}")
        return None, 0
    
    # è½¬æ¢ä¸ºtickå­—å…¸åˆ—è¡¨
    ticks = []
    for _, row in df.iterrows():
        tick = {
            'time': row['time'],
            'lastPrice': row['price'],
            'price': row['price'],
            'true_change_pct': row.get('true_change_pct', 0),
            'volume': 0,  # CSVä¸­æ— volumeï¼Œè®¾ä¸º0
            'amount': 0,
            'flow_5min': row.get('flow_5min', 0),  # ğŸ”¥ å…³é”®ï¼šCSVè‡ªå¸¦èµ„é‡‘æµ
            'flow_15min': row.get('flow_15min', 0),
            'flow_sustainability': row.get('flow_sustainability', 1.0),
            'bidPrice': [0.0]*5,
            'askPrice': [0.0]*5,
            'bidVol': [0]*5,
            'askVol': [0]*5,
        }
        ticks.append(tick)
    
    return ticks, len(ticks)


def get_tick_from_csv(code: str, date: str) -> list:
    """
    ä»CSVæ–‡ä»¶è¯»å–Tickæ•°æ®ï¼ˆè€æ¿æŒ‡å®šè·¯å¾„ï¼‰
    è·¯å¾„: data/wanzhu_data/samples/{code}_{date}_{label}.csv
    
    Args:
        code: è‚¡ç¥¨ä»£ç ï¼Œå¦‚ '000592'
        date: æ—¥æœŸï¼Œå¦‚ '2026-01-20'
    
    Returns:
        tickåˆ—è¡¨ï¼Œæ¯ä¸ªtickæ˜¯dict
    """
    from pathlib import Path
    
    # å°è¯•å¤šä¸ªè·¯å¾„ï¼ˆsamples, samples_v2ç­‰ï¼‰
    base_paths = [
        Path(PROJECT_ROOT) / "data" / "wanzhu_data" / "samples",
        Path(PROJECT_ROOT) / "data" / "wanzhu_data" / "samples_v2",
    ]
    
    date_str = date.replace('-', '')
    
    for base_path in base_paths:
        if not base_path.exists():
            continue
        
        # æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶ï¼ˆå¿½ç•¥labelåç¼€ï¼‰
        pattern = f"{code}_{date}_*.csv"
        import glob
        files = glob.glob(str(base_path / pattern))
        
        if files:
            csv_file = files[0]  # å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ–‡ä»¶
            try:
                df = pd.read_csv(csv_file)
                print(f"âœ… è¯»å–CSV: {len(df)} è¡Œ - {Path(csv_file).name}")
                
                # è½¬æ¢ä¸ºtickå­—å…¸åˆ—è¡¨
                ticks = []
                for _, row in df.iterrows():
                    tick = {
                        'time': row['time'],
                        'lastPrice': float(row['price']),
                        'volume': float(row.get('volume', 0)),
                        'amount': float(row.get('amount', 0)),
                        'flow_5min': row.get('flow_5min', 0),  # ğŸ”¥ å…³é”®ï¼šCSVè‡ªå¸¦èµ„é‡‘æµ
                        'flow_15min': row.get('flow_15min', 0),
                        'flow_sustainability': row.get('flow_sustainability', 1.0),
                        'bidPrice': [float(row['price'])] * 5,
                        'askPrice': [float(row['price']) * 1.001] * 5,
                        'bidVol': [100] * 5,
                        'askVol': [100] * 5,
                    }
                    ticks.append(tick)
                return ticks
            except Exception as e:
                print(f"âŒ è¯»å–CSVå¤±è´¥: {e}")
                continue
    
    print(f"âš ï¸  æœªæ‰¾åˆ°CSV: {code} {date}")
    return []


def infer_flow_from_historical_tick(tick_data, base_signal, last_tick_data=None):
    """
    ä¸“é—¨ä¸ºå†å²Tickæ•°æ®è®¾è®¡çš„èµ„é‡‘æµæ¨æ–­å‡½æ•°
    """
    # æå–å†å²tickæ•°æ®å­—æ®µ
    last_price = tick_data.get('lastPrice', 0)
    volume = tick_data.get('volume', 0)
    amount = tick_data.get('amount', 0)
    bid_vol = tick_data.get('bidVol', [0]*5)
    ask_vol = tick_data.get('askVol', [0]*5)
    
    # ä»ä¸Šä¸€ä¸ªtickè®¡ç®—æˆäº¤é‡å¢é‡
    volume_delta = volume
    if last_tick_data and 'volume' in last_tick_data:
        volume_delta = volume - last_tick_data['volume']
    
    # è®¡ç®—ä¹°å–ç›˜æ€»å’Œ
    total_bid_vol = sum(bid_vol)
    total_ask_vol = sum(ask_vol)
    
    # ä½¿ç”¨ä»·æ ¼å¼ºåº¦å’ŒæŒ‚å•å‹åŠ›æ¥æ¨æ–­èµ„é‡‘æµå‘
    if last_tick_data and 'lastPrice' in last_tick_data:
        price_change = last_price - last_tick_data['lastPrice']
        price_change_ratio = price_change / last_tick_data['lastPrice'] if last_tick_data['lastPrice'] > 0 else 0
    else:
        price_change_ratio = 0
        price_change = 0
    
    # æ ¹æ®ä»·æ ¼å˜åŒ–æ–¹å‘å’Œä¹°å–ç›˜æƒ…å†µæ¨æ–­èµ„é‡‘æµå‘
    if volume_delta > 0:
        # å¦‚æœä»·æ ¼ä¸Šæ¶¨ï¼Œå€¾å‘äºè®¤ä¸ºæ˜¯ä¸»ä¹°
        if price_change > 0:
            main_flow = volume_delta * last_price * 100 * (1 + abs(price_change_ratio))
        # å¦‚æœä»·æ ¼ä¸‹è·Œï¼Œå€¾å‘äºè®¤ä¸ºæ˜¯ä¸»å–
        elif price_change < 0:
            main_flow = volume_delta * last_price * 100 * (-1 - abs(price_change_ratio))
        # å¦‚æœä»·æ ¼ä¸å˜ï¼Œæ ¹æ®ä¹°å–ç›˜å‹åŠ›åˆ¤æ–­
        else:
            if total_bid_vol > total_ask_vol:
                main_flow = volume_delta * last_price * 100 * 0.1  # è½»å¾®ä¹°å…¥å€¾å‘
            else:
                main_flow = volume_delta * last_price * 100 * -0.1  # è½»å¾®å–å‡ºå€¾å‘
    else:
        main_flow = 0  # æ— æˆäº¤é‡å˜åŒ–æ—¶ï¼Œèµ„é‡‘æµä¸º0
    
    # åŸºäºä¹°å–ç›˜æ¯”ä¾‹è®¡ç®—å‹åŠ›
    total_vol = total_bid_vol + total_ask_vol
    if total_vol > 0:
        bid_pressure = total_bid_vol / total_vol
    else:
        bid_pressure = 0.5  # æ— æŒ‚å•æ—¶ä¸­æ€§
    
    # è®¡ç®—ç½®ä¿¡åº¦
    base_confidence = base_signal.confidence if base_signal else 0.5
    volume_factor = max(0.5, min(1.0, volume_delta / 100000 if volume_delta > 0 else 0.1))
    price_factor = min(1.0, abs(price_change_ratio) * 10)
    pressure_factor = abs(bid_pressure - 0.5) * 2
    
    confidence = base_confidence * 0.4 + volume_factor * 0.2 + price_factor * 0.2 + pressure_factor * 0.2
    confidence = max(0.3, min(0.7, confidence))  # é™åˆ¶åœ¨åˆç†èŒƒå›´
    
    return {
        'main_net_inflow': main_flow,
        'super_large_net': main_flow * 0.4,
        'large_net': main_flow * 0.6,
        'confidence': confidence,
        'flow_direction': 'INFLOW' if main_flow > 0 else 'OUTFLOW'
    }


def get_available_dates_from_qmt(code):
    """ä»QMTæ•°æ®ç›®å½•è¯»å–å®é™…å¯ç”¨çš„æ—¥æœŸæ–‡ä»¶"""
    import os
    
    # å°è¯•SZå’ŒSHç›®å½•
    for exchange in ['SZ', 'SH']:
        data_dir = Path(f"E:/MyQuantTool/data/qmt_data/datadir/{exchange}/0/{code}")
        if data_dir.exists():
            # è·å–æ‰€æœ‰æ—¥æœŸæ–‡ä»¶ï¼ˆYYYYMMDDæ ¼å¼ï¼‰
            date_files = sorted([f.name for f in data_dir.iterdir() if f.is_file() and len(f.name) == 8])
            # è½¬æ¢ä¸ºYYYY-MM-DDæ ¼å¼
            dates = [f"{d[:4]}-{d[4:6]}-{d[6:8]}" for d in date_files]
            return dates
    
    return []

def get_recent_trading_days(end_date='2026-02-21', days=5):
    """
    è·å–æœ€è¿‘Nä¸ªäº¤æ˜“æ—¥åˆ—è¡¨ï¼ˆä»QMTæ•°æ®ç›®å½•è¯»å–å®é™…å¯ç”¨æ—¥æœŸï¼‰
    
    Args:
        end_date: ç»“æŸæ—¥æœŸï¼ˆæœªä½¿ç”¨ï¼Œä¿ç•™å‚æ•°å…¼å®¹æ€§ï¼‰
        days: äº¤æ˜“æ—¥æ•°é‡
    
    Returns:
        äº¤æ˜“æ—¥åˆ—è¡¨ ['2026-02-17', '2026-02-18', ...]
    """
    # ä½¿ç”¨ç½‘å®¿ç§‘æŠ€300017ä½œä¸ºå‚è€ƒï¼Œè·å–æ‰€æœ‰å¯ç”¨æ—¥æœŸ
    all_dates = get_available_dates_from_qmt('300017')
    
    if not all_dates:
        # å¤‡ç”¨ï¼šç¡¬ç¼–ç æ­£ç¡®çš„äº¤æ˜“æ—¥ï¼ˆç§»é™¤æ˜¥èŠ‚åæ—¥æœŸï¼‰
        all_dates = [
            '2026-01-20', '2026-01-21', '2026-01-22', '2026-01-23', 
            '2026-01-26', '2026-01-27', '2026-01-28',
            '2026-02-02', '2026-02-03', '2026-02-04', '2026-02-05',
            '2026-02-06', '2026-02-09', '2026-02-10', '2026-02-11',
            '2026-02-12', '2026-02-13'  # æ˜¥èŠ‚åæ— æ•°æ®
        ]
    
    # å–æœ€è¿‘dayså¤©
    if len(all_dates) >= days:
        return all_dates[-days:]
    return all_dates


def extract_wanzhu_features():
    """
    æ‰¹é‡æå–é¡½ä¸»ç¥¨ç‰¹å¾ï¼ˆå¤šæ—¥æ»šåŠ¨å›æµ‹ï¼‰
    
    å›æµ‹è®¾è®¡ï¼š
    - æ—¶é—´èŒƒå›´ï¼šæ¯ä¸ªç¥¨æœ€è¿‘30ä¸ªäº¤æ˜“æ—¥
    - æ ·æœ¬ï¼š150åªç¥¨
    - ç»Ÿè®¡ï¼šæ—¥ä¿¡å·æ•°ï¼ˆè¿‡æ»¤å‰/åï¼‰ã€è¿‡æ»¤ç‡ã€é«˜ç»´æŒå æ¯”
    
    æ•°æ®æ¥æºï¼šè€æ¿æŒ‡å®šCSVè·¯å¾„ data/wanzhu_data/samples/{code}_{date}_{label}.csv
    """
    print("="*80)
    print("é¡½ä¸»æ¯ç¥¨æ± å¤šæ—¥æ»šåŠ¨å›æµ‹ï¼ˆCSVæ•°æ®æºï¼‰")
    print("ğŸ“ æ•°æ®æº: data/wanzhu_data/samples/{code}_{date}_{label}.csv")
    print("="*80)
    
    # åŠ è½½é¡½ä¸»ç¥¨æ± 
    wanzhu_file = Path(PROJECT_ROOT) / "data" / "wanzhu_data" / "processed" / "wanzhu_selected_150.csv"
    if not wanzhu_file.exists():
        print(f"âŒ é¡½ä¸»ç¥¨æ± æ–‡ä»¶ä¸å­˜åœ¨: {wanzhu_file}")
        return
    
    df = pd.read_csv(wanzhu_file)
    print(f"ğŸ“Š åŠ è½½é¡½ä¸»ç¥¨æ± : {len(df)} åªè‚¡ç¥¨")
    
    # åˆå§‹åŒ–EventLifecycleService
    lifecycle_service = EventLifecycleService()
    print("âœ… EventLifecycleServiceè¿‡æ»¤å™¨å·²å¯ç”¨")
    print("   è¿‡æ»¤é˜ˆå€¼: sustainâ‰¥0.5, envâ‰¥0.6")
    print()
    
    # ğŸ”¥ æµ‹è¯•ç½‘å®¿çœŸèµ·çˆ†æ—¥
    trading_days = ['2026-01-26']  # ç½‘å®¿çœŸèµ·çˆ†æ—¥
    print(f"ğŸ“… æµ‹è¯•æ—¥æœŸ: {trading_days[0]} (ç½‘å®¿ç§‘æŠ€çœŸèµ·çˆ†æ—¥)")
    print()
    
    # ğŸ”¥ 4åªé«˜é¢‘å±‚æ³¡æ³¡ç¥¨ï¼ˆPhase 0.6éªŒè¯é›†ï¼‰
    bubble_stocks = [
        ('300017', 'ç½‘å®¿ç§‘æŠ€'),   # çœŸèµ·çˆ†æ—¥æµ‹è¯•
        ('000547', 'èˆªå¤©å‘å±•'),   # é«˜é¢‘å±‚
        ('300058', 'è“è‰²å…‰æ ‡'),   # é«˜é¢‘å±‚
        ('000592', 'å¹³æ½­å‘å±•'),   # é«˜é¢‘å±‚
    ]
    
    # è‡ªåŠ¨æ‰«æsamplesç›®å½•è·å–æ‰€æœ‰å¯ç”¨æ ·æœ¬
    samples_dir = Path(PROJECT_ROOT) / "data/wanzhu_data/samples"
    all_csv_files = list(samples_dir.glob("*.csv"))
    
    # æå–æ‰€æœ‰æ ·æœ¬æ—¥æœŸï¼ˆå»é‡ï¼‰
    trading_days = sorted(set([f.stem.split('_')[1] for f in all_csv_files]))
    print(f"ğŸ“… æ‰«æåˆ° {len(trading_days)} ä¸ªæ ·æœ¬æ—¥æœŸ: {', '.join(trading_days[:5])}...")
    
    # è‚¡ç¥¨ä»£ç åˆ°åç§°çš„æ˜ å°„
    stock_name_map = {
        '300017': 'ç½‘å®¿ç§‘æŠ€',
        '000547': 'èˆªå¤©å‘å±•',
        '300058': 'è“è‰²å…‰æ ‡',
        '000592': 'å¹³æ½­å‘å±•',
        '002792': 'é€šå®‡é€šè®¯',
        '301005': 'è¶…æ·è‚¡ä»½',
        '603516': 'æ·³ä¸­ç§‘æŠ€',
        '603778': 'å›½æ™Ÿç§‘æŠ€',
    }
    
    # ğŸ”¥ ä¿®å¤ï¼šä¸ºæ¯åªè‚¡ç¥¨è®¾ç½®æ­£ç¡®çš„pre_closeï¼ˆä»featuresåˆ†ææ–‡ä»¶è·å–ï¼‰
    stock_pre_close = {
        '300017': 10.0,   # ç½‘å®¿ç§‘æŠ€
        '000547': 28.9,   # èˆªå¤©å‘å±•
        '300058': 8.5,    # è“è‰²å…‰æ ‡
        '000592': 4.2,    # å¹³æ½­å‘å±•
        '002792': 15.3,   # é€šå®‡é€šè®¯
        '301005': 12.8,   # è¶…æ·è‚¡ä»½
        '603516': 22.5,   # æ·³ä¸­ç§‘æŠ€
        '603778': 18.2,   # å›½æ™Ÿç§‘æŠ€
    }
    
    # å¤šæ—¥æ»šåŠ¨ç»Ÿè®¡
    total_stock_days = 0
    total_signals_before = 0
    total_signals_after = 0
    high_sustain_days = 0
    filtered_count = 0
    passed_count = 0
    
    # å­˜å‚¨æ¯æ—¥è¯¦ç»†ç»“æœ
    daily_results = []
    all_features = []
    
    # ğŸ”¥ æ”¹ä¸ºç›´æ¥éå†æ‰€æœ‰CSVæ ·æœ¬æ–‡ä»¶
    csv_samples = []
    for csv_file in all_csv_files:
        # è§£ææ–‡ä»¶å: {code}_{date}_{label}.csv
        parts = csv_file.stem.split('_')
        if len(parts) >= 3:
            code = parts[0]
            date = parts[1]
            label = '_'.join(parts[2:])  # true æˆ– trap
            name = stock_name_map.get(code, 'æœªçŸ¥')
            csv_samples.append({
                'code': code,
                'name': name,
                'date': date,
                'label': label,
                'path': csv_file
            })
    
    print(f"ğŸ«§ æ‰«æåˆ° {len(csv_samples)} ä¸ªCSVæ ·æœ¬")
    
    # éå†æ‰€æœ‰CSVæ ·æœ¬
    for idx, sample in enumerate(csv_samples):
        code = sample['code']
        name = sample['name']
        date_str = sample['date']
        label = sample['label']
        
        print(f"\nğŸ” å¤„ç†ç¬¬ {idx+1}/{len(csv_samples)} ä¸ª: {code} - {name} ({date_str} {label})")
        
        try:
                # æ ¼å¼åŒ–è‚¡ç¥¨ä»£ç 
                formatted_code = f"{code}.SH" if code.startswith(('60', '68')) else f"{code}.SZ"
                
                # ç›´æ¥ä»CSVåŠ è½½tickæ•°æ®ï¼ˆè€æ¿æŒ‡å®šè·¯å¾„ï¼‰
                print(f"   ğŸ“Š åŠ è½½ {date_str} CSVæ•°æ®...")
                ticks, tick_count = load_tick_from_csv(code, date_str)
                
                if tick_count < 100:
                    print(f"   âš ï¸  {date_str} æ— CSVæ•°æ®æˆ–æ•°æ®å¤ªå°‘ï¼Œè·³è¿‡")
                    continue
                
                # ğŸ”¥ ä½¿ç”¨å¯¹åº”è‚¡ç¥¨çš„pre_close
                pre_close = stock_pre_close.get(code, 10.0)
                
                print(f"   âœ… CSV Tickæ•°æ®: {tick_count} è¡Œ, pre_close={pre_close:.2f} ({name})")
                
                # åˆ›å»ºç»Ÿä¸€æˆ˜æ³•æ ¸å¿ƒ
                print(f"   âš”ï¸ åˆå§‹åŒ–UnifiedWarfareCore...")
                warfare_core = UnifiedWarfareCore()
                
                # ğŸ”¥ å›çŒratioåŒ–ç­–ç•¥ï¼šå¯ç”¨æ‰€æœ‰æ£€æµ‹å™¨ï¼ˆå®ç›˜é…ç½®ï¼‰
                # warfare_core.disable_warfare('halfway_breakout')  # ä¸ç¦ç”¨
                # warfare_core.disable_warfare('opening_weak_to_strong')  # ä¸ç¦ç”¨
                print(f"   ğŸ¯ å¯ç”¨ç­–ç•¥: å…¨éƒ¨æˆ˜æ³•ï¼ˆHalfway+Leader+DipBuy+Openingï¼‰")
                print(f"   ğŸ“‹ å½“å‰æ¿€æ´»æ£€æµ‹å™¨: {warfare_core.get_active_detectors()}")
                
                # ğŸ”¥ å›çŒå®ç›˜ratioå‚æ•°ï¼ˆfile:2 ä¸‰æ¼æ–—æ ‡å‡†ï¼‰
                for detector_name in warfare_core.get_active_detectors():
                    detector = warfare_core.event_manager.detectors.get(detector_name)
                    if detector:
                        if hasattr(detector, 'breakout_strength'):
                            detector.breakout_strength = 0.01  # Level 1: ratio >1%
                        if hasattr(detector, 'volume_surge'):
                            detector.volume_surge = 1.5        # 50%æ”¾é‡
                        if hasattr(detector, 'confidence_threshold'):
                            detector.confidence_threshold = 0.3  # 30%ç½®ä¿¡åº¦
                        print(f"   âš™ï¸  {detector_name}: breakout_strength=0.01 (ratioåŒ–å›çŒ)")
                
                # åˆ›å»ºåŸºç¡€èµ„é‡‘æµæä¾›è€…
                dongcai_provider = DongCaiT1Provider()
                
                # åˆå§‹åŒ–ç´¯è®¡å˜é‡
                total_net_inflow = 0
                prev_close = pre_close  # ç”¨ä½œè®¡ç®—æ¶¨å¹…çš„åŸºå‡†
                daily_high = 0          # è®°å½•å½“æ—¥æœ€é«˜ä»·
                event_count = 0
                key_moments = []        # è®°å½•å…³é”®æ—¶åˆ»
                
                last_tick = None
                processed_count = 0     # å¤„ç†è®¡æ•°å™¨
                current_price = 0
                price_change_pct = 0
                
                # å¤„ç†tickæ•°æ®ï¼ˆç›´æ¥è¿­ä»£CSVåŠ è½½çš„ticksï¼‰
                for tick in ticks:
                    processed_count += 1
                    
                    # è·å–æ—¶é—´ï¼ˆCSVä¸­å·²æ˜¯å­—ç¬¦ä¸²æ ¼å¼ HH:MM:SSï¼‰
                    time_str = tick['time']
                    readable_time = time_str
                    
                    # è·å–åŸºç¡€èµ„é‡‘æµä¿¡å·
                    try:
                        base_signal = dongcai_provider.get_realtime_flow(code)
                    except:
                        from logic.data_providers.base import CapitalFlowSignal
                        base_signal = CapitalFlowSignal(
                            code=code,
                            main_net_inflow=0,
                            super_large_inflow=0,
                            large_inflow=0,
                            timestamp=datetime.now().timestamp(),
                            confidence=0.5,
                            source='Default'
                        )
                    
                    # ğŸ”¥ ä½¿ç”¨CSVè‡ªå¸¦çš„flow_5minä½œä¸ºèµ„é‡‘æµï¼ˆCSVæœ‰flow_5minåˆ—ï¼‰
                    flow_5min = tick.get('flow_5min', 0)
                    inferred_flow = {
                        'main_net_inflow': flow_5min,
                        'super_large_net': flow_5min * 0.4,
                        'large_net': flow_5min * 0.6,
                        'confidence': 0.8,
                    }
                    
                    # ç´¯åŠ èµ„é‡‘æµ
                    total_net_inflow += flow_5min
                    
                    # è®¡ç®—å½“æ—¥æ¶¨å¹…
                    current_price = tick['lastPrice']
                    price_change_pct = ((current_price - prev_close) / prev_close) * 100 if prev_close > 0 else 0
                    
                    # æ›´æ–°å½“æ—¥æœ€é«˜ä»·
                    if current_price > daily_high:
                        daily_high = current_price
                    
                    # ç»„è£… Context
                    context = {
                        'stock_code': formatted_code,
                        'date': date_str,
                        'pre_close': prev_close,  # ğŸ”¥ å…³é”®ï¼šä¼ å…¥æ˜¨æ”¶ä»·
                        'main_net_inflow': inferred_flow['main_net_inflow'],
                        'super_large_net_inflow': inferred_flow['super_large_net'],
                        'large_net_inflow': inferred_flow['large_net'],
                        'flow_confidence': inferred_flow['confidence'],
                        'total_net_inflow': total_net_inflow,
                        'price_change_pct': price_change_pct,
                        'daily_high': daily_high,
                    }
                    
                    # é€å…¥å®ç›˜æˆ˜æ³•æ ¸å¿ƒå¼•æ“
                    events = warfare_core.process_tick(tick, context)
                    
                    if events:
                        for event in events:
                            event_count += 1
                            print(f"   ğŸ¯ [{readable_time}] äº‹ä»¶: {event['event_type']}, "
                                  f"æ¶¨å¹…: {price_change_pct:.2f}%, å•æ—¶å‡€æµ: {inferred_flow['main_net_inflow']:.0f}, "
                                  f"ç´¯è®¡å‡€æµ: {total_net_inflow:.0f}")
                            
                            # è®°å½•äº‹ä»¶ç‰¹å¾
                            key_moments.append({
                                'time': readable_time,
                                'event_type': event['event_type'],
                                'price': current_price,
                                'price_change_pct': price_change_pct,
                                'instant_flow': inferred_flow['main_net_inflow'],
                                'total_flow': total_net_inflow,
                                'volume': tick['volume'],
                                'confidence': event['confidence'],
                                'description': event.get('description', 'N/A')
                            })
                    
                    # æ£€æŸ¥å…³é”®æ¶¨å¹…å…³å£
                    if abs(price_change_pct - 5.0) < 0.5 or abs(price_change_pct - 8.0) < 0.5 or \
                       abs(price_change_pct - 10.0) < 0.5 or abs(price_change_pct - 15.0) < 0.5:
                        # å•ç¬”èµ„é‡‘å‡€æµå…¥è¶…è¿‡3000ä¸‡
                        if abs(inferred_flow['main_net_inflow']) > 30000000:
                            print(f"   ğŸ’° [{readable_time}] å…³é”®æ¶¨å¹…{price_change_pct:.2f}% + "
                                  f"å¤§é¢èµ„é‡‘æµå…¥: {inferred_flow['main_net_inflow']:.0f}")
                            key_moments.append({
                                'time': readable_time,
                                'event_type': 'KEY_LEVEL_BULK_FLOW',
                                'price': current_price,
                                'price_change_pct': price_change_pct,
                                'instant_flow': inferred_flow['main_net_inflow'],
                                'total_flow': total_net_inflow,
                                'volume': tick['volume'],
                                'confidence': inferred_flow['confidence'],
                                'description': f'æ¶¨å¹…{price_change_pct:.2f}%å…³é”®ç‚¹èµ„é‡‘å¼‚åŠ¨'
                            })
                    
                    last_tick = tick
                
                print(f"   âœ… Tickå¤„ç†å®Œæˆ: {processed_count} ä¸ªtick, {event_count} ä¸ªäº‹ä»¶")
                
                # ç»Ÿè®¡ä¿¡å·æ•°ï¼ˆè¿‡æ»¤å‰ï¼‰
                total_signals_before += event_count
                
                # ========== EventLifecycleServiceè¿‡æ»¤å™¨ ==========
                sustain_score = 0
                env_score = 0
                is_true_breakout = False
                
                if event_count > 0:
                    print(f"   ğŸ” è¿è¡ŒEventLifecycleServiceåˆ†æ...")
                    lifecycle = lifecycle_service.analyze(code, date_str)
                    
                    sustain_score = lifecycle.get('sustain_score', 0)
                    env_score = lifecycle.get('env_score', 0)
                    is_true_breakout = lifecycle.get('is_true_breakout', False)
                    
                    print(f"   ğŸ“Š ç»´æŒåˆ†: {sustain_score:.2f}, ç¯å¢ƒåˆ†: {env_score:.2f}, é¢„æµ‹: {is_true_breakout}")
                    
                    # ç»Ÿè®¡é«˜ç»´æŒ
                    if sustain_score >= 0.5:
                        high_sustain_days += 1
                    
                    # è¿‡æ»¤å™¨æ£€æŸ¥
                    if sustain_score < 0.5 or env_score < 0.6:
                        print(f"   âš ï¸  è¿‡æ»¤å™¨ï¼šè·³è¿‡ï¼ˆç»´æŒåˆ†={sustain_score:.2f}<0.5 æˆ– ç¯å¢ƒåˆ†={env_score:.2f}<0.6ï¼‰")
                        filtered_count += 1
                    else:
                        print(f"   âœ… è¿‡æ»¤å™¨é€šè¿‡")
                        passed_count += 1
                        total_signals_after += event_count
                else:
                    # æ— ä¿¡å·ï¼Œè§†ä¸ºè¢«è¿‡æ»¤
                    filtered_count += 1
                # ========== è¿‡æ»¤å™¨ç»“æŸ ==========
                
                # è®°å½•æ¯æ—¥ç»“æœ
                daily_results.append({
                    'code': code,
                    'name': name,
                    'date': date_str,
                    'signals_before': event_count,
                    'signals_after': event_count if (sustain_score >= 0.5 and env_score >= 0.6) else 0,
                    'sustain_score': sustain_score,
                    'env_score': env_score,
                    'is_true_breakout': is_true_breakout,
                    'tick_count': tick_count
                })
                
                # è®°å½•è¿™åªè‚¡ç¥¨çš„ç‰¹å¾
                stock_features = {
                    'code': code,
                    'name': name,
                    'date': date_str,
                    'total_ticks': tick_count,
                    'total_events': event_count,
                    'total_net_inflow': total_net_inflow,
                    'final_price': current_price,
                    'final_change_pct': price_change_pct,
                    'sustain_score': sustain_score,
                    'env_score': env_score,
                    'is_true_breakout': is_true_breakout,
                    'key_moments': key_moments
                }
                
                all_features.append(stock_features)
                
                if event_count > 0:
                    print(f"   ğŸ“Š å…³é”®ç‰¹å¾: ç´¯è®¡å‡€æµå…¥ {total_net_inflow:.0f}, æœ€ç»ˆæ¶¨å¹… {price_change_pct:.2f}%")
                
        except Exception as e:
            print(f"   âŒ å¤„ç† {date_str} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            # ğŸ”¥ å³ä½¿å¤±è´¥ä¹Ÿè¦è®°å½•åˆ°æŠ¥å‘Šï¼ˆæ ‡è®°ä¸ºå¤±è´¥ï¼‰
            failed_features = {
                'code': code,
                'name': name,
                'date': date_str,
                'total_ticks': 0,
                'total_events': 0,
                'total_net_inflow': 0,
                'final_price': 0,
                'final_change_pct': 0,
                'sustain_score': 0,
                'env_score': 0,
                'is_true_breakout': False,
                'key_moments': [],
                'error': str(e)
            }
            all_features.append(failed_features)
            continue
    
    # ä¿å­˜ç‰¹å¾ç»“æœ
    if all_features:
        output_file = Path(PROJECT_ROOT) / "data" / "wanzhu_data" / "wanzhu_features_analysis_csv_v2.json"
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_features, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\n" + "="*80)
        print("å¤šæ—¥æ»šåŠ¨å›æµ‹å®Œæˆ")
        print("="*80)
        print(f"ğŸ“Š æ€»ç¥¨æ—¥æ•°: {total_stock_days} (3ç¥¨ Ã— {len(trading_days)}å¤©)")
        print(f"ğŸ“Š è¿‡æ»¤å‰ä¿¡å·: {total_signals_before} ä¸ª")
        print(f"ğŸ“Š è¿‡æ»¤åä¿¡å·: {total_signals_after} ä¸ª")
        if total_signals_before > 0:
            print(f"ğŸ“Š è¿‡æ»¤ç‡: {(1-total_signals_after/total_signals_before)*100:.1f}%")
        if total_stock_days > 0:
            print(f"ğŸ“Š é«˜ç»´æŒå æ¯”: {high_sustain_days/total_stock_days*100:.1f}%")
        print(f"ğŸ“ ç»“æœä¿å­˜: {output_file}")
        
        # å¤šæ—¥æ»šåŠ¨ç»Ÿè®¡æŠ¥å‘Š
        print(f"\nã€å¤šæ—¥æ»šåŠ¨ç»Ÿè®¡ã€‘")
        print(f"ğŸ“… å›æµ‹æ—¶é—´èŒƒå›´: {trading_days[0]} è‡³ {trading_days[-1]}")
        if len(trading_days) > 0:
            print(f"ğŸ“ˆ æ—¥å‡ä¿¡å·æ•°(è¿‡æ»¤å‰): {total_signals_before/len(trading_days):.1f} ä¸ª/å¤©")
            print(f"ğŸ“ˆ æ—¥å‡ä¿¡å·æ•°(è¿‡æ»¤å): {total_signals_after/len(trading_days):.1f} ä¸ª/å¤©")
        
        # æŒ‰æ—¥æœŸåˆ†ç»„ç»Ÿè®¡
        df_results = pd.DataFrame(daily_results)
        if not df_results.empty:
            print(f"\nã€æŒ‰æ—¥æœŸç»Ÿè®¡ã€‘")
            for date in trading_days:
                day_data = df_results[df_results['date'] == date]
                if not day_data.empty:
                    signals_before = day_data['signals_before'].sum()
                    signals_after = day_data['signals_after'].sum()
                    print(f"  {date}: è¿‡æ»¤å‰{signals_before}ä¸ª, è¿‡æ»¤å{signals_after}ä¸ª")
        
        # åˆ†å±‚ç»Ÿè®¡
        sustain_high = [s for s in all_features if s.get('sustain_score', 0) >= 0.5]
        env_high = [s for s in all_features if s.get('env_score', 0) >= 0.6]
        both_high = [s for s in all_features if s.get('sustain_score', 0) >= 0.5 and s.get('env_score', 0) >= 0.6]
        
        print(f"\nã€åˆ†å±‚ç»Ÿè®¡ã€‘")
        if len(all_features) > 0:
            print(f"ğŸ“Š sustainâ‰¥0.5: {len(sustain_high)} ç¥¨æ—¥ ({len(sustain_high)/len(all_features)*100:.1f}%)")
            print(f"ğŸ“Š envâ‰¥0.6: {len(env_high)} ç¥¨æ—¥ ({len(env_high)/len(all_features)*100:.1f}%)")
            print(f"ğŸ“Š åŒé«˜(çœŸèµ·çˆ†): {len(both_high)} ç¥¨æ—¥ ({len(both_high)/len(all_features)*100:.1f}%)")
        
        # ä¿å­˜CSVæŠ¥å‘Š
        csv_file = Path(PROJECT_ROOT) / "data" / "wanzhu_data" / "wanzhu_rolling_backtest_report.csv"
        df_report = pd.DataFrame(all_features)
        df_report.to_csv(csv_file, index=False, encoding='utf-8-sig')
        print(f"\nğŸ“ CSVæŠ¥å‘Š: {csv_file}")
    else:
        print(f"\nâš ï¸ æ— æœ‰æ•ˆæ•°æ®")
    
    print("="*80)


def main():
    extract_wanzhu_features()


if __name__ == "__main__":
    main()