#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç«ä»·å¿«ç…§é‡‡é›†è„šæœ¬ (Phase3 ç¬¬1å‘¨) - CTOå®¡è®¡ä¼˜åŒ–ç‰ˆ

åŠŸèƒ½ï¼š
1. æ¯ä¸ªäº¤æ˜“æ—¥09:25é‡‡é›†å…¨å¸‚åœºç«ä»·å¿«ç…§ï¼ˆæ‰¹é‡APIï¼‰
2. æ‰‹åŠ¨è®¡ç®—æ¶¨è·Œå¹…å’Œé‡æ¯”ï¼ˆQMT APIä¸è¿”å›è¿™äº›å­—æ®µï¼‰
3. ä¿å­˜ç«ä»·æ•°æ®åˆ°SQLiteå’ŒRedis
4. æœ¬åœ°ç¼“å­˜å›é€€æœºåˆ¶ï¼ˆQMTå¤±è´¥æ—¶ä½¿ç”¨æœ¬åœ°æ•°æ®ï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
    # é‡‡é›†ä»Šæ—¥ç«ä»·å¿«ç…§
    python tasks/collect_auction_snapshot.py
    
    # é‡‡é›†æŒ‡å®šæ—¥æœŸç«ä»·å¿«ç…§ï¼ˆç”¨äºè¡¥æ•°æ®ï¼‰
    python tasks/collect_auction_snapshot.py --date 2026-02-10
    
    # æ‰¹é‡é‡‡é›†å†å²ç«ä»·å¿«ç…§
    python tasks/collect_auction_snapshot.py --start-date 2026-02-01 --end-date 2026-02-10

æ•°æ®ä¿å­˜ï¼š
- SQLite: data/auction_snapshots.db
- Redis: auction:YYYYMMDD:CODE (24å°æ—¶è¿‡æœŸ)

æ€§èƒ½ï¼š
- æ‰¹é‡é‡‡é›†ï¼š500åª/æ‰¹
- é¢„æœŸé€Ÿåº¦ï¼š5190åªè‚¡ç¥¨ < 30ç§’

CTOå®¡è®¡ä¼˜åŒ–ï¼š
- æ·»åŠ å®Œæ•´ç±»å‹æ ‡æ³¨
- æœ¬åœ°ç¼“å­˜å›é€€æœºåˆ¶
- æ•°æ®è´¨é‡ç»Ÿè®¡å¢å¼º
"""

import sys
import os
import json
import time
import argparse
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from logic.utils.logger import get_logger
from logic.database_manager import DatabaseManager
from logic.auction_snapshot_manager import AuctionSnapshotManager

logger = get_logger(__name__)


class AuctionSnapshotCollector:
    """
    ç«ä»·å¿«ç…§é‡‡é›†å™¨
    
    è´Ÿè´£é‡‡é›†å…¨å¸‚åœºç«ä»·æ•°æ®å¹¶ä¿å­˜åˆ°æ•°æ®åº“
    æ”¯æŒæœ¬åœ°ç¼“å­˜å›é€€æœºåˆ¶
    """
    
    def __init__(self, db_path: Optional[str] = None) -> None:
        """
        åˆå§‹åŒ–é‡‡é›†å™¨
        
        Args:
            db_path: SQLiteæ•°æ®åº“è·¯å¾„ï¼ˆé»˜è®¤ï¼šdata/auction_snapshots.dbï¼‰
        """
        # æ•°æ®åº“è·¯å¾„
        if db_path is None:
            db_path = project_root / "data" / "auction_snapshots.db"
        else:
            db_path = Path(db_path)
        
        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        db_path.parent.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
        self.db_manager = DatabaseManager()
        self.db_path = str(db_path)
        
        # åˆå§‹åŒ–ç«ä»·å¿«ç…§ç®¡ç†å™¨
        self.snapshot_manager = AuctionSnapshotManager(self.db_manager)
        
        # æœ¬åœ°ç¼“å­˜ç›®å½•
        self.cache_dir = project_root / "data" / "minute_data"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ•°æ®åº“è¡¨
        self._init_database()
        
        logger.info(f"âœ… ç«ä»·å¿«ç…§é‡‡é›†å™¨åˆå§‹åŒ–æˆåŠŸ")
        logger.info(f"ğŸ“ æ•°æ®åº“è·¯å¾„: {self.db_path}")
        logger.info(f"ğŸ“ ç¼“å­˜ç›®å½•: {self.cache_dir}")
        logger.info(f"ğŸ’¾ RedisçŠ¶æ€: {'å¯ç”¨' if self.snapshot_manager.is_available else 'ä¸å¯ç”¨'}")
    
    def _init_database(self) -> None:
        """
        åˆå§‹åŒ–SQLiteæ•°æ®åº“è¡¨
        """
        try:
            import sqlite3
            
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # åˆ›å»ºç«ä»·å¿«ç…§è¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS auction_snapshots (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    date TEXT NOT NULL,
                    code TEXT NOT NULL,
                    name TEXT,
                    auction_time TEXT,
                    auction_price REAL,
                    auction_volume INTEGER,
                    auction_amount REAL,
                    auction_change REAL,
                    volume_ratio REAL,
                    buy_orders INTEGER,
                    sell_orders INTEGER,
                    bid_vol_1 INTEGER,
                    ask_vol_1 INTEGER,
                    market_type TEXT,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(date, code)
                )
            """)
            
            # åˆ›å»ºç´¢å¼•
            cursor.execute("""
                CREATE INDEX IF NOT EXISTS idx_date 
                ON auction_snapshots(date)
            """)
            
            cursor.execute("""
                CREATE INDEX IF NOT EXISTS idx_code 
                ON auction_snapshots(code)
            """)
            
            cursor.execute("""
                CREATE INDEX IF NOT EXISTS idx_date_code 
                ON auction_snapshots(date, code)
            """)
            
            conn.commit()
            conn.close()
            
            logger.info("âœ… æ•°æ®åº“è¡¨åˆå§‹åŒ–æˆåŠŸ")
        
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def get_all_stock_codes(self) -> List[str]:
        """
        è·å–å…¨å¸‚åœºè‚¡ç¥¨ä»£ç åˆ—è¡¨
        
        Returns:
            è‚¡ç¥¨ä»£ç åˆ—è¡¨
        """
        try:
            # å°è¯•ä»QMTè·å–è‚¡ç¥¨åˆ—è¡¨
            try:
                import xtquant.xtdata as xtdata
                
                # è·å–æ‰€æœ‰Aè‚¡ä»£ç 
                sh_stocks = xtdata.get_stock_list_in_sector("æ²ªæ·±Aè‚¡")
                logger.info(f"âœ… ä»QMTè·å–åˆ° {len(sh_stocks)} åªè‚¡ç¥¨")
                return sh_stocks
            
            except Exception as e:
                logger.warning(f"âš ï¸ QMTè·å–å¤±è´¥: {e}ï¼Œå°è¯•ä½¿ç”¨AkShare")
                
                # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨AkShare
                import akshare as ak
                
                stock_list = ak.stock_info_a_code_name()
                codes = stock_list['code'].tolist()
                
                # è½¬æ¢ä¸ºQMTæ ¼å¼ï¼ˆ6ä½æ•°å­—+å¸‚åœºåç¼€ï¼‰
                formatted_codes = []
                for code in codes:
                    if code.startswith('6'):
                        formatted_codes.append(f"{code}.SH")
                    elif code.startswith(('0', '3')):
                        formatted_codes.append(f"{code}.SZ")
                
                logger.info(f"âœ… ä»AkShareè·å–åˆ° {len(formatted_codes)} åªè‚¡ç¥¨")
                return formatted_codes
        
        except Exception as e:
            logger.error(f"âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def _read_from_local_cache(self, code: str) -> Optional[float]:
        """
        ä»æœ¬åœ°ç¼“å­˜è¯»å–å†å²å¹³å‡æˆäº¤é‡
        
        Args:
            code: è‚¡ç¥¨ä»£ç ï¼ˆå¸¦å¸‚åœºåç¼€ï¼Œå¦‚ï¼š600000.SHï¼‰
        
        Returns:
            å¹³å‡æ¯åˆ†é’Ÿæˆäº¤é‡ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            import pandas as pd
            
            # æœ¬åœ°ç¼“å­˜æ–‡ä»¶è·¯å¾„
            cache_file = self.cache_dir / f"{code}_1m.csv"
            
            if not cache_file.exists():
                return None
            
            # è¯»å–ç¼“å­˜æ–‡ä»¶
            df = pd.read_csv(cache_file)
            
            # æ£€æŸ¥å¿…éœ€åˆ—
            if 'volume' not in df.columns:
                return None
            
            # å–æœ€è¿‘5å¤©æ•°æ®
            recent_df = df.tail(5 * 240)  # 5å¤© * 240åˆ†é’Ÿ/å¤©
            
            if len(recent_df) == 0:
                return None
            
            # è®¡ç®—å¹³å‡æˆäº¤é‡
            valid_volumes = recent_df['volume'][recent_df['volume'] > 0]
            
            if len(valid_volumes) == 0:
                return None
            
            avg_volume_per_minute = valid_volumes.mean()
            
            logger.debug(f"âœ… ä»æœ¬åœ°ç¼“å­˜è¯»å– {code} å†å²æ•°æ®")
            return float(avg_volume_per_minute)
        
        except Exception as e:
            logger.debug(f"âš ï¸ ä»æœ¬åœ°ç¼“å­˜è¯»å– {code} å¤±è´¥: {e}")
            return None
    
    def get_historical_avg_volume(self, codes: List[str], date: str) -> Dict[str, Optional[float]]:
        """
        è·å–å†å²5æ—¥å¹³å‡æˆäº¤é‡ï¼ˆç”¨äºè®¡ç®—é‡æ¯”ï¼‰

        Args:
            codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            date: å½“å‰æ—¥æœŸ

        Returns:
            {code: avg_volume_per_minute}  # Noneè¡¨ç¤ºæ— æ•ˆæ•°æ®

        æ•°æ®è´¨é‡ä¿è¯ï¼š
            - è¿”å›Noneæ˜ç¡®æ ‡è®°æ— æ•ˆæ•°æ®
            - è¶…è¿‡10%æ•°æ®å¤±è´¥æ—¶å‘Šè­¦
            - åŒºåˆ†æ¨¡æ‹Ÿç¯å¢ƒå’Œç”Ÿäº§ç¯å¢ƒ
        """
        try:
            import xtquant.xtdata as xtdata
            import pandas as pd

            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨ get_market_data_exï¼ˆé¡¹ç›®æˆåŠŸæ¡ˆä¾‹ä½¿ç”¨çš„APIï¼‰
            # get_market_data_ex è¿”å›ï¼š{code: DataFrame, ...}
            # DataFrameåˆ—åæ˜¯å­—æ®µåï¼Œç´¢å¼•æ˜¯æ—¶é—´
            hist_data = xtdata.get_market_data_ex(
                field_list=['volume'],
                stock_list=codes,
                period='1d',
                count=5,
                dividend_type='none',
                fill_data=True
            )

            result = {}
            invalid_count = 0

            for code in codes:
                try:
                    # get_market_data_ex è¿”å›ç»“æ„ï¼š{code: DataFrame, ...}
                    if code in hist_data:
                        code_data = hist_data[code]

                        # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
                        if isinstance(code_data, pd.DataFrame):
                            if 'volume' in code_data.columns and len(code_data) > 0:
                                volumes = code_data['volume'].tolist()
                                valid_vols = [v for v in volumes if v is not None and v > 0]

                                if len(valid_vols) >= 1:  # è‡³å°‘1å¤©æœ‰æ•ˆæ•°æ®
                                    avg_volume_per_day = sum(valid_vols) / len(valid_vols)
                                    avg_volume_per_minute = avg_volume_per_day / 240.0
                                    result[code] = avg_volume_per_minute
                                else:
                                    result[code] = None
                                    invalid_count += 1
                            else:
                                result[code] = None
                                invalid_count += 1
                        else:
                            result[code] = None
                            invalid_count += 1
                    else:
                        result[code] = None
                        invalid_count += 1
                except Exception as e:
                    logger.warning(f"âš ï¸ å¤„ç†{code}å†å²æ•°æ®å¤±è´¥: {e}")
                    result[code] = None
                    invalid_count += 1

            # æ•°æ®è´¨é‡å‘Šè­¦
            if len(codes) > 0:
                invalid_rate = invalid_count / len(codes)
                if invalid_rate > 0.5:
                    logger.error(f"âŒ å†å²æ•°æ®è·å–å¤±è´¥ç‡{invalid_rate*100:.1f}% ({invalid_count}/{len(codes)})ï¼Œå¯èƒ½ä¸ºQMTæ¨¡æ‹Ÿç¯å¢ƒ")
                elif invalid_rate > 0.1:
                    logger.warning(f"âš ï¸ å†å²æ•°æ®è·å–å¤±è´¥ç‡{invalid_rate*100:.1f}% ({invalid_count}/{len(codes)})")
                else:
                    logger.debug(f"âœ… å†å²æ•°æ®è·å–æˆåŠŸï¼Œå¤±è´¥ç‡{invalid_rate*100:.1f}%")

            return result

        except Exception as e:
            logger.error(f"âŒ è·å–å†å²æˆäº¤é‡å¼‚å¸¸: {e}", exc_info=True)
            return {code: None for code in codes}
    
    def get_historical_avg_volume_with_fallback(self, codes: List[str], date: str) -> Tuple[Dict[str, Optional[float]], int]:
        """
        è·å–å†å²å¹³å‡æˆäº¤é‡ï¼ˆå¸¦æœ¬åœ°ç¼“å­˜å›é€€ï¼‰
        
        Args:
            codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            date: å½“å‰æ—¥æœŸ
        
        Returns:
            (ç»“æœå­—å…¸, æœ¬åœ°ç¼“å­˜å‘½ä¸­æ•°)
        """
        # å…ˆä»QMTè·å–
        result = self.get_historical_avg_volume(codes, date)
        
        # ç»Ÿè®¡å¤±è´¥æ•°é‡
        failed_codes = [code for code, vol in result.items() if vol is None]
        
        if not failed_codes:
            return result, 0
        
        # å°è¯•ä»æœ¬åœ°ç¼“å­˜å›é€€
        cache_hit_count = 0
        
        for code in failed_codes:
            cache_vol = self._read_from_local_cache(code)
            if cache_vol is not None:
                result[code] = cache_vol
                cache_hit_count += 1
        
        if cache_hit_count > 0:
            logger.info(f"ğŸ“¦ æœ¬åœ°ç¼“å­˜å›é€€æˆåŠŸï¼š{cache_hit_count}/{len(failed_codes)} åªè‚¡ç¥¨")
        
        return result, cache_hit_count
    
    def save_snapshots_batch(self, snapshots: List[Dict[str, Any]]) -> int:
        """
        æ‰¹é‡ä¿å­˜å¿«ç…§åˆ°SQLiteï¼ˆä½¿ç”¨äº‹åŠ¡æå‡æ€§èƒ½ï¼‰

        Args:
            snapshots: å¿«ç…§åˆ—è¡¨

        Returns:
            æˆåŠŸä¿å­˜çš„æ•°é‡
        """
        if not snapshots:
            return 0

        try:
            import sqlite3

            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # ç¡®ä¿è¡¨åŒ…å«æ•°æ®è´¨é‡å­—æ®µï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
            try:
                cursor.execute("ALTER TABLE auction_snapshots ADD COLUMN volume_ratio_valid INTEGER DEFAULT 0")
                cursor.execute("ALTER TABLE auction_snapshots ADD COLUMN data_source TEXT DEFAULT 'unknown'")
                conn.commit()
            except sqlite3.OperationalError:
                # å­—æ®µå·²å­˜åœ¨ï¼Œå¿½ç•¥
                pass

            # æ‰¹é‡æ’å…¥ï¼ˆä½¿ç”¨executemanyï¼‰
            cursor.executemany("""
                INSERT OR REPLACE INTO auction_snapshots (
                    date, code, name, auction_time, auction_price, auction_volume,
                    auction_amount, auction_change, volume_ratio, buy_orders,
                    sell_orders, bid_vol_1, ask_vol_1, market_type,
                    volume_ratio_valid, data_source
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, [
                (s['date'], s['code'], s['name'], s['auction_time'],
                 s['auction_price'], s['auction_volume'], s['auction_amount'],
                 s['auction_change'], s['volume_ratio'], s['buy_orders'],
                 s['sell_orders'], s['bid_vol_1'], s['ask_vol_1'], s['market_type'],
                 s.get('volume_ratio_valid', 0), s.get('data_source', 'unknown'))
                for s in snapshots
            ])

            conn.commit()
            conn.close()

            return len(snapshots)

        except sqlite3.DatabaseError as e:
            logger.error(f"âŒ æ•°æ®åº“é”™è¯¯: {e}")
            return 0
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡ä¿å­˜å¤±è´¥: {e}", exc_info=True)
            return 0
    
    def collect_all_snapshots_batch(self, date: Optional[str] = None, batch_size: int = 500) -> Dict[str, Any]:
        """
        æ‰¹é‡é‡‡é›†å…¨å¸‚åœºç«ä»·å¿«ç…§ï¼ˆä½¿ç”¨QMTæ‰¹é‡API + æ‰‹åŠ¨è®¡ç®—ï¼‰
        
        Args:
            date: æ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼Œé»˜è®¤ä¸ºä»Šå¤©ï¼‰
            batch_size: æ¯æ‰¹æ¬¡å¤„ç†çš„è‚¡ç¥¨æ•°é‡ï¼ˆé»˜è®¤500ï¼‰
        
        Returns:
            é‡‡é›†ç»Ÿè®¡ä¿¡æ¯
        """
        if date is None:
            date = datetime.now().strftime("%Y-%m-%d")
        
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡é‡‡é›† {date} çš„ç«ä»·å¿«ç…§")
        
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        stock_codes = self.get_all_stock_codes()
        total = len(stock_codes)
        
        if total == 0:
            logger.error("âŒ æœªè·å–åˆ°è‚¡ç¥¨åˆ—è¡¨")
            return {'total': 0, 'success': 0, 'failed': 0}
        
        total_batches = (total + batch_size - 1) // batch_size
        logger.info(f"ğŸ“Š å…±éœ€é‡‡é›† {total} åªè‚¡ç¥¨ï¼Œåˆ† {total_batches} æ‰¹æ¬¡ï¼Œæ¯æ‰¹ {batch_size} åª")
        
        # æ‰¹é‡é‡‡é›†
        import xtquant.xtdata as xtdata
        
        success_count = 0
        failed_count = 0
        processed = 0
        cache_hit_total = 0
        
        start_time = time.time()
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, total, batch_size):
            batch_codes = stock_codes[i:i + batch_size]
            batch_num = i // batch_size + 1
            
            batch_start = time.time()
            logger.info(f"ğŸ”„ å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹æ¬¡ï¼ˆ{len(batch_codes)} åªè‚¡ç¥¨ï¼‰")

            try:
                # ğŸ”¥ å…³é”®1ï¼šæ‰¹é‡è·å–tickæ•°æ®
                tick_data = xtdata.get_full_tick(batch_codes)

                if not tick_data:
                    logger.warning(f"âš ï¸ ç¬¬ {batch_num} æ‰¹æ¬¡æœªè·å–åˆ°æ•°æ®")
                    failed_count += len(batch_codes)
                    continue

                # ğŸ”¥ å…³é”®2ï¼šæ‰¹é‡è·å–å†å²æˆäº¤é‡ï¼ˆå¸¦æœ¬åœ°ç¼“å­˜å›é€€ï¼‰
                avg_volumes, cache_hits = self.get_historical_avg_volume_with_fallback(batch_codes, date)
                cache_hit_total += cache_hits

                # ğŸ”¥ ä¸‰çº§éªŒè¯ï¼šæ£€æŸ¥å†å²æ•°æ®è´¨é‡
                if avg_volumes:
                    valid_avg_count = sum(1 for v in avg_volumes.values() if v is not None and v > 0)
                    total_avg_count = len(avg_volumes)
                    invalid_rate = 1 - (valid_avg_count / total_avg_count) if total_avg_count > 0 else 1

                    # å‘Šè­¦é˜ˆå€¼ï¼šè¶…è¿‡50%æ•°æ®æ— æ•ˆï¼Œæ¨¡æ‹Ÿç¯å¢ƒ
                    if invalid_rate > 0.5:
                        logger.warning(f"âš ï¸ æ‰¹æ¬¡{batch_num}å†å²æ•°æ®æ— æ•ˆç‡{invalid_rate*100:.1f}%ï¼Œé‡æ¯”è®¡ç®—å¯èƒ½ä¸å‡†ç¡®")

                    # âœ… P1ä¿®å¤ï¼šæ‹¦æˆªé˜ˆå€¼ï¼šè¶…è¿‡90%æ•°æ®æ— æ•ˆï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡
                    if invalid_rate > 0.9:
                        logger.error(f"âŒ æ‰¹æ¬¡{batch_num}å†å²æ•°æ®ä¸¥é‡ç¼ºå¤±ï¼ˆ{invalid_rate*100:.1f}%ï¼‰ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡å¤„ç†")
                        failed_count += len(batch_codes)
                        continue  # è·³è¿‡å½“å‰æ‰¹æ¬¡ï¼Œä¸å¤„ç†æ— æ•ˆæ•°æ®

                # å‡†å¤‡æ‰¹é‡ä¿å­˜çš„æ•°æ®
                batch_snapshots = []
                
                # å¤„ç†æ¯åªè‚¡ç¥¨çš„æ•°æ®
                for code in batch_codes:
                    processed += 1
                    
                    if code not in tick_data:
                        failed_count += 1
                        continue
                    
                    try:
                        data = tick_data[code]
                        
                        # ğŸ”¥ å…³é”®3ï¼šæ‰‹åŠ¨è®¡ç®—æ¶¨è·Œå¹…
                        last_price = data.get('lastPrice', 0)
                        last_close = data.get('lastClose', 0)
                        
                        if last_close > 0:
                            auction_change = (last_price - last_close) / last_close
                        else:
                            auction_change = 0.0
                        
                        # ğŸ”¥ å…³é”®4ï¼šæ‰‹åŠ¨è®¡ç®—é‡æ¯”ï¼ˆå¸¦è´¨é‡æ ‡è®°ï¼‰
                        auction_volume = data.get('volume', 0)
                        avg_volume_per_minute = avg_volumes.get(code)

                        # åˆ¤æ–­å†å²æ•°æ®æ˜¯å¦æœ‰æ•ˆ
                        volume_ratio_valid = avg_volume_per_minute is not None and avg_volume_per_minute > 0

                        if volume_ratio_valid:
                            volume_ratio = auction_volume / avg_volume_per_minute
                            # åˆç†æ€§éªŒè¯ï¼šé‡æ¯”åº”åœ¨0.01-1000èŒƒå›´å†…
                            if volume_ratio < 0.01 or volume_ratio > 1000:
                                logger.warning(f"âš ï¸ {code}é‡æ¯”å¼‚å¸¸({volume_ratio:.2f})ï¼Œè¯·äººå·¥å®¡æ ¸")
                        else:
                            volume_ratio = None  # æ ‡è®°ä¸ºæ— æ•ˆ
                            volume_ratio_valid = False

                        # ç¯å¢ƒæ ‡è®°
                        is_simulated = (avg_volume_per_minute is None)

                        # æå–ç«ä»·æ•°æ®ï¼ˆå«è´¨é‡æ ‡è®°ï¼‰
                        auction_data = {
                            'date': date,
                            'code': code,
                            'name': data.get('stockName', ''),
                            'auction_time': f"{date} 09:25:00",
                            'auction_price': last_price,
                            'auction_volume': auction_volume,
                            'auction_amount': data.get('amount', 0),
                            'auction_change': auction_change,           # âœ… æ‰‹åŠ¨è®¡ç®—
                            'volume_ratio': volume_ratio or 0.0,       # âœ… æ‰‹åŠ¨è®¡ç®—ï¼ˆæ— æ•ˆä¸º0ï¼‰
                            'buy_orders': 0,                            # âš ï¸ QMTä¸æä¾›
                            'sell_orders': 0,                           # âš ï¸ QMTä¸æä¾›
                            'bid_vol_1': data.get('bidVol', [0])[0] if data.get('bidVol') else 0,
                            'ask_vol_1': data.get('askVol', [0])[0] if data.get('askVol') else 0,
                            'market_type': 'SH' if code.endswith('.SH') else 'SZ',
                            # æ–°å¢ï¼šæ•°æ®è´¨é‡å­—æ®µ
                            'volume_ratio_valid': int(volume_ratio_valid),  # 1=æœ‰æ•ˆ, 0=æ— æ•ˆ
                            'data_source': 'simulated' if is_simulated else 'production',  # æ•°æ®æ¥æº
                        }
                        
                        batch_snapshots.append(auction_data)
                        
                        # åŒæ—¶ä¿å­˜åˆ°Redisï¼ˆç”¨äºå¿«é€ŸæŸ¥è¯¢ï¼‰
                        if self.snapshot_manager.is_available:
                            self.snapshot_manager.save_auction_snapshot(
                                code.split('.')[0],
                                auction_data
                            )
                    
                    except Exception as e:
                        logger.warning(f"âš ï¸ å¤„ç† {code} å¤±è´¥: {e}")
                        failed_count += 1
                
                # æ‰¹é‡ä¿å­˜åˆ°SQLite
                saved_count = self.save_snapshots_batch(batch_snapshots)
                success_count += saved_count
                failed_count += len(batch_codes) - saved_count
                
                # æ‰¹æ¬¡ç»Ÿè®¡
                batch_time = time.time() - batch_start
                elapsed = time.time() - start_time
                avg_time_per_stock = elapsed / processed if processed > 0 else 0
                eta_seconds = avg_time_per_stock * (total - processed)
                
                logger.info(
                    f"ğŸ“ˆ è¿›åº¦: {processed}/{total} ({processed/total*100:.1f}%) | "
                    f"æˆåŠŸ: {success_count} | å¤±è´¥: {failed_count} | "
                    f"æ‰¹æ¬¡è€—æ—¶: {batch_time:.1f}s | é¢„è®¡å‰©ä½™: {eta_seconds:.0f}s"
                )
                
                # æ‰¹æ¬¡é—´çŸ­æš‚å»¶è¿Ÿï¼ˆé¿å…è¯·æ±‚è¿‡å¿«ï¼‰
                time.sleep(0.05)
            
            except Exception as e:
                logger.error(f"âŒ ç¬¬ {batch_num} æ‰¹æ¬¡å¤±è´¥: {e}")
                failed_count += len(batch_codes)
        
        total_time = time.time() - start_time
        logger.info(
            f"âœ… æ‰¹é‡é‡‡é›†å®Œæˆ - æ€»æ•°: {total}, æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}, "
            f"æ€»è€—æ—¶: {total_time:.1f}s, å¹³å‡: {total_time/total*1000:.1f}ms/è‚¡"
        )
        
        if cache_hit_total > 0:
            logger.info(f"ğŸ“¦ æœ¬åœ°ç¼“å­˜å›é€€ç»Ÿè®¡ï¼š{cache_hit_total} åªè‚¡ç¥¨ä½¿ç”¨æœ¬åœ°æ•°æ®")
        
        return {
            'total': total,
            'success': success_count,
            'failed': failed_count,
            'time_seconds': total_time,
            'cache_hits': cache_hit_total
        }
    
    def get_snapshot_stats(self, date: Optional[str] = None) -> Dict[str, Any]:
        """
        è·å–ç«ä»·å¿«ç…§ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¢å¼ºç‰ˆï¼šåŒ…å«æ•°æ®è´¨é‡ç»Ÿè®¡ï¼‰
        
        Args:
            date: æ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼Œé»˜è®¤ä¸ºä»Šå¤©ï¼‰
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            import sqlite3
            
            if date is None:
                date = datetime.now().strftime("%Y-%m-%d")
            
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # æŸ¥è¯¢ç»Ÿè®¡ä¿¡æ¯
            cursor.execute("""
                SELECT 
                    COUNT(*) as total,
                    COUNT(CASE WHEN auction_change > 0.03 THEN 1 END) as high_open_count,
                    COUNT(CASE WHEN auction_change < -0.03 THEN 1 END) as low_open_count,
                    COUNT(CASE WHEN volume_ratio > 2.0 AND volume_ratio_valid = 1 THEN 1 END) as high_volume_count,
                    AVG(auction_change) as avg_change,
                    AVG(volume_ratio) as avg_volume_ratio,
                    COUNT(CASE WHEN volume_ratio_valid = 1 THEN 1 END) as valid_data_count,
                    COUNT(CASE WHEN data_source = 'simulated' THEN 1 END) as simulated_count,
                    COUNT(CASE WHEN data_source = 'production' THEN 1 END) as production_count
                FROM auction_snapshots
                WHERE date = ?
            """, (date,))
            
            row = cursor.fetchone()
            conn.close()
            
            if row and row[0] > 0:
                total = row[0]
                valid_data_count = row[6] or 0
                valid_data_rate = (valid_data_count / total * 100) if total > 0 else 0
                
                return {
                    'date': date,
                    'total': total,
                    'high_open_count': row[1],
                    'low_open_count': row[2],
                    'high_volume_count': row[3],
                    'avg_change': row[4],
                    'avg_volume_ratio': row[5],
                    'valid_data_count': valid_data_count,
                    'valid_data_rate': valid_data_rate,
                    'simulated_count': row[7] or 0,
                    'production_count': row[8] or 0
                }
            else:
                return {'date': date, 'total': 0}
        
        except Exception as e:
            logger.error(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {'date': date, 'error': str(e)}


def main() -> None:
    """
    ä¸»å‡½æ•°
    """
    parser = argparse.ArgumentParser(description='ç«ä»·å¿«ç…§é‡‡é›†è„šæœ¬ï¼ˆCTOå®¡è®¡ä¼˜åŒ–ç‰ˆï¼‰')
    parser.add_argument('--date', type=str, help='é‡‡é›†æ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰')
    parser.add_argument('--start-date', type=str, help='å¼€å§‹æ—¥æœŸï¼ˆç”¨äºæ‰¹é‡é‡‡é›†ï¼‰')
    parser.add_argument('--end-date', type=str, help='ç»“æŸæ—¥æœŸï¼ˆç”¨äºæ‰¹é‡é‡‡é›†ï¼‰')
    parser.add_argument('--batch-size', type=int, default=500, help='æ¯æ‰¹æ¬¡è‚¡ç¥¨æ•°é‡ï¼ˆé»˜è®¤500ï¼‰')
    parser.add_argument('--stats', action='store_true', help='æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–é‡‡é›†å™¨
    collector = AuctionSnapshotCollector()
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    if args.stats:
        stats = collector.get_snapshot_stats(args.date)
        logger.info(f"\nğŸ“Š ç«ä»·å¿«ç…§ç»Ÿè®¡ä¿¡æ¯ï¼š")
        logger.info(f"æ—¥æœŸ: {stats.get('date')}")
        logger.info(f"æ€»æ•°: {stats.get('total')}")
        logger.info(f"é«˜å¼€è‚¡ç¥¨: {stats.get('high_open_count')} (æ¶¨å¹…>3%)")
        logger.info(f"ä½å¼€è‚¡ç¥¨: {stats.get('low_open_count')} (è·Œå¹…>3%)")
        logger.info(f"æ”¾é‡è‚¡ç¥¨: {stats.get('high_volume_count')} (é‡æ¯”>2.0ä¸”æœ‰æ•ˆ)")
        logger.info(f"å¹³å‡æ¶¨å¹…: {stats.get('avg_change', 0)*100:.2f}%")
        logger.info(f"å¹³å‡é‡æ¯”: {stats.get('avg_volume_ratio', 0):.2f}")
        logger.info(f"æ•°æ®æœ‰æ•ˆç‡: {stats.get('valid_data_rate', 0):.1f}%")
        logger.info(f"æ•°æ®æ¥æº: ç”Ÿäº§ç¯å¢ƒ {stats.get('production_count', 0)} | æ¨¡æ‹Ÿç¯å¢ƒ {stats.get('simulated_count', 0)}")
        return
    
    # æ‰¹é‡é‡‡é›†
    if args.start_date and args.end_date:
        start = datetime.strptime(args.start_date, "%Y-%m-%d")
        end = datetime.strptime(args.end_date, "%Y-%m-%d")
        
        current = start
        while current <= end:
            date_str = current.strftime("%Y-%m-%d")
            
            # è·³è¿‡å‘¨æœ«
            if current.weekday() < 5:  # 0-4 æ˜¯å‘¨ä¸€åˆ°å‘¨äº”
                logger.info(f"\n{'='*60}")
                logger.info(f"é‡‡é›†æ—¥æœŸ: {date_str}")
                logger.info(f"{'='*60}")
                
                result = collector.collect_all_snapshots_batch(date_str, args.batch_size)
                
                logger.info(
                    f"\nç»“æœ: æ€»æ•°={result['total']}, æˆåŠŸ={result['success']}, "
                    f"å¤±è´¥={result['failed']}, è€—æ—¶={result.get('time_seconds', 0):.1f}s, "
                    f"ç¼“å­˜å‘½ä¸­={result.get('cache_hits', 0)}"
                )
            else:
                logger.info(f"â­ï¸  è·³è¿‡å‘¨æœ«: {date_str}")
            
            current += timedelta(days=1)
            time.sleep(1)  # é¿å…é¢‘ç¹è¯·æ±‚
    
    # å•æ—¥é‡‡é›†
    else:
        date = args.date or datetime.now().strftime("%Y-%m-%d")
        
        logger.info(f"\n{'='*60}")
        logger.info(f"é‡‡é›†æ—¥æœŸ: {date}")
        logger.info(f"{'='*60}\n")
        
        result = collector.collect_all_snapshots_batch(date, args.batch_size)
        
        logger.info(f"\n{'='*60}")
        logger.info(f"âœ… é‡‡é›†å®Œæˆ")
        logger.info(f"æ€»æ•°: {result['total']}")
        logger.info(f"æˆåŠŸ: {result['success']}")
        logger.info(f"å¤±è´¥: {result['failed']}")
        logger.info(f"æ€»è€—æ—¶: {result.get('time_seconds', 0):.1f}s")
        logger.info(f"ç¼“å­˜å‘½ä¸­: {result.get('cache_hits', 0)}")
        logger.info(f"{'='*60}\n")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = collector.get_snapshot_stats(date)
        logger.info(f"ğŸ“Š ç«ä»·å¿«ç…§ç»Ÿè®¡ä¿¡æ¯ï¼š")
        logger.info(f"é«˜å¼€è‚¡ç¥¨: {stats.get('high_open_count')} (æ¶¨å¹…>3%)")
        logger.info(f"ä½å¼€è‚¡ç¥¨: {stats.get('low_open_count')} (è·Œå¹…>3%)")
        logger.info(f"æ”¾é‡è‚¡ç¥¨: {stats.get('high_volume_count')} (é‡æ¯”>2.0ä¸”æœ‰æ•ˆ)")
        logger.info(f"æ•°æ®æœ‰æ•ˆç‡: {stats.get('valid_data_rate', 0):.1f}%")


if __name__ == "__main__":
    main()
