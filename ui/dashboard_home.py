"""
ä»ªè¡¨ç›˜é¦–é¡µ - å±•ç¤ºç³»ç»Ÿå¥åº·çŠ¶æ€å’Œæ•°æ®è®°å½•æƒ…å†µ
"""

import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime, timedelta
from pathlib import Path

from logic.database_manager import DatabaseManager
from logic.logger import get_logger
from logic.auction_snapshot_manager import AuctionSnapshotManager

logger = get_logger(__name__)


def render_dashboard_home():
    """æ¸²æŸ“ä»ªè¡¨ç›˜é¦–é¡µ"""

    st.set_page_config(
        page_title="é‡åŒ–å·¥å…· - ä»ªè¡¨ç›˜",
        page_icon="ğŸ“Š",
        layout="wide"
    )

    st.title("ğŸ“Š ç³»ç»Ÿä»ªè¡¨ç›˜")
    st.caption("å®æ—¶ç›‘æ§ç³»ç»Ÿå¥åº·çŠ¶æ€å’Œæ•°æ®è®°å½•æƒ…å†µ")
    st.markdown("---")

    # æ•°æ®åº“å’ŒRedisçŠ¶æ€æ£€æŸ¥
    if 'dashboard_db' not in st.session_state:
        # åŠ è½½æ•°æ®åº“é…ç½®
        import json
        config_path = Path(__file__).parent.parent / 'config_database.json'
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                db_config = json.load(f)
        else:
            db_config = {}
        st.session_state.dashboard_db = DatabaseManager(db_config)

    db = st.session_state.dashboard_db

    # è‡ªåŠ¨åˆ·æ–°
    col_refresh1, col_refresh2 = st.columns([1, 1])
    with col_refresh1:
        auto_refresh = st.checkbox("è‡ªåŠ¨åˆ·æ–°ï¼ˆæ¯30ç§’ï¼‰", value=False, key="dashboard_auto_refresh")
    with col_refresh2:
        if st.button("ğŸ”„ ç«‹å³åˆ·æ–°", key="dashboard_refresh"):
            st.rerun()

    if auto_refresh:
        import time
        time.sleep(30)
        st.rerun()

    st.markdown("---")

    # ç¬¬ä¸€è¡Œï¼šæœåŠ¡çŠ¶æ€æ¦‚è§ˆ
    st.subheader("ğŸ”Œ æœåŠ¡çŠ¶æ€æ¦‚è§ˆ")
    col1, col2, col3, col4 = st.columns(4)

    # RedisçŠ¶æ€
    with col1:
        redis_status = check_redis_status(db)
        if redis_status['status'] == 'online':
            st.success(f"ğŸŸ¢ Redis: {redis_status['status']}")
            st.metric("è¿æ¥æ•°", redis_stats.get('connected_clients', 0))
            st.metric("é”®æ•°é‡", redis_stats.get('key_count', 0))
        else:
            st.error(f"ğŸ”´ Redis: {redis_status['status']}")
            st.warning(f"é”™è¯¯: {redis_status.get('error', 'æœªçŸ¥é”™è¯¯')}")

    # SQLiteçŠ¶æ€
    with col2:
        sqlite_status = check_sqlite_status(db)
        if sqlite_status['status'] == 'online':
            st.success(f"ğŸŸ¢ SQLite: {sqlite_status['status']}")
            st.metric("è¡¨æ•°é‡", sqlite_status.get('table_count', 0))
            st.metric("æ€»è®°å½•æ•°", sqlite_status.get('total_records', 0))
        else:
            st.error(f"ğŸ”´ SQLite: {sqlite_status['status']}")
            st.warning(f"é”™è¯¯: {sqlite_status.get('error', 'æœªçŸ¥é”™è¯¯')}")

    # æ•°æ®å¥åº·åº¦
    with col3:
        health_score = calculate_data_health(db)
        health_color = "ğŸŸ¢" if health_score >= 80 else "ğŸŸ¡" if health_score >= 60 else "ğŸ”´"
        st.metric(f"{health_color} æ•°æ®å¥åº·åº¦", f"{health_score}/100")

    # ç³»ç»Ÿè¿è¡Œæ—¶é—´
    with col4:
        uptime = get_system_uptime()
        st.metric("â±ï¸ ç³»ç»Ÿè¿è¡Œæ—¶é—´", uptime)

    st.markdown("---")

    # ç¬¬äºŒè¡Œï¼šç«ä»·å¿«ç…§ç®¡ç†
    st.subheader("âš¡ ç«ä»·å¿«ç…§ç®¡ç†")

    auction_status = check_auction_snapshot_status(db)

    col_auction1, col_auction2, col_auction3, col_auction4 = st.columns(4)

    with col_auction1:
        if auction_status['is_available']:
            st.success(f"ğŸŸ¢ ç«ä»·å¿«ç…§: å¯ç”¨")
        else:
            st.error(f"ğŸ”´ ç«ä»·å¿«ç…§: ä¸å¯ç”¨")
            if 'error' in auction_status:
                st.warning(f"é”™è¯¯: {auction_status['error']}")

    with col_auction2:
        if auction_status['is_auction_time']:
            st.info(f"ğŸ• å½“å‰æ—¶é—´: ç«ä»·æ—¶æ®µ")
        elif auction_status['is_after_market_open']:
            st.success(f"ğŸ• å½“å‰æ—¶é—´: å¼€ç›˜å")
        else:
            st.info(f"ğŸ• å½“å‰æ—¶é—´: éäº¤æ˜“æ—¶æ®µ")

    with col_auction3:
        if auction_status['is_available']:
            st.metric("ä»Šæ—¥å¿«ç…§æ•°", auction_status.get('snapshot_count', 0))
        else:
            st.metric("ä»Šæ—¥å¿«ç…§æ•°", "N/A")

    with col_auction4:
        st.metric("æ—¥æœŸ", auction_status.get('today', 'N/A'))

    # æ˜¾ç¤ºæœ€è¿‘å¿«ç…§ç¤ºä¾‹
    if auction_status.get('recent_snapshots'):
        st.markdown("### ğŸ“‹ æœ€è¿‘ç«ä»·å¿«ç…§ï¼ˆå‰5æ¡ï¼‰")
        snapshot_df = pd.DataFrame(auction_status['recent_snapshots'])
        snapshot_df['ç«ä»·é‡(æ‰‹)'] = snapshot_df['auction_volume'].apply(lambda x: f"{x:,.0f}")
        snapshot_df['ç«ä»·é‡‘é¢(å…ƒ)'] = snapshot_df['auction_amount'].apply(lambda x: f"{x:,.0f}")
        snapshot_df['å¿«ç…§æ—¶é—´'] = pd.to_datetime(snapshot_df['snapshot_time'], unit='s').dt.strftime('%H:%M:%S')
        snapshot_df = snapshot_df[['stock_code', 'ç«ä»·é‡(æ‰‹)', 'ç«ä»·é‡‘é¢(å…ƒ)', 'å¿«ç…§æ—¶é—´']]
        snapshot_df.columns = ['è‚¡ç¥¨ä»£ç ', 'ç«ä»·é‡(æ‰‹)', 'ç«ä»·é‡‘é¢(å…ƒ)', 'å¿«ç…§æ—¶é—´']
        st.dataframe(snapshot_df, use_container_width=True)
    elif auction_status['is_available']:
        st.info("ğŸ“­ ä»Šæ—¥æš‚æ— ç«ä»·å¿«ç…§æ•°æ®")
    else:
        st.warning("âš ï¸ ç«ä»·å¿«ç…§åŠŸèƒ½ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ Redis è¿æ¥")

    st.markdown("---")

    # ç¬¬ä¸‰è¡Œï¼šæ•°æ®è®°å½•æƒ…å†µ
    st.subheader("ğŸ“ˆ æ•°æ®è®°å½•æƒ…å†µ")

    col_left, col_right = st.columns([2, 1])

    with col_left:
        # æ•°æ®è®°å½•è¶‹åŠ¿å›¾
        st.markdown("### ğŸ“Š æ•°æ®è®°å½•è¶‹åŠ¿ï¼ˆè¿‘7å¤©ï¼‰")
        trend_data = get_data_record_trend(db)
        if trend_data:
            fig = px.line(
                trend_data,
                x='date',
                y='record_count',
                title='æ¯æ—¥æ•°æ®è®°å½•æ•°é‡',
                markers=True
            )
            fig.update_layout(
                xaxis_title='æ—¥æœŸ',
                yaxis_title='è®°å½•æ•°é‡',
                hovermode='x unified'
            )
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.warning("æš‚æ— æ•°æ®è®°å½•è¶‹åŠ¿")

    with col_right:
        # æ•°æ®è®°å½•åˆ†å¸ƒ
        st.markdown("### ğŸ“Š æ•°æ®è®°å½•åˆ†å¸ƒ")
        distribution = get_data_distribution(db)
        if distribution:
            fig = px.pie(
                distribution,
                values='count',
                names='table',
                title='å„è¡¨æ•°æ®åˆ†å¸ƒ'
            )
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.warning("æš‚æ— æ•°æ®åˆ†å¸ƒä¿¡æ¯")

    st.markdown("---")

    # ç¬¬å››è¡Œï¼šæ•°æ®é—®é¢˜è¯Šæ–­
    st.subheader("ğŸ” æ•°æ®é—®é¢˜è¯Šæ–­")

    col_diag1, col_diag2 = st.columns([1, 1])

    with col_diag1:
        st.markdown("### âš ï¸ æ•°æ®ç¼ºå¤±å¤©æ•°")
        missing_days = find_missing_data_days(db)
        if missing_days:
            st.warning(f"å‘ç° {len(missing_days)} å¤©æ•°æ®ç¼ºå¤±")
            missing_df = pd.DataFrame({
                'æ—¥æœŸ': missing_days,
                'çŠ¶æ€': ['ç¼ºå¤±'] * len(missing_days)
            })
            st.dataframe(missing_df, use_container_width=True)
        else:
            st.success("âœ… è¿‘7å¤©æ•°æ®å®Œæ•´ï¼Œæ— ç¼ºå¤±")

    with col_diag2:
        st.markdown("### ğŸ“ æ•°æ®è´¨é‡æŠ¥å‘Š")
        quality_report = generate_quality_report(db)
        if quality_report:
            for item in quality_report:
                if item['status'] == 'ok':
                    st.success(f"âœ… {item['name']}: {item['message']}")
                elif item['status'] == 'warning':
                    st.warning(f"âš ï¸ {item['name']}: {item['message']}")
                else:
                    st.error(f"âŒ {item['name']}: {item['message']}")
        else:
            st.info("æš‚æ— è´¨é‡æŠ¥å‘Š")

    st.markdown("---")

    # ç¬¬å››è¡Œï¼šRedisæ•°æ®è¯¦æƒ…
    st.subheader("ğŸ’¾ Redisæ•°æ®è¯¦æƒ…")

    col_redis1, col_redis2 = st.columns([1, 1])

    with col_redis1:
        st.markdown("### ğŸ“Š Redisé”®åˆ†å¸ƒ")
        redis_keys_info = get_redis_keys_info(db)
        if redis_keys_info:
            keys_df = pd.DataFrame(redis_keys_info)
            st.dataframe(keys_df, use_container_width=True)
        else:
            st.info("Redisä¸­æš‚æ— æ•°æ®")

    with col_redis2:
        st.markdown("### ğŸ“ˆ Redisé”®è¿‡æœŸæƒ…å†µ")
        expiration_info = get_redis_expiration_info(db)
        if expiration_info:
            exp_df = pd.DataFrame(expiration_info)
            st.dataframe(exp_df, use_container_width=True)
        else:
            st.info("æš‚æ— è¿‡æœŸä¿¡æ¯")

    st.markdown("---")

    # ç¬¬äº”è¡Œï¼šæœåŠ¡æ€§èƒ½ç›‘æ§
    st.subheader("âš¡ æœåŠ¡æ€§èƒ½ç›‘æ§")

    col_perf1, col_perf2, col_perf3 = st.columns(3)

    with col_perf1:
        st.metric("ğŸš€ Rediså“åº”æ—¶é—´", f"{redis_status.get('response_time', 0):.2f}ms")

    with col_perf2:
        st.metric("ğŸš€ SQLiteå“åº”æ—¶é—´", f"{sqlite_status.get('response_time', 0):.2f}ms")

    with col_perf3:
        st.metric("ğŸ’¾ ç£ç›˜ä½¿ç”¨", f"{get_disk_usage():.1f}%")

    st.markdown("---")

    # ç¬¬å…­è¡Œï¼šæ—¥å¿—æ‘˜è¦
    st.subheader("ğŸ“œ æ—¥å¿—æ‘˜è¦")

    col_log1, col_log2 = st.columns([1, 1])

    with col_log1:
        st.markdown("### ğŸ”´ é”™è¯¯æ—¥å¿—ï¼ˆæœ€è¿‘10æ¡ï¼‰")
        error_logs = get_recent_logs(level='ERROR', limit=10)
        if error_logs:
            for log in error_logs:
                st.error(f"{log['time']}: {log['message']}")
        else:
            st.success("âœ… æœ€è¿‘æ— é”™è¯¯æ—¥å¿—")

    with col_log2:
        st.markdown("### âš ï¸ è­¦å‘Šæ—¥å¿—ï¼ˆæœ€è¿‘10æ¡ï¼‰")
        warning_logs = get_recent_logs(level='WARNING', limit=10)
        if warning_logs:
            for log in warning_logs:
                st.warning(f"{log['time']}: {log['message']}")
        else:
            st.success("âœ… æœ€è¿‘æ— è­¦å‘Šæ—¥å¿—")


def check_redis_status(db):
    """æ£€æŸ¥RedisçŠ¶æ€ï¼ˆæ‡’åŠ è½½æ¨¡å¼ï¼‰"""
    import time
    start_time = time.time()

    try:
        # å°è¯•åˆå§‹åŒ–Redisè¿æ¥ï¼ˆå¦‚æœå°šæœªåˆå§‹åŒ–ï¼‰
        if not db._redis_initialized:
            db._init_redis()
            db._redis_initialized = True

        # æ£€æŸ¥Rediså®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        if db._redis_client:
            db._redis_client.ping()
            response_time = (time.time() - start_time) * 1000

            # è·å–Redisç»Ÿè®¡ä¿¡æ¯
            global redis_stats
            redis_stats = {
                'connected_clients': db._redis_client.client_list().__len__() if hasattr(db._redis_client, 'client_list') else 1,
                'key_count': db._redis_client.dbsize(),
                'used_memory': db._redis_client.info().get('used_memory_human', '0B'),
                'uptime': db._redis_client.info().get('uptime_in_days', 0)
            }

            return {
                'status': 'online',
                'response_time': response_time,
                'stats': redis_stats
            }
        else:
            return {
                'status': 'offline',
                'error': 'RedisæœåŠ¡æœªè¿è¡Œæˆ–è¿æ¥å¤±è´¥'
            }
    except Exception as e:
        return {
            'status': 'offline',
            'error': str(e)
        }


def check_sqlite_status(db):
    """æ£€æŸ¥SQLiteçŠ¶æ€"""
    import time
    start_time = time.time()

    try:
        cursor = db.conn.cursor()

        # è·å–è¡¨æ•°é‡
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
        tables = cursor.fetchall()

        # è·å–æ€»è®°å½•æ•°
        total_records = 0
        for table in tables:
            cursor.execute(f"SELECT COUNT(*) FROM {table[0]}")
            total_records += cursor.fetchone()[0]

        response_time = (time.time() - start_time) * 1000

        return {
            'status': 'online',
            'response_time': response_time,
            'table_count': len(tables),
            'total_records': total_records
        }
    except Exception as e:
        return {
            'status': 'offline',
            'error': str(e)
        }


def calculate_data_health(db):
    """è®¡ç®—æ•°æ®å¥åº·åº¦"""
    try:
        # åŸºç¡€åˆ†ï¼š100åˆ†
        health_score = 100

        # æ£€æŸ¥Redis
        if not check_redis_status(db)['status'] == 'online':
            health_score -= 20

        # æ£€æŸ¥SQLite
        if not check_sqlite_status(db)['status'] == 'online':
            health_score -= 30

        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        missing_days = find_missing_data_days(db)
        if missing_days:
            health_score -= len(missing_days) * 5

        # æ£€æŸ¥æœ€è¿‘é”™è¯¯æ—¥å¿—
        error_logs = get_recent_logs(level='ERROR', limit=1)
        if error_logs:
            health_score -= 10

        return max(0, health_score)
    except Exception as e:
        logger.error(f"è®¡ç®—æ•°æ®å¥åº·åº¦å¤±è´¥: {e}")
        return 0


def get_system_uptime():
    """è·å–ç³»ç»Ÿè¿è¡Œæ—¶é—´"""
    try:
        # ä¼˜å…ˆä½¿ç”¨psutilï¼ˆè·¨å¹³å°ï¼‰
        try:
            import psutil
            boot_time = datetime.fromtimestamp(psutil.boot_time())
            uptime = datetime.now() - boot_time

            days = uptime.days
            hours, remainder = divmod(uptime.seconds, 3600)
            minutes, _ = divmod(remainder, 60)

            return f"{days}å¤© {hours}å°æ—¶ {minutes}åˆ†é’Ÿ"
        except ImportError:
            pass

        # Windowsç³»ç»Ÿï¼šä½¿ç”¨systeminfoå‘½ä»¤
        try:
            import subprocess
            import re
            result = subprocess.run(['systeminfo'], capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                # æŸ¥æ‰¾ç³»ç»Ÿå¯åŠ¨æ—¶é—´
                match = re.search(r'ç³»ç»Ÿå¯åŠ¨æ—¶é—´:\s*(.+)', result.stdout)
                if match:
                    boot_time_str = match.group(1).strip()
                    # å°è¯•è§£æå¯åŠ¨æ—¶é—´
                    try:
                        # Windowsä¸­æ–‡ç³»ç»Ÿæ ¼å¼ï¼š2026/1/21, 9:00:00
                        boot_time = datetime.strptime(boot_time_str, '%Y/%m/%d, %H:%M:%S')
                    except:
                        # å°è¯•å…¶ä»–æ ¼å¼
                        boot_time = datetime.strptime(boot_time_str.split(',')[0], '%Y/%m/%d')

                    uptime = datetime.now() - boot_time
                    days = uptime.days
                    hours, remainder = divmod(uptime.seconds, 3600)
                    minutes, _ = divmod(remainder, 60)

                    return f"{days}å¤© {hours}å°æ—¶ {minutes}åˆ†é’Ÿ"
        except:
            pass

        # å¦‚æœéƒ½å¤±è´¥ï¼Œè¿”å›åº”ç”¨è¿è¡Œæ—¶é—´
        from time import time
        if 'app_start_time' not in st.session_state:
            st.session_state.app_start_time = time()

        app_uptime = time() - st.session_state.app_start_time
        hours, remainder = divmod(int(app_uptime), 3600)
        minutes, _ = divmod(remainder, 60)

        return f"{hours}å°æ—¶ {minutes}åˆ†é’Ÿï¼ˆåº”ç”¨è¿è¡Œæ—¶é—´ï¼‰"

    except:
        return "æœªçŸ¥"


def check_auction_snapshot_status(db):
    """æ£€æŸ¥ç«ä»·å¿«ç…§çŠ¶æ€"""
    try:
        # åˆå§‹åŒ–ç«ä»·å¿«ç…§ç®¡ç†å™¨
        auction_manager = AuctionSnapshotManager(db)
        status = auction_manager.get_snapshot_status()

        # å¦‚æœ Redis å¯ç”¨ï¼Œè·å–ä»Šæ—¥ç«ä»·å¿«ç…§æ•°é‡
        if status['is_available']:
            try:
                # æ‰«æ Redis ä¸­çš„ç«ä»·å¿«ç…§é”®
                today = status['today']
                pattern = f"auction:{today}:*"

                # ä½¿ç”¨ Redis çš„ SCAN å‘½ä»¤ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if db._redis_client:
                    # å°è¯•è·å–æ‰€æœ‰åŒ¹é…çš„é”®
                    keys = db._redis_client.keys(pattern)
                    status['snapshot_count'] = len(keys)

                    # è·å–å‰5ä¸ªå¿«ç…§ç¤ºä¾‹
                    status['recent_snapshots'] = []
                    for key in keys[:5]:
                        stock_code = key.split(':')[-1]
                        raw_data = db._redis_client.get(key)
                        if raw_data:
                            import json
                            data = json.loads(raw_data)
                            status['recent_snapshots'].append({
                                'stock_code': stock_code,
                                'auction_volume': data.get('auction_volume', 0),
                                'auction_amount': data.get('auction_amount', 0),
                                'snapshot_time': data.get('snapshot_time', 0)
                            })
            except Exception as e:
                logger.error(f"è·å–ç«ä»·å¿«ç…§ç»Ÿè®¡å¤±è´¥: {e}")
                status['snapshot_count'] = 0
                status['recent_snapshots'] = []

        return status
    except Exception as e:
        logger.error(f"æ£€æŸ¥ç«ä»·å¿«ç…§çŠ¶æ€å¤±è´¥: {e}")
        return {
            'is_available': False,
            'is_auction_time': False,
            'is_after_market_open': False,
            'today': datetime.now().strftime("%Y%m%d"),
            'redis_connected': False,
            'snapshot_count': 0,
            'recent_snapshots': [],
            'error': str(e)
        }


def get_data_record_trend(db):
    """è·å–æ•°æ®è®°å½•è¶‹åŠ¿ï¼ˆè¿‘7å¤©ï¼‰"""
    try:
        cursor = db.conn.cursor()

        # æ£€æŸ¥æ˜¯å¦æœ‰daily_barsè¡¨
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='daily_bars'")
        if not cursor.fetchone():
            return []

        # è·å–è¿‘7å¤©çš„æ•°æ®è®°å½•æ•°é‡
        cursor.execute("""
            SELECT date, COUNT(*) as record_count
            FROM daily_bars
            WHERE date >= date('now', '-7 days')
            GROUP BY date
            ORDER BY date
        """)

        results = cursor.fetchall()
        if not results:
            return []

        return pd.DataFrame(results, columns=['date', 'record_count']).to_dict('records')
    except Exception as e:
        logger.error(f"è·å–æ•°æ®è®°å½•è¶‹åŠ¿å¤±è´¥: {e}")
        return []


def get_data_distribution(db):
    """è·å–æ•°æ®åˆ†å¸ƒ"""
    try:
        cursor = db.conn.cursor()

        cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
        tables = cursor.fetchall()

        distribution = []
        for table in tables:
            table_name = table[0]
            cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
            count = cursor.fetchone()[0]
            distribution.append({
                'table': table_name,
                'count': count
            })

        return distribution
    except Exception as e:
        logger.error(f"è·å–æ•°æ®åˆ†å¸ƒå¤±è´¥: {e}")
        return []


def find_missing_data_days(db):
    """æŸ¥æ‰¾ç¼ºå¤±æ•°æ®çš„å¤©æ•°ï¼ˆè¿‘7å¤©ï¼‰"""
    try:
        cursor = db.conn.cursor()

        # æ£€æŸ¥æ˜¯å¦æœ‰daily_barsè¡¨
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='daily_bars'")
        if not cursor.fetchone():
            return []

        # è·å–è¿‘7å¤©çš„æ—¥æœŸ
        cursor.execute("""
            SELECT DISTINCT date
            FROM daily_bars
            WHERE date >= date('now', '-7 days')
            ORDER BY date
        """)

        existing_days = [row[0] for row in cursor.fetchall()]

        # ç”Ÿæˆè¿‘7å¤©çš„æ—¥æœŸåˆ—è¡¨
        missing_days = []
        for i in range(7):
            date = (datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d')
            if date not in existing_days:
                missing_days.append(date)

        return missing_days
    except Exception as e:
        logger.error(f"æŸ¥æ‰¾ç¼ºå¤±æ•°æ®å¤©æ•°å¤±è´¥: {e}")
        return []


def generate_quality_report(db):
    """ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š"""
    report = []

    try:
        # æ£€æŸ¥Redis
        redis_status = check_redis_status(db)
        if redis_status['status'] == 'online':
            report.append({
                'name': 'Redisè¿æ¥',
                'status': 'ok',
                'message': f'è¿æ¥æ­£å¸¸ï¼Œå“åº”æ—¶é—´ {redis_status["response_time"]:.2f}ms'
            })
        else:
            report.append({
                'name': 'Redisè¿æ¥',
                'status': 'error',
                'message': f'è¿æ¥å¤±è´¥: {redis_status["error"]}'
            })

        # æ£€æŸ¥SQLite
        sqlite_status = check_sqlite_status(db)
        if sqlite_status['status'] == 'online':
            report.append({
                'name': 'SQLiteè¿æ¥',
                'status': 'ok',
                'message': f'è¿æ¥æ­£å¸¸ï¼Œ{sqlite_status["table_count"]}ä¸ªè¡¨ï¼Œ{sqlite_status["total_records"]}æ¡è®°å½•'
            })
        else:
            report.append({
                'name': 'SQLiteè¿æ¥',
                'status': 'error',
                'message': f'è¿æ¥å¤±è´¥: {sqlite_status["error"]}'
            })

        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        missing_days = find_missing_data_days(db)
        if missing_days:
            report.append({
                'name': 'æ•°æ®å®Œæ•´æ€§',
                'status': 'warning',
                'message': f'è¿‘7å¤©ç¼ºå¤± {len(missing_days)} å¤©æ•°æ®'
            })
        else:
            report.append({
                'name': 'æ•°æ®å®Œæ•´æ€§',
                'status': 'ok',
                'message': 'è¿‘7å¤©æ•°æ®å®Œæ•´'
            })

        return report
    except Exception as e:
        logger.error(f"ç”Ÿæˆè´¨é‡æŠ¥å‘Šå¤±è´¥: {e}")
        return []


def get_redis_keys_info(db):
    """è·å–Redisé”®ä¿¡æ¯"""
    try:
        if not db._redis_client:
            return []

        keys = db._redis_client.keys('*')
        if not keys:
            return []

        keys_info = []
        for key in keys[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ªé”®
            key_type = db._redis_client.type(key)
            ttl = db._redis_client.ttl(key)
            keys_info.append({
                'é”®å': key,
                'ç±»å‹': key_type,
                'TTL': ttl if ttl > 0 else 'æ°¸ä¸è¿‡æœŸ'
            })

        return keys_info
    except Exception as e:
        logger.error(f"è·å–Redisé”®ä¿¡æ¯å¤±è´¥: {e}")
        return []


def get_redis_expiration_info(db):
    """è·å–Redisè¿‡æœŸä¿¡æ¯"""
    try:
        if not db._redis_client:
            return []

        keys = db._redis_client.keys('*')
        if not keys:
            return []

        expiration_info = []
        for key in keys:
            ttl = db._redis_client.ttl(key)
            if ttl > 0:
                expiration_info.append({
                    'é”®å': key,
                    'å‰©ä½™æ—¶é—´(ç§’)': ttl,
                    'è¿‡æœŸæ—¶é—´': datetime.now() + timedelta(seconds=ttl)
                })

        return expiration_info
    except Exception as e:
        logger.error(f"è·å–Redisè¿‡æœŸä¿¡æ¯å¤±è´¥: {e}")
        return []


def get_disk_usage():
    """è·å–ç£ç›˜ä½¿ç”¨ç‡"""
    try:
        import psutil
        disk = psutil.disk_usage('/')
        return disk.percent
    except:
        return 0


def get_recent_logs(level='ERROR', limit=10):
    """è·å–æœ€è¿‘çš„æ—¥å¿—"""
    try:
        log_dir = Path('logs')
        if not log_dir.exists():
            return []

        # æŸ¥æ‰¾æœ€æ–°çš„æ—¥å¿—æ–‡ä»¶
        log_files = sorted(log_dir.glob('*.log'), key=lambda x: x.stat().st_mtime, reverse=True)
        if not log_files:
            return []

        latest_log = log_files[0]

        # è¯»å–æ—¥å¿—æ–‡ä»¶
        logs = []
        with open(latest_log, 'r', encoding='utf-8') as f:
            for line in f:
                if f' - {level} - ' in line:
                    try:
                        # è§£ææ—¥å¿—è¡Œ
                        parts = line.split(' - ')
                        if len(parts) >= 3:
                            time_str = parts[0]
                            message = ' - '.join(parts[2:])
                            logs.append({
                                'time': time_str,
                                'message': message.strip()
                            })
                            if len(logs) >= limit:
                                break
                    except:
                        continue

        return logs
    except Exception as e:
        logger.error(f"è·å–æ—¥å¿—å¤±è´¥: {e}")
        return []


# åˆå§‹åŒ–å…¨å±€å˜é‡
redis_stats = {}