"""
ä»ªè¡¨ç›˜é¦–é¡µ - å±•ç¤ºç³»ç»Ÿå¥åº·çŠ¶æ€å’Œæ•°æ®è®°å½•æƒ…å†µ
"""

import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime, timedelta
from pathlib import Path

from logic.database_manager import DatabaseManager
from logic.logger import get_logger

logger = get_logger(__name__)


def render_dashboard_home():
    """æ¸²æŸ“ä»ªè¡¨ç›˜é¦–é¡µ"""

    st.set_page_config(
        page_title="é‡åŒ–å·¥å…· - ä»ªè¡¨ç›˜",
        page_icon="ğŸ“Š",
        layout="wide"
    )

    st.title("ğŸ“Š ç³»ç»Ÿä»ªè¡¨ç›˜")
    st.caption("å®æ—¶ç›‘æ§ç³»ç»Ÿå¥åº·çŠ¶æ€å’Œæ•°æ®è®°å½•æƒ…å†µ")
    st.markdown("---")

    # æ•°æ®åº“å’ŒRedisçŠ¶æ€æ£€æŸ¥
    if 'dashboard_db' not in st.session_state:
        st.session_state.dashboard_db = DatabaseManager()

    db = st.session_state.dashboard_db

    # è‡ªåŠ¨åˆ·æ–°
    col_refresh1, col_refresh2 = st.columns([1, 1])
    with col_refresh1:
        auto_refresh = st.checkbox("è‡ªåŠ¨åˆ·æ–°ï¼ˆæ¯30ç§’ï¼‰", value=False, key="dashboard_auto_refresh")
    with col_refresh2:
        if st.button("ğŸ”„ ç«‹å³åˆ·æ–°", key="dashboard_refresh"):
            st.rerun()

    if auto_refresh:
        import time
        time.sleep(30)
        st.rerun()

    st.markdown("---")

    # ç¬¬ä¸€è¡Œï¼šæœåŠ¡çŠ¶æ€æ¦‚è§ˆ
    st.subheader("ğŸ”Œ æœåŠ¡çŠ¶æ€æ¦‚è§ˆ")
    col1, col2, col3, col4 = st.columns(4)

    # RedisçŠ¶æ€
    with col1:
        redis_status = check_redis_status(db)
        if redis_status['status'] == 'online':
            st.success(f"ğŸŸ¢ Redis: {redis_status['status']}")
            st.metric("è¿æ¥æ•°", redis_stats.get('connected_clients', 0))
            st.metric("é”®æ•°é‡", redis_stats.get('key_count', 0))
        else:
            st.error(f"ğŸ”´ Redis: {redis_status['status']}")
            st.warning(f"é”™è¯¯: {redis_status.get('error', 'æœªçŸ¥é”™è¯¯')}")

    # SQLiteçŠ¶æ€
    with col2:
        sqlite_status = check_sqlite_status(db)
        if sqlite_status['status'] == 'online':
            st.success(f"ğŸŸ¢ SQLite: {sqlite_status['status']}")
            st.metric("è¡¨æ•°é‡", sqlite_status.get('table_count', 0))
            st.metric("æ€»è®°å½•æ•°", sqlite_status.get('total_records', 0))
        else:
            st.error(f"ğŸ”´ SQLite: {sqlite_status['status']}")
            st.warning(f"é”™è¯¯: {sqlite_status.get('error', 'æœªçŸ¥é”™è¯¯')}")

    # æ•°æ®å¥åº·åº¦
    with col3:
        health_score = calculate_data_health(db)
        health_color = "ğŸŸ¢" if health_score >= 80 else "ğŸŸ¡" if health_score >= 60 else "ğŸ”´"
        st.metric(f"{health_color} æ•°æ®å¥åº·åº¦", f"{health_score}/100")

    # ç³»ç»Ÿè¿è¡Œæ—¶é—´
    with col4:
        uptime = get_system_uptime()
        st.metric("â±ï¸ ç³»ç»Ÿè¿è¡Œæ—¶é—´", uptime)

    st.markdown("---")

    # ç¬¬äºŒè¡Œï¼šæ•°æ®è®°å½•æƒ…å†µ
    st.subheader("ğŸ“ˆ æ•°æ®è®°å½•æƒ…å†µ")

    col_left, col_right = st.columns([2, 1])

    with col_left:
        # æ•°æ®è®°å½•è¶‹åŠ¿å›¾
        st.markdown("### ğŸ“Š æ•°æ®è®°å½•è¶‹åŠ¿ï¼ˆè¿‘7å¤©ï¼‰")
        trend_data = get_data_record_trend(db)
        if trend_data:
            fig = px.line(
                trend_data,
                x='date',
                y='record_count',
                title='æ¯æ—¥æ•°æ®è®°å½•æ•°é‡',
                markers=True
            )
            fig.update_layout(
                xaxis_title='æ—¥æœŸ',
                yaxis_title='è®°å½•æ•°é‡',
                hovermode='x unified'
            )
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.warning("æš‚æ— æ•°æ®è®°å½•è¶‹åŠ¿")

    with col_right:
        # æ•°æ®è®°å½•åˆ†å¸ƒ
        st.markdown("### ğŸ“Š æ•°æ®è®°å½•åˆ†å¸ƒ")
        distribution = get_data_distribution(db)
        if distribution:
            fig = px.pie(
                distribution,
                values='count',
                names='table',
                title='å„è¡¨æ•°æ®åˆ†å¸ƒ'
            )
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.warning("æš‚æ— æ•°æ®åˆ†å¸ƒä¿¡æ¯")

    st.markdown("---")

    # ç¬¬ä¸‰è¡Œï¼šæ•°æ®é—®é¢˜è¯Šæ–­
    st.subheader("ğŸ” æ•°æ®é—®é¢˜è¯Šæ–­")

    col_diag1, col_diag2 = st.columns([1, 1])

    with col_diag1:
        st.markdown("### âš ï¸ æ•°æ®ç¼ºå¤±å¤©æ•°")
        missing_days = find_missing_data_days(db)
        if missing_days:
            st.warning(f"å‘ç° {len(missing_days)} å¤©æ•°æ®ç¼ºå¤±")
            missing_df = pd.DataFrame({
                'æ—¥æœŸ': missing_days,
                'çŠ¶æ€': ['ç¼ºå¤±'] * len(missing_days)
            })
            st.dataframe(missing_df, use_container_width=True)
        else:
            st.success("âœ… è¿‘7å¤©æ•°æ®å®Œæ•´ï¼Œæ— ç¼ºå¤±")

    with col_diag2:
        st.markdown("### ğŸ“ æ•°æ®è´¨é‡æŠ¥å‘Š")
        quality_report = generate_quality_report(db)
        if quality_report:
            for item in quality_report:
                if item['status'] == 'ok':
                    st.success(f"âœ… {item['name']}: {item['message']}")
                elif item['status'] == 'warning':
                    st.warning(f"âš ï¸ {item['name']}: {item['message']}")
                else:
                    st.error(f"âŒ {item['name']}: {item['message']}")
        else:
            st.info("æš‚æ— è´¨é‡æŠ¥å‘Š")

    st.markdown("---")

    # ç¬¬å››è¡Œï¼šRedisæ•°æ®è¯¦æƒ…
    st.subheader("ğŸ’¾ Redisæ•°æ®è¯¦æƒ…")

    col_redis1, col_redis2 = st.columns([1, 1])

    with col_redis1:
        st.markdown("### ğŸ“Š Redisé”®åˆ†å¸ƒ")
        redis_keys_info = get_redis_keys_info(db)
        if redis_keys_info:
            keys_df = pd.DataFrame(redis_keys_info)
            st.dataframe(keys_df, use_container_width=True)
        else:
            st.info("Redisä¸­æš‚æ— æ•°æ®")

    with col_redis2:
        st.markdown("### ğŸ“ˆ Redisé”®è¿‡æœŸæƒ…å†µ")
        expiration_info = get_redis_expiration_info(db)
        if expiration_info:
            exp_df = pd.DataFrame(expiration_info)
            st.dataframe(exp_df, use_container_width=True)
        else:
            st.info("æš‚æ— è¿‡æœŸä¿¡æ¯")

    st.markdown("---")

    # ç¬¬äº”è¡Œï¼šæœåŠ¡æ€§èƒ½ç›‘æ§
    st.subheader("âš¡ æœåŠ¡æ€§èƒ½ç›‘æ§")

    col_perf1, col_perf2, col_perf3 = st.columns(3)

    with col_perf1:
        st.metric("ğŸš€ Rediså“åº”æ—¶é—´", f"{redis_status.get('response_time', 0):.2f}ms")

    with col_perf2:
        st.metric("ğŸš€ SQLiteå“åº”æ—¶é—´", f"{sqlite_status.get('response_time', 0):.2f}ms")

    with col_perf3:
        st.metric("ğŸ’¾ ç£ç›˜ä½¿ç”¨", f"{get_disk_usage():.1f}%")

    st.markdown("---")

    # ç¬¬å…­è¡Œï¼šæ—¥å¿—æ‘˜è¦
    st.subheader("ğŸ“œ æ—¥å¿—æ‘˜è¦")

    col_log1, col_log2 = st.columns([1, 1])

    with col_log1:
        st.markdown("### ğŸ”´ é”™è¯¯æ—¥å¿—ï¼ˆæœ€è¿‘10æ¡ï¼‰")
        error_logs = get_recent_logs(level='ERROR', limit=10)
        if error_logs:
            for log in error_logs:
                st.error(f"{log['time']}: {log['message']}")
        else:
            st.success("âœ… æœ€è¿‘æ— é”™è¯¯æ—¥å¿—")

    with col_log2:
        st.markdown("### âš ï¸ è­¦å‘Šæ—¥å¿—ï¼ˆæœ€è¿‘10æ¡ï¼‰")
        warning_logs = get_recent_logs(level='WARNING', limit=10)
        if warning_logs:
            for log in warning_logs:
                st.warning(f"{log['time']}: {log['message']}")
        else:
            st.success("âœ… æœ€è¿‘æ— è­¦å‘Šæ—¥å¿—")


def check_redis_status(db):
    """æ£€æŸ¥RedisçŠ¶æ€"""
    import time
    start_time = time.time()

    try:
        if db._redis_client:
            db._redis_client.ping()
            response_time = (time.time() - start_time) * 1000

            # è·å–Redisç»Ÿè®¡ä¿¡æ¯
            global redis_stats
            redis_stats = {
                'connected_clients': db._redis_client.client_list().__len__() if hasattr(db._redis_client, 'client_list') else 1,
                'key_count': db._redis_client.dbsize(),
                'used_memory': db._redis_client.info().get('used_memory_human', '0B'),
                'uptime': db._redis_client.info().get('uptime_in_days', 0)
            }

            return {
                'status': 'online',
                'response_time': response_time,
                'stats': redis_stats
            }
        else:
            return {
                'status': 'offline',
                'error': 'Rediså®¢æˆ·ç«¯æœªåˆå§‹åŒ–'
            }
    except Exception as e:
        return {
            'status': 'offline',
            'error': str(e)
        }


def check_sqlite_status(db):
    """æ£€æŸ¥SQLiteçŠ¶æ€"""
    import time
    start_time = time.time()

    try:
        cursor = db.conn.cursor()

        # è·å–è¡¨æ•°é‡
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
        tables = cursor.fetchall()

        # è·å–æ€»è®°å½•æ•°
        total_records = 0
        for table in tables:
            cursor.execute(f"SELECT COUNT(*) FROM {table[0]}")
            total_records += cursor.fetchone()[0]

        response_time = (time.time() - start_time) * 1000

        return {
            'status': 'online',
            'response_time': response_time,
            'table_count': len(tables),
            'total_records': total_records
        }
    except Exception as e:
        return {
            'status': 'offline',
            'error': str(e)
        }


def calculate_data_health(db):
    """è®¡ç®—æ•°æ®å¥åº·åº¦"""
    try:
        # åŸºç¡€åˆ†ï¼š100åˆ†
        health_score = 100

        # æ£€æŸ¥Redis
        if not check_redis_status(db)['status'] == 'online':
            health_score -= 20

        # æ£€æŸ¥SQLite
        if not check_sqlite_status(db)['status'] == 'online':
            health_score -= 30

        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        missing_days = find_missing_data_days(db)
        if missing_days:
            health_score -= len(missing_days) * 5

        # æ£€æŸ¥æœ€è¿‘é”™è¯¯æ—¥å¿—
        error_logs = get_recent_logs(level='ERROR', limit=1)
        if error_logs:
            health_score -= 10

        return max(0, health_score)
    except Exception as e:
        logger.error(f"è®¡ç®—æ•°æ®å¥åº·åº¦å¤±è´¥: {e}")
        return 0


def get_system_uptime():
    """è·å–ç³»ç»Ÿè¿è¡Œæ—¶é—´"""
    try:
        import psutil
        boot_time = datetime.fromtimestamp(psutil.boot_time())
        uptime = datetime.now() - boot_time

        days = uptime.days
        hours, remainder = divmod(uptime.seconds, 3600)
        minutes, _ = divmod(remainder, 60)

        return f"{days}å¤© {hours}å°æ—¶ {minutes}åˆ†é’Ÿ"
    except:
        return "æœªçŸ¥"


def get_data_record_trend(db):
    """è·å–æ•°æ®è®°å½•è¶‹åŠ¿ï¼ˆè¿‘7å¤©ï¼‰"""
    try:
        cursor = db.conn.cursor()

        # æ£€æŸ¥æ˜¯å¦æœ‰daily_barsè¡¨
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='daily_bars'")
        if not cursor.fetchone():
            return []

        # è·å–è¿‘7å¤©çš„æ•°æ®è®°å½•æ•°é‡
        cursor.execute("""
            SELECT date, COUNT(*) as record_count
            FROM daily_bars
            WHERE date >= date('now', '-7 days')
            GROUP BY date
            ORDER BY date
        """)

        results = cursor.fetchall()
        if not results:
            return []

        return pd.DataFrame(results, columns=['date', 'record_count']).to_dict('records')
    except Exception as e:
        logger.error(f"è·å–æ•°æ®è®°å½•è¶‹åŠ¿å¤±è´¥: {e}")
        return []


def get_data_distribution(db):
    """è·å–æ•°æ®åˆ†å¸ƒ"""
    try:
        cursor = db.conn.cursor()

        cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
        tables = cursor.fetchall()

        distribution = []
        for table in tables:
            table_name = table[0]
            cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
            count = cursor.fetchone()[0]
            distribution.append({
                'table': table_name,
                'count': count
            })

        return distribution
    except Exception as e:
        logger.error(f"è·å–æ•°æ®åˆ†å¸ƒå¤±è´¥: {e}")
        return []


def find_missing_data_days(db):
    """æŸ¥æ‰¾ç¼ºå¤±æ•°æ®çš„å¤©æ•°ï¼ˆè¿‘7å¤©ï¼‰"""
    try:
        cursor = db.conn.cursor()

        # æ£€æŸ¥æ˜¯å¦æœ‰daily_barsè¡¨
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='daily_bars'")
        if not cursor.fetchone():
            return []

        # è·å–è¿‘7å¤©çš„æ—¥æœŸ
        cursor.execute("""
            SELECT DISTINCT date
            FROM daily_bars
            WHERE date >= date('now', '-7 days')
            ORDER BY date
        """)

        existing_days = [row[0] for row in cursor.fetchall()]

        # ç”Ÿæˆè¿‘7å¤©çš„æ—¥æœŸåˆ—è¡¨
        missing_days = []
        for i in range(7):
            date = (datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d')
            if date not in existing_days:
                missing_days.append(date)

        return missing_days
    except Exception as e:
        logger.error(f"æŸ¥æ‰¾ç¼ºå¤±æ•°æ®å¤©æ•°å¤±è´¥: {e}")
        return []


def generate_quality_report(db):
    """ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š"""
    report = []

    try:
        # æ£€æŸ¥Redis
        redis_status = check_redis_status(db)
        if redis_status['status'] == 'online':
            report.append({
                'name': 'Redisè¿æ¥',
                'status': 'ok',
                'message': f'è¿æ¥æ­£å¸¸ï¼Œå“åº”æ—¶é—´ {redis_status["response_time"]:.2f}ms'
            })
        else:
            report.append({
                'name': 'Redisè¿æ¥',
                'status': 'error',
                'message': f'è¿æ¥å¤±è´¥: {redis_status["error"]}'
            })

        # æ£€æŸ¥SQLite
        sqlite_status = check_sqlite_status(db)
        if sqlite_status['status'] == 'online':
            report.append({
                'name': 'SQLiteè¿æ¥',
                'status': 'ok',
                'message': f'è¿æ¥æ­£å¸¸ï¼Œ{sqlite_status["table_count"]}ä¸ªè¡¨ï¼Œ{sqlite_status["total_records"]}æ¡è®°å½•'
            })
        else:
            report.append({
                'name': 'SQLiteè¿æ¥',
                'status': 'error',
                'message': f'è¿æ¥å¤±è´¥: {sqlite_status["error"]}'
            })

        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        missing_days = find_missing_data_days(db)
        if missing_days:
            report.append({
                'name': 'æ•°æ®å®Œæ•´æ€§',
                'status': 'warning',
                'message': f'è¿‘7å¤©ç¼ºå¤± {len(missing_days)} å¤©æ•°æ®'
            })
        else:
            report.append({
                'name': 'æ•°æ®å®Œæ•´æ€§',
                'status': 'ok',
                'message': 'è¿‘7å¤©æ•°æ®å®Œæ•´'
            })

        return report
    except Exception as e:
        logger.error(f"ç”Ÿæˆè´¨é‡æŠ¥å‘Šå¤±è´¥: {e}")
        return []


def get_redis_keys_info(db):
    """è·å–Redisé”®ä¿¡æ¯"""
    try:
        if not db._redis_client:
            return []

        keys = db._redis_client.keys('*')
        if not keys:
            return []

        keys_info = []
        for key in keys[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ªé”®
            key_type = db._redis_client.type(key)
            ttl = db._redis_client.ttl(key)
            keys_info.append({
                'é”®å': key,
                'ç±»å‹': key_type,
                'TTL': ttl if ttl > 0 else 'æ°¸ä¸è¿‡æœŸ'
            })

        return keys_info
    except Exception as e:
        logger.error(f"è·å–Redisé”®ä¿¡æ¯å¤±è´¥: {e}")
        return []


def get_redis_expiration_info(db):
    """è·å–Redisè¿‡æœŸä¿¡æ¯"""
    try:
        if not db._redis_client:
            return []

        keys = db._redis_client.keys('*')
        if not keys:
            return []

        expiration_info = []
        for key in keys:
            ttl = db._redis_client.ttl(key)
            if ttl > 0:
                expiration_info.append({
                    'é”®å': key,
                    'å‰©ä½™æ—¶é—´(ç§’)': ttl,
                    'è¿‡æœŸæ—¶é—´': datetime.now() + timedelta(seconds=ttl)
                })

        return expiration_info
    except Exception as e:
        logger.error(f"è·å–Redisè¿‡æœŸä¿¡æ¯å¤±è´¥: {e}")
        return []


def get_disk_usage():
    """è·å–ç£ç›˜ä½¿ç”¨ç‡"""
    try:
        import psutil
        disk = psutil.disk_usage('/')
        return disk.percent
    except:
        return 0


def get_recent_logs(level='ERROR', limit=10):
    """è·å–æœ€è¿‘çš„æ—¥å¿—"""
    try:
        log_dir = Path('logs')
        if not log_dir.exists():
            return []

        # æŸ¥æ‰¾æœ€æ–°çš„æ—¥å¿—æ–‡ä»¶
        log_files = sorted(log_dir.glob('*.log'), key=lambda x: x.stat().st_mtime, reverse=True)
        if not log_files:
            return []

        latest_log = log_files[0]

        # è¯»å–æ—¥å¿—æ–‡ä»¶
        logs = []
        with open(latest_log, 'r', encoding='utf-8') as f:
            for line in f:
                if f' - {level} - ' in line:
                    try:
                        # è§£ææ—¥å¿—è¡Œ
                        parts = line.split(' - ')
                        if len(parts) >= 3:
                            time_str = parts[0]
                            message = ' - '.join(parts[2:])
                            logs.append({
                                'time': time_str,
                                'message': message.strip()
                            })
                            if len(logs) >= limit:
                                break
                    except:
                        continue

        return logs
    except Exception as e:
        logger.error(f"è·å–æ—¥å¿—å¤±è´¥: {e}")
        return []


# åˆå§‹åŒ–å…¨å±€å˜é‡
redis_stats = {}